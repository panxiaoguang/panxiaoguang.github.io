{"meta":{"title":"XiaoHanys","subtitle":"Dont't forget to be awesome!","description":"生物信息知识库, 编程, 前端, 网站搭建, 人工智能, AI","author":"XiaoGuang","url":"https://xiaohanys.top","root":"/"},"pages":[{"title":"","date":"2024-09-20T04:22:15.938Z","updated":"2024-09-20T04:22:15.938Z","comments":true,"path":"404.html","permalink":"https://xiaohanys.top/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"所有分类","date":"2024-09-20T04:20:37.706Z","updated":"2024-09-20T04:20:37.706Z","comments":true,"path":"categories/index.html","permalink":"https://xiaohanys.top/categories/index.html","excerpt":"","text":""},{"title":"","date":"2024-09-20T08:00:07.836Z","updated":"2024-09-20T08:00:07.836Z","comments":true,"path":"about/index.html","permalink":"https://xiaohanys.top/about/index.html","excerpt":"","text":"一个孤独的人。 哥本哈根生物学博士，华大基因研究员生物信息副研究员。 1991-12-26 出生 2010-09-01 -&gt; 2018-06-20 于山东省内的“985工程”，“211工程”的山东大学攻读本科及硕士学位 2018-07-01 青岛华大基因研究院，继续在生物专业领域深耕科研。主攻基因组编辑生物信息学分析，次要方向为染色体外环状DNA(eccDNA) 在癌症等多种疾病中的潜在作用及标记基因发现。 2022-11-01 丹麦哥本哈根大学深造，攻读博士研究生学位，方向为基因组编辑。 发表文献 Gál, Z., Boukoura, S., Oxe, K.C. et al. Hyper-recombination in ribosomal DNA is driven by long-range resection-independent RAD51 accumulation. Nat Commun 15, 7797 (2024). Lv W, Pan X, Han P, et al. Extrachromosomal circular DNA orchestrates genome heterogeneity in urothelial bladder carcinoma. Theranostics 2024; 14(13):5102-5122. Xiang, X., Pan, X., Lv, W., et al. (2023), Identification and functional analysis of circulating extrachromosomal circular DNA in schizophrenia implicate its negative effect on the disorder. Clin. Transl. Med., 13: e1488. Xiang, X., Pan, X., Li, W. et al. Genome-wide characterization of extrachromosomal circular DNA in gastric cancer and its potential role in carcinogenesis and cancer progression. Cell. Mol. Life Sci. 80, 191 (2023). Pan, X., Qu, K., Yuan, H. et al. Massively targeted evaluation of therapeutic CRISPR off-targets in cells. Nat Commun 13, 4049 (2022). Corsi, G.I., Qu, K., Alkan, F. et al. CRISPR&#x2F;Cas9 gRNA activity depends on free energy changes and on the target PAM context. Nat Commun 13, 3006 (2022). Lv W, Pan X, Han P, et al. Circle-Seq reveals genomic and disease-specific hallmarks in urinary cell-free eccDNAs. Clin Transl Med. 2022; 12:e817. Xiang, X., Corsi, G.I., Anthon, C. et al. Enhancing CRISPR-Cas9 gRNA efficiency prediction by data integration and deep learning. Nat Commun 12, 3238 (2021)."},{"title":"","date":"2024-09-20T04:21:15.257Z","updated":"2024-09-20T04:21:15.257Z","comments":true,"path":"mylist/index.html","permalink":"https://xiaohanys.top/mylist/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2024-09-20T04:20:56.912Z","updated":"2024-09-20T04:20:56.912Z","comments":true,"path":"tags/index.html","permalink":"https://xiaohanys.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Flux:新一代AI图像生成模型","slug":"Flux下一代AI图像生成","date":"2024-10-18T05:12:00.000Z","updated":"2024-10-18T05:30:43.288Z","comments":true,"path":"flux-new-model/","permalink":"https://xiaohanys.top/flux-new-model/","excerpt":"在快速发展的 AI 图像生成领域，出现了一个新的参与者，它将重新定义AI绘画的可能性。Flux.1 就是由黑森林实验室开发的一套开创性的模型，它正在席卷 AI 社区。让我们深入了解 Flux.1 的特别之处，以及它为何被誉为图像合成领域的新王？","text":"在快速发展的 AI 图像生成领域，出现了一个新的参与者，它将重新定义AI绘画的可能性。Flux.1 就是由黑森林实验室开发的一套开创性的模型，它正在席卷 AI 社区。让我们深入了解 Flux.1 的特别之处，以及它为何被誉为图像合成领域的新王？ 何为 Flux.1 ?Flux.1 不仅仅是另一个 AI 模型，它是由 Black Forest Labs(黑森林实验室) 开发的一系列最先进的文本转图像生成模型。Black Forest Labs 由一群杰出的 AI 研究人员和工程师创立，其中包括 Stable Diffusion 的创建者，致力于推动生成 AI 的发展。 Flux.1 套件包含三个版本： Flux.1 [pro]：旗舰型号，在图像生成方面提供无与伦比的性能。 Flux.1 [dev]：针对非商业应用的开放权重模型。 Flux.1 [schnell]：最快的型号，专为本地开发和个人使用而设计。 每个版本都针对不同的用例进行了定制，确保无论您是普通用户还是专业开发人员，都有适合您需求的 Flux.1 模型。 背后的技术Flux.1 的核心是混合架构，它结合了多模态和并行扩散变压器块。Flux.1 可扩展到令人印象深刻的 120 亿个参数，利用流匹配、旋转位置嵌入和并行注意层等先进技术来实现其卓越的性能。 这种创新方法使 Flux.1 在图像生成的各个方面表现出色，包括： 视觉质量 及时遵守 尺寸和长宽比变化 排版 输出分集 Flux.1 为何脱颖而出 无与伦比的性能：Flux.1 [pro] 和 [dev] 在多个基准测试中的表现优于 Midjourney v6.0、DALL·E 3 (HD) 和 SD3-Ultra 等热门型号。 多功能性：通过支持各种宽高比和分辨率，Flux.1 在图像创建方面提供了前所未有的灵活性。 速度：Flux.1 [schnell] 是目前最先进的快速模型，可在不影响质量的情况下快速生成图像。 开源：[dev] 和 [schnell] 版本是开源的，促进了 AI 社区内的创新和协作。 企业解决方案：对于希望利用尖端人工智能的企业，Flux.1 [pro] 提供专用和定制的企业解决方案。 如何开始使用 Flux.1准备好体验 AI 图像生成的未来了吗？您可以按照以下方法开始： 对于 Flux.1 [pro] 去官方获取API 在 Hugging Face 上尝试 Flux.1 [dev] 或者 Replicate 从 GitHub 下载 Flux.1 [schnell] 供本地使用 今天我们推荐一个目前来说完全免费的网站可以完全使用三个模型，无论是 Pro亦或者schnell, 都是免费无限制的，速速体验。 fluximageaihttps://www.fluximageai.com/ 结论Flux.1 代表了 AI 图像生成技术的重大飞跃。它结合了先进的架构、多功能模型和开源可用性，使其成为该领域的游戏规则改变者。无论您是 AI 爱好者、专业开发人员还是希望利用生成 AI 力量的企业，Flux.1 都能提供满足甚至超出您期望的解决方案。 随着人工智能领域的不断发展，有一点是明确的：Flux.1 正引领我们进入一个充满创意可能性的新时代。若想站在这项激动人心的技术的前沿，请务必访问https://www.fluximageai.com探索 Flux.1 ，并加入塑造人工智能生成媒体未来的创新者社区。","categories":[{"name":"网站分享","slug":"网站分享","permalink":"https://xiaohanys.top/categories/%E7%BD%91%E7%AB%99%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://xiaohanys.top/tags/AI/"},{"name":"网站分享","slug":"网站分享","permalink":"https://xiaohanys.top/tags/%E7%BD%91%E7%AB%99%E5%88%86%E4%BA%AB/"}],"author":"Xiaoguang Pan"},{"title":"小白如何用reflex搭建一个AI应用?","slug":"小白用reflex搭建一个AI绘画的应用是怎么样的体验","date":"2024-10-16T09:12:34.000Z","updated":"2024-10-18T05:33:05.164Z","comments":true,"path":"reflex-ai-app/","permalink":"https://xiaohanys.top/reflex-ai-app/","excerpt":"我们在一年前启动了 Reflex，以便任何了解 Python 的人都可以轻松构建 Web 应用程序并与世界分享，而无需学习新语言和拼凑一堆不同的工具。 Web 开发是最流行的编程用例之一。 Python 是世界上最流行的编程语言之一。那么为什么我们不能用 Python 构建 Web 应用程序呢？ 以上来自Reflex构建者发布的博客，原文链接如下： 设计纯Python Web框架https://reflex.dev/blog/2024-03-21-reflex-architecture/","text":"我们在一年前启动了 Reflex，以便任何了解 Python 的人都可以轻松构建 Web 应用程序并与世界分享，而无需学习新语言和拼凑一堆不同的工具。 Web 开发是最流行的编程用例之一。 Python 是世界上最流行的编程语言之一。那么为什么我们不能用 Python 构建 Web 应用程序呢？ 以上来自Reflex构建者发布的博客，原文链接如下： 设计纯Python Web框架https://reflex.dev/blog/2024-03-21-reflex-architecture/ 所以真的有一种感觉，Reflex使用一种语言让我们小白可以直接做到以前需要整合 Html, Javascript, Python等众多语言才能构建的复杂应用，简直是神奇！那么今天，我来介绍一下我自己搭建的基于免费API的AI工具的。 首先看一下默认界面。 首先是 AI 绘画应用， 功能介绍： 目前一共支持 4 个模型，其中Flux系列是最好的，Flux-dev是收费的 支持魔术提示词，即使输入中文，也可以翻译成合适且有想象力的提示词 支持不同尺寸图形绘制 支持高级选项，例如负面提示词，步数和cfg_scale 懒，没有优化手机端，所以必须用电脑，否则，页面会乱 如何使用：免费版的API有速率限制，我一开始也想直接内置我的算了，后来发现不行，于是修改为可以手动设置自己的API，而获取API也很简单，只需要去到SiliconFlow官网就可以获取了。 而魔术提示词（Magic Prompt）是需要 OpenAI提供的大语言模型来实现翻译并优化的，或者使用 OpenAI兼容的 API端点。这里推荐大家使用 小海Chathttps://api1.zhtec.xyz/ 提供的 API，GPT4的价格都非常便宜。 设置界面如图所示 其中： SiliconFlow API Key: 就是从siliconflow获取的 API的 Magic Prompt endpoint: OpenAI（GPT）兼容的API端点链接 Magic Prompt API key: GPT模型对应的API Magic Prompt Model: 你打算用哪个模型来优化提示词，gpt-4o最强，但是贵！！！ 测试当你设置好以上信息，这些信息会被储存到你自己的浏览器中，然后下次再打开就不用重复输入了。然后我们尝试输入一下提示词看看： 虽然输入了中文，但是画面依旧很优秀 直达链接: aiarthttps://sequencetool.reflex.run/aiart/ 至于AI 对话应用就和 AI 绘画是相同的逻辑，你只需要获取API就可以了，没有别的要注意的。 聊天页面 直达链接: aichathttps://sequencetool.reflex.run/aichat/ 源代码因为实现逻辑比较复杂，所以在本文就不再详细的介绍实现的原理了，感兴趣可以查看代码： panxiaoguang/SequenceToolhttps://github.com/panxiaoguang/SequenceTool","categories":[{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/categories/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://xiaohanys.top/tags/AI/"},{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"},{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/tags/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"},{"name":"前端","slug":"前端","permalink":"https://xiaohanys.top/tags/%E5%89%8D%E7%AB%AF/"}],"author":"Xiaoguang Pan"},{"title":"如何在Reflex里面使用IGV浏览器？","slug":"如何在Reflex里面使用IGV浏览器","date":"2024-10-09T04:49:48.000Z","updated":"2024-10-18T05:32:23.667Z","comments":true,"path":"reflex-igv/","permalink":"https://xiaohanys.top/reflex-igv/","excerpt":"note Reflex 是一个开源库，旨在使用纯 Python 构建全栈 Web 应用程序。它的设计理念是让开发者能够完全使用 Python 编写应用的前端和后端，无需学习 JavaScript。这对于那些希望专注于 Python 编程语言的开发者来说是一个重大的福音。 上面说的都是场面话，这都依托于它自身包装的第三方应用够丰富。但是，对于一些冷门的应用，比如我这里要说的IGV浏览器，就没有那么容易了。这里我就来分享一下我是如何在 Reflex 里面使用 IGV 浏览器的。","text":"note Reflex 是一个开源库，旨在使用纯 Python 构建全栈 Web 应用程序。它的设计理念是让开发者能够完全使用 Python 编写应用的前端和后端，无需学习 JavaScript。这对于那些希望专注于 Python 编程语言的开发者来说是一个重大的福音。 上面说的都是场面话，这都依托于它自身包装的第三方应用够丰富。但是，对于一些冷门的应用，比如我这里要说的IGV浏览器，就没有那么容易了。这里我就来分享一下我是如何在 Reflex 里面使用 IGV 浏览器的。 包装igv.js到react组件首先，我们需要把 igv.js 包装成一个 React 组件。我们以两个需要实现的交互为例子来说明： 第一个是我们有一个外部的控制器，可以通过这个控制器来控制 igv.js 显示的坐标范围。 第二个是当我们鼠标点击某个区域的时候，我们可以获取到这个区域的所有信息。 首先，通过查阅 IGV.js的文档，我们知道，我们可以通过 igv.createBrowser 来创建一个 IGV 浏览器。我们可以通过这个函数来创建一个 IGV 浏览器，然后通过这个浏览器的实例来控制 IGV 的显示。 import igv from &quot;https://cdn.jsdelivr.net/npm/igv@3.0.0/dist/igv.esm.min.js&quot;const div = document.getElementById(&quot;igv_div&quot;)const config = &#123; genome: &quot;hg19&quot;&#125;const browser = await igv.createBrowser(div, config) 然后我们可以通过igv.removeAllBrowsers()来移除所有的浏览器。 那么，我们放到 React 组件里面，就是这样的： import React, &#123; useEffect, useRef &#125; from &quot;react&quot;;import igv from &quot;igv&quot;;const IgvComponent = () =&gt; &#123; const igvContainerRef = useRef(null); const browserRef = useRef(null); useEffect(()=&gt;&#123; const options = &#123; genome: &#x27;hg19&#x27;, &#125;; igv.createBrowser(igvContainerRef.current, options).then(browser =&gt; &#123; browserRef.current = browser; &#125;); &#125;); return ( &lt;&gt; &lt;div ref=&#123;igvContainerRef&#125; /&gt;; &lt;/&gt; );&#125;;export default IgvComponent; 现在，我们来添加交互，我们想要实现的第二个功能需要 browser.on来监听事件。比如我们想要监听鼠标点击事件，我们可以这样： browser.on(&quot;trackclick&quot;, (track, popoverData) =&gt; &#123;&#125;) 我们放入 React 组件里面，就是这样的： import React, &#123; useEffect, useRef &#125; from &quot;react&quot;;import igv from &quot;igv&quot;;const IgvComponent = (genome, locus, tracks, onTrackClick) =&gt; &#123; const igvContainerRef = useRef(null); const browserRef = useRef(null); useEffect(()=&gt;&#123; const options = &#123; genome, locus, tracks, &#125;; igv.createBrowser(igvContainerRef.current, options).then(browser =&gt; &#123; browserRef.current = browser; if (onTrackClick) &#123; browser.on(&quot;trackclick&quot;, (track, popoverData) =&gt; &#123; const trackName = track.name; const trackValue = popoverData.map((data) =&gt; (&#123; name: data.name, value: data.value, &#125;)); onTrackClick(trackName, trackValue); &#125;); &#125; &#125;); &#125;); return ( &lt;&gt; &lt;div ref=&#123;igvContainerRef&#125; /&gt;; &lt;/&gt; );&#125;;export default IgvComponent; 而第一个要求里，我们要外部控制，需要获取到browser的实例，然后通过这个实例来控制 IGV 的显示。我们可以通过 browser.search 来搜索某个基因，通过 browser.zoomIn 和 browser.zoomOut 来缩放。这在react里是这样的： import React, &#123; useEffect, useRef &#125; from &quot;react&quot;;import igv from &quot;../node_modules/igv/dist/igv.esm.min.js&quot;;const IgvComponent = (&#123; genome, locus, tracks, onTrackClick &#125;) =&gt; &#123; const igvContainerRef = useRef(null); const browserRef = useRef(null); useEffect(() =&gt; &#123; const options = &#123; genome, locus, tracks, &#125;; if (!browserRef.current) &#123; igv.createBrowser(igvContainerRef.current, options).then((browser) =&gt; &#123; browserRef.current = browser; if (onTrackClick) &#123; browser.on(&quot;trackclick&quot;, (track, popoverData) =&gt; &#123; const trackName = track.name; const trackValue = popoverData.map((data) =&gt; (&#123; name: data.name, value: data.value, &#125;)); onTrackClick(trackName, trackValue); &#125;); &#125; &#125;); &#125; // 清理函数，在组件卸载时调用 return () =&gt; &#123; if (browserRef.current) &#123; igv.removeAllBrowsers() &#125; &#125;; &#125;, []); useEffect(() =&gt; &#123; if (browserRef.current) &#123; browserRef.current.search(locus); &#125; &#125;, [locus]); return &lt;div ref=&#123;igvContainerRef&#125; /&gt;;&#125;;export default IgvComponent; 这样，我们就完成了最终的包装。我们可以自己搭建React环境来测试一下是否可行，或者可以去找在线平台测试，例如playcode 在 Reflex 里面使用根据官方文档的表述，React组件的包装就相当简单了，我们这里是本地的组件，所以就可以这样： ### IGV.pyimport reflex as rxfrom typing import List, Dictfrom reflex.components.component import NoSSRComponent### 一定要注意这个只可以在客户端渲染，不支持服务端class IGV(NoSSRComponent): ###官方文档告诉我们，我们的组件需要放在assets文件夹下,但是这里要写public library = &quot;/public/igv&quot; tag = &quot;IgvComponent&quot; is_default = True lib_dependencies: list[str] = [ &quot;igv@3.0.6&quot; ] genome: rx.Var[str] locus: rx.Var[str] tracks: rx.Var[List[Dict[str, str]]] on_track_click: rx.EventHandler[lambda e0, e1: [e0, e1]]igvComponent = IGV.create 一定要注意的是，Reflex默认是服务端渲染的，但是显然 IGV浏览器无法在服务端兼容，那么我们就是用 NoSSRComponent来包装。 如何使用？我们可以写一个小的 Demo来测试一下： import reflex as rxfrom .IGV import igvComponentclass State(rx.State): &quot;&quot;&quot;The app state.&quot;&quot;&quot; genome: str = &quot;hg38&quot; loc: str = &quot;chr8:127736588-127739371&quot; tracks: list[dict[str, str]] = [ &#123; &quot;name&quot;: &quot;HG00103&quot;, &quot;url&quot;: &quot;https://s3.amazonaws.com/1000genomes/data/HG00103/alignment/HG00103.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram&quot;, &quot;indexURL&quot;: &quot;https://s3.amazonaws.com/1000genomes/data/HG00103/alignment/HG00103.alt_bwamem_GRCh38DH.20150718.GBR.low_coverage.cram.crai&quot;, &quot;format&quot;: &quot;cram&quot; &#125;] location: str trackName: str trackdata: list[dict[str, str]] def change_loc(self): self.loc = self.location def get_click_info(self, e0: str, e1: list[dict[str, str]]): self.trackName = e0 self.trackdata = [&#123;&quot;name&quot;: itm[&#x27;name&#x27;], &quot;value&quot;: itm[&#x27;value&#x27;]&#125; for itm in e1 if itm[&#x27;name&#x27;]]def data_item(name: str, value: str) -&gt; rx.Component: return rx.data_list.item( rx.data_list.label(name), rx.data_list.value(value) )def data_card(trackdata: list[dict[str, str]]) -&gt; rx.Component: return rx.card( rx.data_list.root( rx.foreach(trackdata, lambda item: data_item( item[&#x27;name&#x27;], item[&#x27;value&#x27;])) ) )def index() -&gt; rx.Component: # Welcome Page (Index) return rx.container( rx.vstack( rx.heading(&quot;IGV test!&quot;, size=&quot;9&quot;), rx.box( igvComponent(genome=State.genome, locus=State.loc, tracks=State.tracks, on_track_click=State.get_click_info), width=&quot;100%&quot;, ), rx.hstack( rx.input(placeholder=&quot;Enter location&quot;, on_blur=State.set_location, width=&quot;40%&quot;), rx.button(&quot;GoTo!&quot;, on_click=State.change_loc), width=&quot;100%&quot;, justify=&quot;center&quot; ), rx.divider(), rx.cond( State.trackName, rx.vstack( rx.heading(State.trackName, size=&quot;4&quot;), data_card(State.trackdata) ), rx.heading(&quot;No track clicked&quot;, size=&quot;4&quot;) ), spacing=&quot;5&quot;, justify=&quot;center&quot;, min_height=&quot;85vh&quot;, ), rx.logo() )app = rx.App()app.add_page(index) 使用就很简单了，一旦包装好，我们导入它就可以当做官方的组件来使用了，我们可以为他写 State，写 EventHandler，写页面，写交互，写样式，写逻辑，写一切。 总结这里我们介绍了如何将 igv.js 包装成 React 组件，然后将 React 组件包装成 Reflex 组件，最后在 Reflex 里面使用。这里只是一个简单的例子，实陿上，我们可以包装任何我们想要的组件，只要我们有足够的耐心和技术。本文的代码在 Github 可以获取。 note 这里的 igv.js 是一个简单的例子，实陿上，我们可以包装任何我们想要的组件，只要我们有足够的耐心和技术。","categories":[{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/categories/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"},{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/tags/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"}],"author":"Xiaoguang Pan"},{"title":"使用二代和三代WGS数据进行基因组组装","slug":"使用二代和三代WGS数据进行基因组组装","date":"2024-09-20T05:12:14.000Z","updated":"2024-10-18T05:32:48.022Z","comments":true,"path":"sec-third-16s-assembly/","permalink":"https://xiaohanys.top/sec-third-16s-assembly/","excerpt":"本教程中，我们的目标是使用第二代和第三代全基因组测序 (WGS) 数据组装细菌基因组。我们将以此为例来探讨WGS数据分析，并探讨测序技术之间的差异。","text":"本教程中，我们的目标是使用第二代和第三代全基因组测序 (WGS) 数据组装细菌基因组。我们将以此为例来探讨WGS数据分析，并探讨测序技术之间的差异。 软件安装Docker镜像您可以使用以下命令从DockerHub中提取镜像： docker pull yanhui09/mac2023_extra 用mamba安装建议使用mamba在独立 conda环境中安装软件。 假设您已经在系统中安装了mamba软件，您可以使用mamba创建一个新conda环境： 转到下载的数据目录。确保您知道自己当前的工作目录位置。 例如，下载的数据目录位于/home/username/MAC2023-extera cd /home/username/MAC2023-exterapwd 您将看到下载的数据目录的路径，如下所示。 /home/username/MAC2023-extera 现在您可以使用以下命令创建新conda环境并安装软件： mamba env create -n wgs1 -f envs/env1.yaml 激活环境以进行以下分析。 conda activate wgs1 使用演示数据进行探索WGS 数据可以像以前一样从MAC2023-extra获取。 首先用seqkit来查看数据seqkit stat data/wgs/*.fastq.gz data/wgs/ncbi_pacbio_TL110.fasta 预期输出： file format type num_seqs sum_len min_len avg_len max_lendata/wgs/NXT20x_R1.fastq.gz FASTQ DNA 200,000 30,200,000 151 151 151data/wgs/NXT20x_R2.fastq.gz FASTQ DNA 200,000 30,200,000 151 151 151data/wgs/ont_r10_20x.fastq.gz FASTQ DNA 7,862 51,201,670 129 6,512.6 87,688data/wgs/ncbi_pacbio_TL110.fasta FASTA DNA 1 2,566,312 2,566,312 2,566,312 2,566,312 这是来自费氏丙酸杆菌菌株的一组测序数据。我们对 Illumina 和 Oxford Nanopore Technologies (ONT) 读数的测序数据进行二次采样，覆盖率达到 20 倍。PacBio 参考基因组来自NCBI RefSeq 数据库。 Q1：该菌株的基因组大小是多少？测序覆盖度是如何计算的？ illumina：30,200,000*2&#x2F;2,566,312≈20 ONT：51,201,670&#x2F;2,566,312≈20 我们用fastqc来看看测序数据的质量。 mkdir -p fastqc/illumina fastqc/ont_r10fastqc data/wgs/NXT20x_R*.fastq.gz -o ./fastqc/illuminafastqc data/wgs/ont_r10_20x.fastq.gz -o ./fastqc/ont_r10 您可以打开.html该fastqc目录下的文件来查看测序数据的质量。 ONT Illumina 我们可以看到 ONT 读取比 Illumina 读取更长，但包含更多错误。 使用 Illumina 数据进行基因组组装trimmomatic去除接头trimmomatic是一个用于移除接头和低质量数据的工具。[阅读更多] 使用Nextera文库制备试剂盒从NextSeq平台收集 Illumina 读数。 mkdir illuminatrimmomatic PE -threads 4 -phred33 data/wgs/NXT20x_R1.fastq.gz data/wgs/NXT20x_R2.fastq.gz illumina/NXT20x_R1_paired.fastq.gz illumina/NXT20x_R1_unpaired.fastq.gz illumina/NXT20x_R2_paired.fastq.gz illumina/NXT20x_R2_unpaired.fastq.gz ILLUMINACLIP:data/wgs/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 MINLEN:50 预期输出： illumina/NXT20x_R1_paired.fastq.gzillumina/NXT20x_R1_unpaired.fastq.gzillumina/NXT20x_R2_paired.fastq.gzillumina/NXT20x_R2_unpaired.fastq.gz 数据质量控制bbmapbbmap是一套用于测序读数质量控制的工具。它可用于删除冗余数据和 PhiX 对照。[阅读更多] clumpify.sh是一个删除冗余数据的工具。[阅读更多] bbduk.sh是一种去除污染数据的工具（例如，宿主基因组、PhiX 对照）。[阅读更多] #clumpifyclumpify.sh in=illumina/NXT20x_R1_paired.fastq.gz in2=illumina/NXT20x_R2_paired.fastq.gz out=illumina/NXT20x_R1_paired_dedup.fastq.gz out2=illumina/NXT20x_R2_paired_dedup.fastq.gz dedupe optical spany adjacent# bbdukbbduk.sh in=illumina/NXT20x_R1_paired_dedup.fastq.gz in2=illumina/NXT20x_R2_paired_dedup.fastq.gz out=illumina/NXT20x_R1_paired_dedup_deduk.fastq.gz out2=illumina/NXT20x_R2_paired_dedup_deduk.fastq.gz ref=data/wgs/phiX174.fasta k=31 hdist=1 预期输出： illumina/NXT20x_R1_paired_dedup.fastq.gzillumina/NXT20x_R1_paired_dedup_deduk.fastq.gzillumina/NXT20x_R2_paired_dedup.fastq.gzillumina/NXT20x_R2_paired_dedup_deduk.fastq.gz 基因组组装spadesspades是一个用于短读长的基因组组装器。[阅读更多] spades.py --isolate -t 4 -1 illumina/NXT20x_R1_paired_dedup_deduk.fastq.gz -2 illumina/NXT20x_R2_paired_dedup_deduk.fastq.gz -o illumina/spades 预计组装： illumina/spades/contigs.fasta Q2：我们在这里从分离株中组装了细菌基因组。如果我们有宏基因组样本怎么办？ 我们可以使用--meta选项在spades中来组装宏基因组样本。[阅读更多] 使用 ONT 数据进行基因组组装使用guppy（可选）或者porechop移除接头 本练习中未提供guppy和porechop的安装如果您想使用它们，请尝试自行安装。 guppy是一种用于 ONT 数据的碱基识别和接头修剪的工具。guppy不是开源的，因此需要注册ONT帐户才能查看其文档和下载。[阅读更多] porechop是一个开源工具，用于 ONT 数据的接头修剪。[阅读更多] 默认情况下，条形码将在guppy多路分解步骤中被修剪。我们不会对我们的数据重复进行条形码修剪。 如果您想修剪条形码，可以使用以下命令guppy。 guppy_barcoder -i data/wgs/ont_r10_20x.fastq.gz -s ont_r10/ont_r10_20x_barcoded.fastq.gz --barcode_kits EXP-NBD104 --trim_barcodes 以及以下命令porechop。 porechop -i data/wgs/ont_r10_20x.fastq.gz -o ont_r10/ont_r10_20x_porechop.fastq.gz --threads 4 seqkit读取质量控制seqkit是一种用于操作测序数据的工具。[阅读更多]这里我们用来seqkit去除短读和低质量数据。 mkdir ont_r10seqkit seq -j 4 -Q 10 -m 2000 -i data/wgs/ont_r10_20x.fastq.gz -o ont_r10/ont_r10_20x_f.fastq.gz 使用seqkit stat来检查质量控制之前和之后的 ONT 读数。 seqkit stat data/wgs/ont_r10_20x.fastq.gz ont_r10/ont_r10_20x_f.fastq.gz 预期输出： file format type num_seqs sum_len min_len avg_len max_lendata/wgs/ont_r10_20x.fastq.gz FASTQ DNA 7,862 51,201,670 129 6,512.6 87,688ont_r10/ont_r10_20x_f.fastq.gz FASTQ DNA 6,561 48,742,515 2,001 7,429.1 87,688 flye基因组组装flye是 ONT 推荐的长读基因组组装器。[阅读更多] flye --nano-raw ont_r10/ont_r10_20x_f.fastq.gz --out-dir ont_r10/flye --threads 4 预期的组装输出： ont_r10/flye/assembly.fasta Q3：默认flye用于组装基因组。如果我们有宏基因组样本怎么办？ 我们可以使用--meta选项来在flye中组装宏基因组样本。[阅读更多] 使用racon与medaka进行基因组矫正长读长基因组组装程序通常会产生连续性高但准确性低的基因组草图。需要额外的矫正步骤来提高基因组草图的准确性。但最佳的矫正策略和工具仍在争论中。 racon是一种基于图的相似度算法，用于完善长读长基因组组装。[阅读更多]。 medaka是官方为ONT数据打造的基于神经网络的数据矫正工具。[阅读更多] 最近，ONT 推荐用medaka直接矫正flye组装后的数据。但结合racon和medaka仍然是一种常见的做法。 这里我们以medaka在例子： 由于medaka是基于神经网络的，选择合适的模型会影响抛光效果。您可以用来medaka tools list_models列出所有可用的模型。对于我们的示例，ONT 读数是从R10.4.1 flowcell收集的，并且guppy采用hac模式来做碱基生成。 medaka tools list_modelsmedaka_consensus -i ont_r10/ont_r10_20x_f.fastq.gz -d ont_r10/flye/assembly.fasta -o ont_r10/medaka -t 4 -m r1041_e82_260bps_hac_v4.1.0 预期结果： ont_r10/medaka/consensus.fasta racon和medaka混合模式 mkdir ont_r10/raconminimap2 -t 4 -x map-ont ont_r10/flye/assembly.fasta ont_r10/ont_r10_20x_f.fastq.gz &gt; ont_r10/racon/flye_assembly.pafracon -t 4 ont_r10/ont_r10_20x_f.fastq.gz ont_r10/racon/flye_assembly.paf ont_r10/flye/assembly.fasta &gt; ont_r10/racon/racon.fastamedaka_consensus -i ont_r10/ont_r10_20x_f.fastq.gz -d ont_r10/racon/racon.fasta -o ont_r10/racon/medaka -t 4 -m r1041_e82_260bps_hac_v4.1.0 预期的结果： ont_r10/racon/racon.fastaont_r10/racon/medaka/consensus.fasta ONT 和 Illumina 读取的混合组装混合组装是结合不同测序技术优势的常见做法。这里我们选择两种常用的混合组装策略： pilon：直接使用 ONT 组装作为主干，并用 Illumina 读取对其进行矫正。 unicycler：短读优先混合组装。 pilonILLUMINA 读取矫正pilon是一种用短读长优化基因组组装的工具。[阅读更多] mkdir -p hybrid/pilonbwa index ont_r10/medaka/consensus.fastabwa mem -t 4 ont_r10/medaka/consensus.fasta illumina/NXT20x_R1_paired_dedup_deduk.fastq.gz illumina/NXT20x_R2_paired_dedup_deduk.fastq.gz | samtools sort -@ 4 -o hybrid/pilon/aligned.bamsamtools index hybrid/pilon/aligned.bampilon --genome ont_r10/medaka/consensus.fasta --frags hybrid/pilon/aligned.bam --output hybrid/pilon/pilon --threads 4 预期产出： hybrid/pilon/pilon.fasta 可选：unicycler混合组装unicycler可以进行短读优先混合组装。[阅读更多] 这需要一些时间，因此我们将在示例中跳过这一步。完成其他步骤后，请随时返回此步骤。 Linux用户 unicycler -l ont_r10/ont_r10_20x_f.fastq.gz -1 illumina/NXT20x_R1_paired_dedup_deduk.fastq.gz -2 illumina/NXT20x_R2_paired_dedup_deduk.fastq.gz -o hybrid/unicycler --threads 4 MacOS 用户 由于on的最新MacOS版本落后于0.5.0，我们附加两个标志和来保持最大的兼容性。unicyclerconda--no_correct--no_pilon unicycler --no_correct --no_pilon -l ont_r10/ont_r10_20x_f.fastq.gz -1 illumina/NXT20x_R1_paired_dedup_deduk.fastq.gz -2 illumina/NXT20x_R2_paired_dedup_deduk.fastq.gz -o hybrid/unicycler 预期的产出： hybrid/unicycler/assembly.fasta 使用quast对组装基因组进行质量评估我们已经生成了许多具有不同策略的程序集。让我们使用基于PacBio深度测序的完整组装作为参考基因组来评估每个数据集的质量。如果您尚未完成所有组装步骤，您可以使用./data/wgs/assemblies提供的组装文件。 seqkit stat data/wgs/assemblies/*.fasta data/wgs/ncbi_pacbio_TL110.fasta 预期输出： file format type num_seqs sum_len min_len avg_len max_lendata/wgs/assemblies/hybrid_pilon.fasta FASTA DNA 2 2,579,926 29,242 1,289,963 2,550,684data/wgs/assemblies/hybrid_pilon_proovframe.fasta FASTA DNA 2 2,579,979 29,245 1,289,989.5 2,550,734data/wgs/assemblies/hybrid_unicycler.fasta FASTA DNA 1 2,564,177 2,564,177 2,564,177 2,564,177data/wgs/assemblies/illumina.fasta FASTA DNA 217 2,527,918 78 11,649.4 60,978data/wgs/assemblies/ont_flye.fasta FASTA DNA 2 2,579,333 29,239 1,289,666.5 2,550,094data/wgs/assemblies/ont_flye_medaka.fasta FASTA DNA 2 2,579,052 29,238 1,289,526 2,549,814data/wgs/assemblies/ont_flye_racon.fasta FASTA DNA 2 2,578,932 29,231 1,289,466 2,549,701data/wgs/assemblies/ont_flye_racon_medaka.fasta FASTA DNA 2 2,578,566 29,229 1,289,283 2,549,337data/wgs/ncbi_pacbio_TL110.fasta FASTA DNA 1 2,566,312 2,566,312 2,566,312 2,566,312 与参考相比，各个组装的总长度相似。 但序列数 ( num_seqs) 和最大序列长度 ( max_len) 变化很大。 让我们用quast来检查各个组装的质量。 quast data/wgs/assemblies/*.fasta -r data/wgs/ncbi_pacbio_TL110.fasta -o quast 您可以打开quast目录中的report.html文件来查看各个组装数据集的质量。 Q4：根据示例，您认为哪种测序技术在基因组完整性和连续性方面效果更好？ 检查Genome fraction和NGA50 Q5：根据示例，您认为哪种装配的精度最好？ 检查Misassemblies和Mismatches Q6：根据示例，ONT 组件中的主要错误类型是什么？ 检查mismatches和indels。[阅读更多] Q7：您认为此示例的最佳组装策略是什么？ 可选：参考引导校正proovframe高频率indels会导致 ONT 组装的编码序列 (CDS) 发生移码。通过参考引导校正，我们可以校正 ONT 组装的 CDS 中的移码。 此步骤是可选的，如果您没有时间，可以跳过它。 为了使用该工具，由于依赖冲突，proovframe我们需要创建另一个conda环境。 conda deactivatemamba env create -n wgs2 -f envs/env2.yamlconda activate wgs2 基因组注释prokkaprokka是快速注释细菌基因组的常用工具。[阅读更多] mkdir proovframeconda activate wgs2prokka --outdir proovframe/prokka --prefix pacbio --cpus 4 data/wgs/ncbi_pacbio_TL110.fasta 预期输出： proovframe/prokka/pacbio.errproovframe/prokka/pacbio.faaproovframe/prokka/pacbio.ffnproovframe/prokka/pacbio.fnaproovframe/prokka/pacbio.fsaproovframe/prokka/pacbio.gbkproovframe/prokka/pacbio.gffproovframe/prokka/pacbio.logproovframe/prokka/pacbio.sqnproovframe/prokka/pacbio.tblproovframe/prokka/pacbio.tsvproovframe/prokka/pacbio.txt 我们有许多来自prokka的输出文件。这里我们仅使用翻译后的蛋白质序列（./proovframe/prokka/pacbio.faa文件） 移码校正proovframe proovframe是 CDS 中基于参考序列的移码校正工具。[阅读更多] proovframe map -a proovframe/prokka/pacbio.faa -o proovframe/pilon.tsv hybrid/pilon/pilon.fastaproovframe fix -o proovframe/pilon_corrected.fasta hybrid/pilon/pilon.fasta proovframe/pilon.tsv 预期的输出： proovframe/pilon_corrected.fasta Q8：在之前的报告中，shybrid_pilon_proovframe.fasta中的N&#39;会增加。你觉得这是好是坏？ 本文为学习记录，课程作者为：YanHui","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"author":"Xiaoguang Pan"},{"title":"使用NART通过读取分类进行长扩增子分析","slug":"使用NART通过读取分类进行长扩增子分析","date":"2024-09-20T05:11:35.000Z","updated":"2024-10-18T05:32:57.411Z","comments":true,"path":"nart-16s-long/","permalink":"https://xiaohanys.top/nart-16s-long/","excerpt":"NART设计用于基于图谱的纳米孔扩增子（实时）分析，例如 16S rRNA 基因。NART由NART（Nanopore Amplicon Real-Time entry）和 NAWF（Nanopore Amplicon snakemake WorkFlow entry）组成。通过基于映射的策略提供从基础调用读取到最终计数矩阵的（实时）端到端解决方案。","text":"NART设计用于基于图谱的纳米孔扩增子（实时）分析，例如 16S rRNA 基因。NART由NART（Nanopore Amplicon Real-Time entry）和 NAWF（Nanopore Amplicon snakemake WorkFlow entry）组成。通过基于映射的策略提供从基础调用读取到最终计数矩阵的（实时）端到端解决方案。 nawf提供三个选项（emu，minimap2lca和blast2lca）来确定微生物组成。 NART安装完整的安装指南NART可在此处获取。 您可以根据您的喜好选择NART使用docker映像安装（这只是MacOS用户的解决方案）或从GitHub存储库安装。 Docker镜像最简单的使用方法是从Docker Hub拉取NART镜像以获得跨平台支持。 docker pull yanhui09/nart NART是通过docker为linux/amd64平台而构建的， MacOS用户需要使用docker容器来运行NART。 从 GitHub 存储库安装1.克隆Github仓库并创建隔离conda环境 git clone https://github.com/yanhui09/nart.gitcd nartmamba env create -n nart -f env.yaml 2.安装NART 为了避免不一致，建议NART在上述conda环境中安装 conda activate nartpip install --editable . 使用NART运行的演示在这里找到完整的使用指南。 可以在此处找到视频教程。 快速启动示例单批次扩增子分析nawf可用于分析Nanopore 运行或批次中的任何单个碱基调用文件。 nawf config -b /path/to/basecall_fastq -d /path/to/database # init config file and checknawf run all # start analysis 实时分析nart提供实用程序来记录、处理和分析连续生成的fastq批次。 在开始实时分析之前，您需要nawf根据需要配置工作流程。 nawf config -d /path/to/database # init config file and check 在常见情况下，您需要三个独立的会话来分别处理监控、处理和可视化。 1.监听 bascall 输出并记录 nart monitor -q /path/to/basecall_fastq_dir # monitor basecall output 2.开始新的 fastq 的扩增子分析 nart run -t 10 # real-time process in batches 3.更新特征表以在浏览器中交互式可视化 nart visual # interactive visualization 熟悉NART使用NART由两组脚本组成：nart和nawf，分别控制实时分析和工作流性能。 如果NART安装在conda环境中，请记住激活环境conda。 conda activate nartnawf -hnart -h 要使用 docker 镜像，您需要将数据目录（例如pwd ）挂载到容器中/home目录。 docker run -it -v `pwd`:/home --network host --privileged yanhui09/nartnawf -hnart -h 注意：--network host需要nart monitor才能正常工作。 使用演示数据集运行NART0.确保您已从此处下载所需的演示数据集。然后进入目录cd。 例如，输入绝对路径（“长路径”）的目录是/home/me/MAC2023-extra。 cd /home/me/MAC2023-extra 如果您尚未下载使用Git下载数据， git clone https://github.com/yanhui09/MAC2023-extra.gitcd ./MAC2023-extra nawf分析已完成的 ONT1.1. 检查您所在的位置并尝试laca init检查生成的config.yaml文件。 pwdnawf config -b ./data/ont16s/*.fastq.gz -d ./database -w ./nart_outputcat ./nart_output/config.yaml 1.2.开始nawf试运行 nawf run all -w ./nart_output -n 2. 实时分析nart2.1. 重新生成config.yaml不带-b标志的文件 rm -rf ./nart_outputnawf config -d ./database -w ./nart_outputhead ./nart_output/config.yaml basecall_fq检查文件中的更改config.yaml。 2.2. 监控 bascalling 输出并记录 nart monitor -q ./data/ont16s -w ./nart_output nart monitor在工作目录中创建一个fqs.txt来记录即将到来的fastq文件。 2.3. 开始扩增子分析（需要新终端） nart run -t 4 -w ./nart_output 在新终端中，检查以下操作nart monitor ls ./nart_outputcat ./nart_output/fqs.txtcp ./data/ont16s/*.fastq.gz ./data/ont16s/new.fastq.gzcat ./nart_output/fqs.txt 检查文件内容fqs.txt的变化。 nart run将特定于批次的计数矩阵存储在文件夹batches下。当创建新矩阵时，合并表 ( otu_table.tsv) 会迭代更新。 当您在输出文件夹中看到一个otu_table.tsv，例如nart_output/，您可以尝试下面的交互式可视化。 2.4. 浏览器中的交互式可视化（需要新终端） nart visual -i ./nart_output 在浏览器中打开生成的链接。您预计会看到如下所示的交互式条形图。 MacOS用户无法通过docker体验nart visual。:( 主机网络驱动程序仅适用于 Linux 主机，在 Docker Desktop for Mac、Docker Desktop for Windows 或 Docker EE for Windows Server 上不受支持。[阅读更多] 本文为教程整理，原作者为YanHui","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"author":"Xiaoguang Pan"},{"title":"使用LACA从长扩增子中从头挑选OTU","slug":"使用LACA从长扩增子中从头挑选OTU","date":"2024-09-20T05:10:53.000Z","updated":"2024-10-18T05:32:50.996Z","comments":true,"path":"laca-16s-long/","permalink":"https://xiaohanys.top/laca-16s-long/","excerpt":"LACA是用于长扩增子一致性分析（例如 16S rRNA 基因扩增子分析）的可重复且可扩展的工作流程。它使用用snakemake管理工作流程以及conda来管理环境。","text":"LACA是用于长扩增子一致性分析（例如 16S rRNA 基因扩增子分析）的可重复且可扩展的工作流程。它使用用snakemake管理工作流程以及conda来管理环境。 LACA的安装完整的安装指南LACA可在此处获取。 您可以根据您的喜好选择使用docker或从GitHub存储库安装LACA Docker镜像最简单的使用方法是从Docker Hub拉取LACA镜像以获得跨平台支持 docker pull yanhui09/laca LACA是通过docker为linux/amd64平台而构建的， MacOS用户需要使用docker容器来运行LACA。 从 GitHub 存储库安装1.克隆Github仓库并创建隔离conda环境 git clone https://github.com/yanhui09/laca.gitcd lacamamba env create -n laca -f env.yaml 2.安装LACA 为了避免不一致，建议在上面建立的conda环境中安装LACA conda activate lacapip install --editable . 使用LACA运行演示数据在这里找到完整的使用指南。 快速启动示例laca init -b /path/to/basecalled_fastqs -d /path/to/database # init config file and checklaca run all # start analysis 熟悉LACA使用LACA很容易使用。您可以使用laca init和laca run分两步开始新的分析。 如果LACA安装在conda环境中，请记住激活conda环境。 conda activate lacalaca -h 要使用 docker 镜像，您需要将数据目录（例如pwd）挂载到容器中/home 目录。 docker run -it -v `pwd`:/home --privileged yanhui09/lacalaca -h 1.初始化配置文件laca init laca init会在工作目录中生成一个配置文件，其中包含运行LACA所需的所有参数。 laca init -h 2.开始laca run分析 laca run将相应地触发完整的工作流程或定义资源下的特定模块。使用laca run -h获得试运行概述。 laca run -h 使用演示数据集运行LACA0.确保您已从此处下载所需的演示数据集。然后cd进入目录。 例如，输入绝对路径（“长路径”）/home/me/MAC2023-extra。 cd /home/me/MAC2023-extra 如果您尚未下载数据用Git下载， git clone https://github.com/yanhui09/MAC2023-extra.gitcd ./MAC2023-extra 1.检查您所在的位置并尝试laca init检查生成的config.yaml文件。 pwdlaca init -b ./data/ont16s -d ./database -w ./laca_output --fqs-min 50cat ./laca_output/config.yaml 2.LACA伪运行和真实运行 laca run all -w ./laca_output -n laca run kmerCon -j 4 -w ./laca_output LACA能够生成otu table，taxonomy table以及phylogenetic tree如果您使用laca run all运行完整的工作流程。但第一次使用需要时间准备数据库和安装。 作为一个例子，这里我们只运行模块kmerCon来根据 kmer 频率提取一致序列。 看看这些共有序列，取第一个序列对rRNA/ITS数据库进行BLAST搜索。 head -n2 ./laca_output/kmerCon/kmerCon.fna 预期输出： &gt;pooled_0b000_0cand1CACAATGGGCGCAAGCCTGATGCAGCGACGCCGCGTGCGGGATGACGGCCTTCGGGTTGTAAACCGCTTTTGACTGGGAGCAAGCCCTTCGGGGTGAGTGTACCTTTCGAATAAGCACCGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGGTGCAAGCGTTATCCGGAATTATTGGGCGTAAAGGGCTCGTAGGCGGTTCGTCGCGTCCGGTGTGAAAGTCCATCGCTTAACGGTGGATCCGCGCCGGGTACGGGCGGGCTTGAGTGCGGTAGGGGAGACTGGAATTCCCGGTGTAACGGTGGAATGTGTAGATATCGGGAAGAACACCAATGGCGAAGGCAGGTCTCTGGGCCGTCACTGACGCTGAGGAGCGAAAGCGTGGGGAGCGAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGGTGGATGCTGGATGTGGGGACCATTCCACGGTCTCCGTGTCGGAGCCAACGCGTTAAGCATCCCGCCTGGGGAGTACGGCCGCAAGGCTAAAACTCAAAGAAATTGACGGGGGCCCGCACAAGCGGCGGAGCATGCGGATTAATTCGATGCAACGCGAAGAACCTTACCTGGGCTTGACATGTTCCCGACAGCCGTAGAGATACGGCCTCCCTTCGGGGCGGGTTCACAGGTGGTGCATGGTCGTCGTCAGCTCGTGTCGTGAGATGTTGGGTTAAGTCCCGCAACGAGCGCAACCCTCGCCCTGTGTTGCCAGCACGTCGTGGTGGGAACTCACGGGGGACCGCCGGGGTCAACTCGGAGGAAGGTGGGGATGACGTCAGATCATCATGCCCCTTACGTCCAGGGCTTCACGCATGCTACAATGGCCGGTACAACGGGATGCGACCTCGCGAGGGGGAGCGGATCCCTTAAAACCGGTCTCAGTTCGGATTGGAGTCTGCAACCCGACTCCATGAAGGCGGAGTCGCTAGTAATCGCGGATCAGCAACGCCGCGGTGAATGCGTTCCCGGGCC 结果BLAST表明，该序列是一个16S rRNA基因片段，Bifidobacterium一性性超过99%。 读取一个 ONT 并进行相同的BLAST搜索。您希望看到什么？ zcat ./data/ont16s/*.fastq.gz | head 本文为学习记录，课程作者为：YanHui","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"author":"Xiaoguang Pan"},{"title":"16S rRNA基因扩增子分析","slug":"16SrRNA基因扩增子分析","date":"2024-09-20T05:10:07.000Z","updated":"2024-10-18T03:36:37.525Z","comments":true,"path":"undefined/","link":"16s-long-analysis","permalink":"https://xiaohanys.top/undefined/","excerpt":"在此工作流程中，介绍了 Qiime2 和 R 中 16S rRNA 基因扩增子数据分析的主要步骤。本教程是为哥本哈根大学食品科学系的 MAC 2023 课程准备的。尽管这些步骤是为 Oxford Nanopore Tech (ONT) 测序设计的，但也在 Ilumina 短读长上进行了测试。","text":"在此工作流程中，介绍了 Qiime2 和 R 中 16S rRNA 基因扩增子数据分析的主要步骤。本教程是为哥本哈根大学食品科学系的 MAC 2023 课程准备的。尽管这些步骤是为 Oxford Nanopore Tech (ONT) 测序设计的，但也在 Ilumina 短读长上进行了测试。 Qiime2中的步骤1. 制作清单文件。该文件对于将原始读数导入 Qiime2 非常重要。该文件只是一个制表符分隔的文本，有两列（如果我们有双端读取）；正向绝对文件路径和反向绝对文件路径。每列包含正向和反向通道每次读取的绝对路径。 #path to forwardls -f ~/microbiome_analysis_ku/data/*_R1_001.fastq.gz | sort -V &gt; r1path#path to reversels -f ~/microbiome_analysis_ku/data/*_R2_001.fastq.gz | sort -V &gt; r2path#checking if we have all pairs of forward and reverse readsdiff -s &lt;(cat r1path | cut -d_ -f4) &lt;(cat r2path | cut -d_ -f4) #if the outcome is &quot;identical&quot;, then we good :)#Cutting sample IDs from the path file (from either is good)cut -d_ -f4 r1path &gt; SampleID#adding sample ids to pathspaste SampleID r1path r2path &gt; manifest#Binning ids and forward reverse columns together with tab-delimited seperationsed -i $&#x27;1 i\\\\\\nsampleid \\t forward-absolute-filepath \\t reverse-absolute-filepath&#x27; manifest 一些数据的推理调查 ## Exploring the readszcat ./data/MAC2023_iSeq001_S1_L001_R2_001.fastq.gz zcat ./data/MAC2023_iSeq001_S1_L001_R2_001.fastq.gz | grep ^@FS10000714| cut -d &quot;:&quot; -f1 | wc -lwc -l ./data/*_R1_001.fastq.gz | awk &#x27;&#123;$1=$1&#125;;1&#x27;| cut -d&quot; &quot; -f1|datamash min 1wc -l ./data/*_R1_001.fastq.gz | awk &#x27;&#123;$1=$1&#125;;1&#x27;| cut -d&quot; &quot; -f1|head -n -1|datamash max 1wc -l ./data/*_R1_001.fastq.gz | awk &#x27;&#123;$1=$1&#125;;1&#x27;| cut -d&quot; &quot; -f1|head -n -1|datamash mean 1#reversewc -l ./data/*_R2_001.fastq.gz| awk &#x27;&#123;$1=$1&#125;;1&#x27; | cut -d &quot; &quot; -f1|datamash min 1wc -l ./data/*_R2_001.fastq.gz| awk &#x27;&#123;$1=$1&#125;;1&#x27; | cut -d &quot; &quot; -f1|datamash max 1wc -l ./data/*_R2_001.fastq.gz| awk &#x27;&#123;$1=$1&#125;;1&#x27; | cut -d &quot; &quot; -f1|datamash mean 1#searching for primerszcat ./data/MAC2023_iSeq001_S1_L001_R2_001.fastq.gz | headzgrep &#x27;^CCTACGGG.GGC.GCAG&#x27; ./data/MAC2022_iSeq001_S1_L001_R2_001.fastq.gz | wc -lzgrep &#x27;^GACTAC..GGGTATCTAATCC&#x27; ./data/MAC2022_iSeq001_S1_L001_R2_001.fastq.gz | wc -l#Eventually you can use Seqkit packageseqkit stats ./data/MAC2023_iSeq001_S1_L001_R2_001.fastq.gz 2. 使用创建的清单文件将读取导入 Qiime2source activate qiime2.Xqiime tools import \\ --type &quot;SampleData[PairedEndSequencesWithQuality]&quot; \\ --input-format PairedEndFastqManifestPhred33V2 \\ --input-path ~/MAC2023/manifest.tsv \\ # link to the folder in which my manifest file is located --output-path ~/MAC2023/demuxed_MAC23.qza # link to the path where I want my demultiplexed data to be exported in 3. 可视化解复用读取的推论统计qiime demux summarize \\ --i-data ./demuxed_mac23.qza \\ --o-visualization ./demuxed_mac23.qzv 4. qiime2中DADA2包的过滤、去重复、样本推断、嵌合体识别、双端读段合并。qiime dada2 denoise-paired \\--i-demultiplexed-seqs ~/demuxed_mac23.qza \\ #the input file for denoising is our demultiplexed pairedEnd reads that was generated in the previous step.--p-trim-left-f 17 \\ #length of my forward primer (17 nt) --p-trim-left-r 21 \\ #length of my reverse primer (21 nt)--p-trunc-len-f 260 \\ #truncation length for forward reads. I.e. we truncate reads over 260 base since their quality started dropping from this point onwards. --p-trunc-len-r 220 \\ #truncation length for reverse reads. I.e. we truncate reads over 220 base since their quality started dropping from this point onwards. --o-table ~/count_table.qza \\ #the ASV count table--o-representative-sequences ~/repseqs.qza \\ #the representative sequences for in each read--o-denoising-stats ~/denoising_stats.qza \\ #the status of the denoising process in a full catalogue--p-n-threads 10 4.1. 可视化计数表qiime feature-table summarize --i-table ~/count_table.qza \\ #The ASV table as the input--m-sample-metadata-file ~/metadataA.tsv \\ #The metadata as the input--o-visualization ./table.qzv #The qzv format of our ASV table 4.2. 可视化代表性序列qiime feature-table tabulate-seqs \\--i-data ~/repseqs.qza \\--o-visualization ~/repseqs.qzv 4.3. 可视化去噪统计数据qiime metadata tabulate \\--m-input-file ./denoising_stats.qza \\--o-visualization ./denoising_stats.qzv 5. 通过朴素贝叶斯方法训练基于引物的区域特异性分类器（在 Qiime2 中）qiime feature-classifier fit-classifier-naive-bayes \\ # here you can train your classifier based on a Naive-Bayes method--i-reference-reads ~/derepseqs-uniq-341f-805r.qza \\ # Dereplicated sequences based on the primer set as input--i-reference-taxonomy ~/dereptaxa-uniq-341f-805r.qza\\ # Dereplicated taxonomic annotations based on the primer set as input --o-classifier ~/silva-classifier-primered4.qza # Running the classifierqiime feature-classifier classify-sklearn \\ # Sklearn package for classification--i-reads ~/repseqs.qza \\ # Representative sequences as the (input)--i-classifier ~/classifier/silva138-classifier-341f-805r.qza \\ # Our costumized SILVA 138 classifier (input)--o-classification ~/taxonomy.qza \\ # The taxonomic annotation linked to the repseqs (output)--p-n-jobs 10 # The number of cores/jobs 5.1. 可视化分类表qiime metadata tabulate \\--m-input-file ~/Taxonomy/taxonomy.qza \\--o-visualization ~/Taxonomy/taxonomy.qzv 6. 使用支持 SATE 的系统发育放置 (SEPP) 方法创建系统发育树以进行多样性分析qiime fragment-insertion sepp \\ # The package for generating the phylogenetic tree--i-representative-sequences ~/repseqs.qza \\ # Our representative sequences (input)--i-reference-database ~/epp-ref-gg-13-8.qza \\ # The SEPP sequence dataset (input)--o-tree ~/tree.qza \\ # The rooted tree (output)--o-placements ~/dss/tree-placements.qza \\--p-threads 10 R 中的步骤1.安装和加载库#Update Rinstall.packages(&#x27;installr&#x27;)installr::updateR()github.pkg &lt;- c(&quot;jfukuyama/phyloseqGraphTest&quot;, &quot;jbisanz/qiime2R&quot;) bioc.pkg &lt;- c(&quot;phyloseq&quot;, &quot;DESeq2&quot;, &quot;MicrobiotaProcess&quot;, &quot;ggtree&quot;, &quot;vsn&quot;)cran.pkg &lt;- c(&quot;tidyverse&quot;, &quot;readr&quot;, &quot;ape&quot;, &quot;pacman&quot;, &quot;picante&quot;, &quot;glue&quot;, &quot;vegan&quot;, &quot;devtools&quot;, &quot;ggrepel&quot;, &quot;reshape2&quot;, &quot;BiocManager&quot;, &quot;ggnetwork&quot;, &quot;DT&quot;, &quot;VennDiagram&quot;, &quot;lsmeans&quot;, &quot;pheatmap&quot;, &quot;phyloseqGraphTest&quot;)inst.pkg &lt;- cran.pkg %in% installed.packages()BiocManager::install(&quot;ggtree&quot;)if (any(!inst.pkg))&#123; install.packages(cran.pkg[!inst.pkg],repos = &quot;http://cran.rstudio.com/&quot;) &#125; inst.pkg &lt;- github.pkg %in% installed.packages() if (any(!inst.pkg))&#123; devtools::install_github(github.pkg[!inst.pkg], force = TRUE) &#125; inst.pkg &lt;- bioc.pkg %in% installed.packages() if(any(!inst.pkg))&#123; BiocManager::install(bioc.pkg[!inst.pkg])&#125; pkg = c(&quot;tidyverse&quot;, &quot;phyloseq&quot;, &quot;DESeq2&quot;, &quot;qiime2R&quot; , &quot;gridExtra&quot;, &quot;BiocManager&quot;, &quot;vegan&quot;, &quot;ggtree&quot;, &quot;devtools&quot;, &quot;ggrepel&quot;, &quot;reshape2&quot;, &quot;ggnetwork&quot;, &quot;igraph&quot;, &quot;biomformat&quot;, &quot;pheatmap&quot;, &quot;glue&quot;, &quot;ape&quot;, &quot;readr&quot;, &quot;vsn&quot;, &quot;vegan&quot;)for(i in 1:length(pkg))&#123; library(pkg[i], character.only = TRUE, verbose = FALSE, attach.required = FALSE)&#125; save.image(&#x27;./data/r.RData&#x27;)load(&quot;./data/r.RData&quot;) 2.从qiime2导入工件到r#making phyloseq objects from qiime filesps &lt;- qiime2R::qza_to_phyloseq(features = &quot;~/data/ccd/table-ccd.qza&quot;, taxonomy = &quot;~/data/ccd/taxonomy-ccd.qza&quot;, tree = &quot;./tree-ccd.qza&quot;)repseqs &lt;- qiime2R::read_qza(&quot;~/data/ccd/repseqa.qza&quot;)$data#we need to merge the refseqs like this since in above fun, there is no argument for thatps= merge_phyloseq(ps, repseqs)# Metadata#importing the metadatametadata &lt;- read.table(&quot;./metadata.tsv&quot;, header = TRUE, sep = &quot;\\t&quot;)#converting non numeric and non-logical variables to factorsfor(i in seq_len(ncol(metadata))) &#123; if(!is.numeric(metadata[[i]]) &amp;&amp; !is.logical(metadata[[i]]) &amp;&amp; !is.integer(metadata[[i]])) &#123; metadata[[i]] = as.factor(metadata[[i]]) &#125; else &#123; metadata[[i]] &#125; &#125;#changing sample namesasvs &lt;- otu_table(ps) %&gt;% as.matrix#Merging all artifactspst = phyloseq(otu_table(asvs, taxa_are_rows = TRUE), phy_tree(ps), sample_data(metadata), refseq(ps), tax_table(ps)) 将 Illumina 输出导入 r ft = read_tsv(&#x27;./MAC2023.github.io/data/illumina/Results_MAC23_MAC96_S96/OTU-tables/zOTU_table_GG.txt&#x27; ) %&gt;% data.frame()ft &lt;- column_to_rownames(ft, &quot;X.OTU.ID&quot;)tx = ft %&gt;% as.data.frame() %&gt;% select(81)ft[,dim(ft)[2]] &lt;-NULLtx = tidyr::separate(tx, col = taxonomy, into =c(&quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;), sep = &#x27;;&#x27;) %&gt;% as.matrix()tx = apply(tx, 2, function(x) &#123;gsub(&quot;^.__&quot;, &quot;&quot;, x)&#125;)colnames(ft) &lt;- sapply(colnames(ft), function(x) &#123;gsub(&quot;.*[S]([0-9]+)$&quot;, &quot;BRK\\\\1&quot;, x)&#125;) %&gt;% as.vector()mt = read.table(&#x27;./data/metadata.txt&#x27;, header = TRUE) mtdata.frame(barcode = colnames(ft))mt[!rownames(mt) %in% colnames(ft),] %&gt;% select(barcode) %&gt;% pull()tr$tip.label %&gt;% length()tr = ggtree::read.tree(&#x27;./MAC2023.github.io/data/illumina/Results_MAC23_MAC96_S96/zOTU.tree&#x27;) phyloseq(otu_table(ft, TRUE), phy_tree(tr), tax_table(tx)) %&gt;% ggtree(layout = &quot;circular&quot;, aes(color = Phylum)) 将 ONT 输出导入 R #setwd(&#x27;/home/hackion/Dropbox/Old-2gb/postdoc/mac2023&#x27;)#Reading feature tableft = read_tsv(&#x27;./data/count_matrix.tsv&#x27;) %&gt;% as.data.frame() %&gt;% column_to_rownames(&#x27;#OTU ID&#x27;) %&gt;% as.matrix()# Reading taxonomy tabletx = read.table(&#x27;./data/taxonomy.tsv&#x27;, header = F, sep = &#x27;\\t&#x27;, row.names = 1)dim(tx) ## [1] 5056 1 # Parse the taxonomytx = tidyr::separate(data = tx, col = V2, into = c(&quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;), sep = &#x27;;&#x27;) #removing taxon tags tx = apply(tx, 2, function(x) &#123;gsub(&quot;^.__&quot;, &quot;&quot;, x)&#125;)#reading metadatamt = read.table(&#x27;./data/metadata.txt&#x27;, header = TRUE) # Reading rep_seqsseqs = Biostrings::readDNAStringSet(&#x27;./data/rep_seqs.fasta&#x27;)# Reading the treetr = ggtree::read.tree(&#x27;./data/tree.nwk&#x27;)# Merging datapst = phyloseq(otu_table(ft, taxa_are_row=TRUE), phyloseq::tax_table(tx), phy_tree(tr), sample_data(mt), refseq(seqs))pst ## phyloseq-class experiment-level object## otu_table() OTU Table: [ 5056 taxa and 95 samples ]## sample_data() Sample Data: [ 95 samples by 2 sample variables ]## tax_table() Taxonomy Table: [ 5056 taxa by 7 taxonomic ranks ]## phy_tree() Phylogenetic Tree: [ 5056 tips and 4980 internal nodes ]## refseq() DNAStringSet: [ 5056 reference sequences ] sum(rowSums(pst@otu_table)==0) ## [1] 0 3、reads的过滤和预处理#removing unassigned and NA phylum taxapst &lt;- subset_taxa(pst, !is.na(Phylum) &amp; !Phylum %in% c(&quot;&quot;, &quot;uncharacterized&quot;, &quot;unassigned&quot;))#keeping only bacterial kingdompst &lt;- subset_taxa(pst, Kingdom %in% &quot;Bacteria&quot;)pst ## phyloseq-class experiment-level object## otu_table() OTU Table: [ 4858 taxa and 95 samples ]## sample_data() Sample Data: [ 95 samples by 2 sample variables ]## tax_table() Taxonomy Table: [ 4858 taxa by 7 taxonomic ranks ]## phy_tree() Phylogenetic Tree: [ 4858 tips and 4784 internal nodes ]## refseq() DNAStringSet: [ 4858 reference sequences ] 基于（低）流行率的分类过滤：监督 #monitoring the number of the samples in which the prevalence of a taxon is at least oneprevdf &lt;- apply(otu_table(pst),ifelse(taxa_are_rows(pst), 1, 2), function(x)&#123;sum(x&gt;0)&#125;)weight_df &lt;- data.frame(ASVprev = prevdf, TaxaAbund = taxa_sums(pst), tax_table(pst))head(weight_df) ## ASVprev TaxaAbund Kingdom Phylum Class## OTU_1133 14 22 Bacteria Actinobacteriota Actinobacteria## OTU_3649 28 50 Bacteria Actinobacteriota Actinobacteria## OTU_3778 15 30 Bacteria Actinobacteriota Actinobacteria## OTU_614 70 934 Bacteria Actinobacteriota Actinobacteria## OTU_1186 51 110 Bacteria Firmicutes Clostridia## OTU_4237 22 35 Bacteria Firmicutes Clostridia## Order Family Genus## OTU_1133 Bifidobacteriales Bifidobacteriaceae Bifidobacterium## OTU_3649 Bifidobacteriales Bifidobacteriaceae Bifidobacterium## OTU_3778 Bifidobacteriales Bifidobacteriaceae Bifidobacterium## OTU_614 Bifidobacteriales Bifidobacteriaceae Bifidobacterium## OTU_1186 Lachnospirales Lachnospiraceae Blautia## OTU_4237 Lachnospirales Lachnospiraceae Blautia## Species## OTU_1133 Bifidobacterium_adolescentis## OTU_3649 Bifidobacterium_longum## OTU_3778 Bifidobacterium_bifidum## OTU_614 Bifidobacterium_adolescentis## OTU_1186 &lt;NA&gt;## OTU_4237 &lt;NA&gt; #Find out the phyla that are of mostly low-prevalence features by computing the total and average prev of features in Phylumplyr::ddply(weight_df, &quot;Phylum&quot;, function(x)&#123;cbind(means = round(mean(x$ASVprev), 2), sums = round(sum(x$ASVprev),2))&#125;) %&gt;% mutate(sanity = ifelse(means == sums, &quot;TRUE&quot;, &quot;FALSE&quot;)) ## Phylum means sums sanity## 1 Actinobacteriota 40.19 17645 FALSE## 2 Bacteroidota 29.43 25282 FALSE## 3 Cyanobacteria 20.25 486 FALSE## 4 Desulfobacterota 22.89 435 FALSE## 5 Firmicutes 35.80 116352 FALSE## 6 Fusobacteriota 24.02 985 FALSE## 7 Proteobacteria 26.01 4941 FALSE## 8 Spirochaetota 9.80 49 FALSE## 9 Verrucomicrobiota 35.52 1101 FALSE 根据所有样本中 n 个样本中出现的流行阈值过滤 ASV # This means that each ASV should have appeared at least in n samples to be kept.asv.filter = function(asvtab, n.samples = 1 )&#123; filter.threshold &lt;- n.samples/ncol(asvtab) *100 # In how many samples out of total samples an ASV should have occured table_count &lt;- apply(otu_table(asvtab), 2, function(x) ifelse(x&gt;0, 1, 0)) %&gt;% as.data.frame() suspected_ASV = table_count[which((rowSums(table_count)/ncol(table_count))*100 &lt; filter.threshold),] %&gt;% rownames() return(suspected_ASV)&#125;print(glue(&quot;&#123; asv.filter(asvtab = otu_table(pst), n.samples = 2) &#125; occured only in two samples&quot;))sus_ASV = asv.filter(asvtab = otu_table(pst), n.samples = 2) ## Will you remove them?#pst = subset_taxa(pst, !taxa_names(pst) %in% sus_ASV)#or you could also do it by the phyloseq functioncondition &lt;- function(x) x&gt;0TaxaTokeep &lt;- genefilter_sample(pst,condition,2) #pst = subset_taxa(pst, taxa_names(pst) %in% TaxaTokeep) 根据丰度删除单调 #A function to find singletones. You need to be careful about this step!out.ASV = function(phyloseq, threshold =1, binwidth = 0.01) &#123; #Loading necessary pkgs pacman::p_load(glue, tidyverse, reshape2, ggrepel, S4Vectors) # nolint#This function requires phyloseq, tidyverse and glue packages to be loaded. if (sum(colSums(otu_table(phyloseq)))/ncol(otu_table(phyloseq)) == 100 ) &#123;#making the relative abundance table rel_abund = as(t(otu_table(phyloseq)), &quot;matrix&quot;) &#125; else if (sum(colSums(otu_table(phyloseq)))/ncol(otu_table(phyloseq)) == 1) &#123; rel_abund = as(t(otu_table(phyloseq)), &quot;matrix&quot;) &#125; else &#123; rel_abund = as(t(apply(otu_table(phyloseq), ifelse(taxa_are_rows(phyloseq), 1,2), function(x) x/sum(x))), &quot;matrix&quot;) &#125; names.single = apply(rel_abund, 1, function(x)&#123;ifelse(x == threshold, TRUE, ifelse(x == sum(x), TRUE, FALSE))&#125;) %&gt;% reshape2::melt() %&gt;% filter(value == TRUE) %&gt;% dplyr::select(2) %&gt;% pull%&gt;% as.vector() if (length(names.single) == 0 ) &#123; print(glue(&quot;WOW! &#123;length(names.single)&#125; singletones detected in this dataset&quot;)) qplot.noSing = qplot(rel_abund, geom = &quot;histogram&quot;, binwidth = binwidth, show.legend = F, main = &quot;Frequency count of relative abundance, no singletones detected&quot;) + xlab (&quot;Relative abundance in samples&quot;) + ylab(&quot;Frequency&quot;) + theme_bw() return(structure(list(qplot.noSing))) &#125; else &#123; single.ASV = rel_abund[rownames(rel_abund) %in% names.single,]single.ASV[single.ASV == 0] &lt;- NA # A separate dataset for annotation of singletones on the barplot qplot.withSing = qplot(rel_abund, geom = &quot;histogram&quot;, binwidth = binwidth, main = &quot;Frequency count of relative abundance with singletones&quot;) + geom_bar(aes(single.ASV), fill = &quot;red&quot;, color = NA, width = binwidth)+ xlab (&quot;Relative abundance in samples&quot;) + ylab(&quot;Frequency&quot;) + geom_label_repel(aes(x = 1, y =length(rel_abund)/5), label.padding = unit(0.55, &quot;lines&quot;), label = glue(&quot;&#123;length(names.single)&#125;\\n Singletones&quot;), color = &quot;black&quot;) + theme_bw() qplot.rmSing = qplot(rel_abund[!rownames(rel_abund) %in% names.single, ], geom = &quot;histogram&quot;, binwidth = binwidth, main = &quot;Frequency count of relative abundance without singletones&quot;) + xlab (&quot;Relative abundance in samples&quot;) + ylab(&quot;Frequency&quot;)+ theme_bw() print(glue(&#x27;Oh no..! &#123;length(names.single)&#125; singletones detected in the dataset&#x27;)) return(structure(list(qplot.withSing, qplot.rmSing, unlist(names.single))) ) &#125; &#125; single.test = out.ASV(phyloseq = pst, threshold = 2, binwidth = 0.01)#singletones = single.test[[3]] #here you can extract the names of the singletonessingle.test[[1]]#to show the plot with singletonessingle.test[[2]]#to show the plot without singletones#Now you can remove the singletones from your pst file as follows:#pst = subset_taxa(pst, !taxa_names(ps)%in% singletones)rm(single.test) 4.阿尔法多样性4.1. 稀疏#library(MicrobiotaProcess)#This takes a bit of timeps_rar_curve &lt;- MicrobiotaProcess::ggrarecurve(obj = pst, indexNames = c(&quot;Observe&quot;, &quot;Shannon&quot;), chunks=400, theme(legend.spacing.y = unit(0.02, &quot;cm&quot;), legend.text = element_text(size = 6)), show.legend=F) ps_rar_curve + theme_bw() + geom_vline(xintercept = 30000, lty = 2, color = alpha(&quot;red&quot;, 0.5)) + ggtitle(&quot;Rarefaction curves based on Richnessh and evenness&quot;, &quot;Each line is a sample&quot;) #ggsave(&quot;./Alpha/rarefaction.curve.jpeg&quot;, device = &quot;png&quot;, dpi = 300, height = 6, width = 9)#rarefying the table with minimum depth of 10000 reads per sampleps_rar = rarefy_even_depth(pst, sample.size = 30000, replace = FALSE)#in this depth we have lost one sapmle (F.29) and no ASVs.# Taxonomic filtering based on abundance for rarefied data: supervsed## Abumdance: ASV &gt; 0.0001% overall abundance across all samplestotal.depth &lt;- sum(otu_table(ps_rar))totAbuThreshold &lt;- 1e-4 * total.depthps_rar &lt;- prune_taxa(taxa_sums(ps_rar)&gt;totAbuThreshold, ps_rar)ps_rar ## phyloseq-class experiment-level object## otu_table() OTU Table: [ 1221 taxa and 87 samples ]## sample_data() Sample Data: [ 87 samples by 2 sample variables ]## tax_table() Taxonomy Table: [ 1221 taxa by 7 taxonomic ranks ]## phy_tree() Phylogenetic Tree: [ 1221 tips and 1214 internal nodes ]## refseq() DNAStringSet: [ 1221 reference sequences ] sum(rowSums(ps_rar@otu_table)==0) ## [1] 0 4.2. Alpha 多样性指标#Calculating the alpha diversity indexes## RichnessChao1 = estimate_richness(pst, split = TRUE, measures = &quot;Chao1&quot;)#for richness, we don&#x27;t use rarefied table## Evenness: shannon =&gt; H&#x27; = -Σ (Pi * ln(Pi))Shannon = estimate_richness(ps_rar, split = TRUE, measures = &quot;Shannon&quot;)#Faith Phylogenetic Diversity: PD = Σ(branch lengths leading to selected species)library(picante, verbose = FALSE)FaithPD = picante::pd(samp = t(otu_table(ps_rar)), tree = phy_tree(ps_rar))$PD#adding the indexes to the metadatas sample_data(ps_rar) &lt;- data.frame(sample_data(ps_rar), Chao1=Chao1[rownames(Chao1)%in% rownames(sample_data(ps_rar)),][[1]], Shannon = Shannon$Shannon, FaithPD = FaithPD) #note that we have removed that sample which has been removed by rarefaction. 4.3.可视化α多样性指数library(ggpubr, verbose = FALSE)library(reshape2, verbose = FALSE)alpha_ccd = sample_data(ps_rar) %&gt;% data.framelong_mtdat &lt;- melt(alpha_ccd, )long_mtdat&lt;- long_mtdat[long_mtdat$variable %in% c(&quot;Chao1&quot;, &quot;Shannon&quot;, &quot;FaithPD&quot;),]long_mtdat$variable &lt;- factor(long_mtdat$variable , levels = c(&quot;Chao1&quot;, &quot;Shannon&quot;, &quot;FaithPD&quot;))pie(rep(10, 9), col = colors()[c(190, 20, 30, 15, 8, 90, 35, 120, 42)], clockwise = TRUE, labels = as.character(colors()[c(1, 20, 30, 15, 8, 90, 35, 120, 42)])) cols = colors()[c(190, 20, 30, 15, 8, 90, 35, 120, 42)]my_comp = list(c(&quot;A&quot;, &quot;B&quot;), c(&quot;A&quot;, &quot;C&quot;), c(&quot;A&quot;, &quot;E&quot;), c(&quot;A&quot;, &quot;Mock&quot;), c(&quot;A&quot;, &quot;B&quot;))#comparison between Samples by SampleID and digestaalpha_p = ggplot(long_mtdat, aes(x = group, y = value)) + geom_violin(aes(fill = group), trim = F) + stat_compare_means(paired = FALSE, comparison = my_comp, method = &quot;t.test&quot;, label = &quot;p.signif&quot;) + geom_boxplot(width = 0.10) + geom_jitter(color = &quot;black&quot;, alpha = 0.5)+ facet_wrap(~long_mtdat$variable, scales = &quot;free_y&quot;) + theme_bw() + scale_fill_manual(values = cols) + theme(legend.title = element_text( size = 15, face = &quot;bold&quot;), axis.title.x = element_text( face = &quot;bold&quot;, size = 15), axis.text.x = element_text( size = 15), axis.title.y = element_text( face = &quot;bold&quot;, size = 15), axis.text.y = element_text( size = 15), strip.text.x = element_text( size = 15, face = &quot;bold&quot;)) + labs(fill = &quot;Sample type&quot;, y = &quot;Alpha diversity&quot;, title = &quot;Alpha diversity metrics in different group samples.\\n Means are compared by unpaired t.test&quot;) + xlab(&quot;Sample type&quot;)#ggsave(plot = alpha_p, &quot;./Alpha/alpha.jpeg&quot;, device = &quot;jpeg&quot;, width = 15, dpi =300)alpha_p lm1 = lm(Chao1 ~ group, data = alpha_ccd)summary(lm1) ## ## Call:## lm(formula = Chao1 ~ group, data = alpha_ccd)## ## Residuals:## Min 1Q Median 3Q Max ## -679.86 -196.49 40.58 234.99 533.47 ## ## Coefficients:## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2588.961 92.412 28.015 &lt; 2e-16 ***## groupB 335.459 130.690 2.567 0.012177 * ## groupC -31.262 133.628 -0.234 0.815637 ## groupD 5.494 130.690 0.042 0.966577 ## groupE 535.158 130.690 4.095 0.000102 ***## groupF 169.873 130.690 1.300 0.197493 ## groupG -802.872 170.400 -4.712 1.05e-05 ***## groupH -1066.241 141.162 -7.553 6.86e-11 ***## groupMock -1103.125 244.499 -4.512 2.24e-05 ***## ---## Signif. codes: 0 &#x27;***&#x27; 0.001 &#x27;**&#x27; 0.01 &#x27;*&#x27; 0.05 &#x27;.&#x27; 0.1 &#x27; &#x27; 1## ## Residual standard error: 320.1 on 78 degrees of freedom## Multiple R-squared: 0.7232, Adjusted R-squared: 0.6948 ## F-statistic: 25.47 on 8 and 78 DF, p-value: &lt; 2.2e-16 5. 贝塔多样性计数数据的转换又称为特征缩放 library(&quot;patchwork&quot;, verbose = FALSE)#Untransformedp1 = ggplot() + geom_histogram(aes(x = rowSums(otu_table(pst))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 2000) + theme_bw() + ggtitle(&quot;Histogram for raw&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;)p2 = ggplot() + geom_histogram(aes(x = log10(rowSums(otu_table(pst)))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 0.05) + theme_bw() + ggtitle(&quot;Histogram for raw, Log10T&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;)p1 + p2 + plot_layout(ncol = 2 , heights = 8) #Rarefiedp3 = ggplot() + geom_histogram(aes(x = rowSums(otu_table(ps_rar))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 2000) + theme_bw() + ggtitle(&quot;Histogram for rarefied data&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;)p4 = ggplot() + geom_histogram(aes(x = log10(rowSums(otu_table(ps_rar)))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 0.07) + theme_bw() + ggtitle(&quot;Histogram for raw, rarefied Log10T&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;)p3 + p4 +plot_layout(ncol = 2 , heights = 8) #Relabundps_rel = transform_sample_counts(ps_rar, function(x) &#123;x / sum(x) * 100&#125;)p5 = ggplot() + geom_histogram(aes(x = rowSums(otu_table(ps_rel))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 5) + theme_bw() + ggtitle(&quot;Histogram for relabund data&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;)p6 = ggplot() + geom_histogram(aes(x = log10(rowSums(otu_table(ps_rel)))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 0.07) + theme_bw() + ggtitle(&quot;Histogram for relabund, Log10T&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;) p5 + p6+ plot_layout(ncol = 2 , heights = 8) #Natural logps_log = transform_sample_counts(ps_rar, function(x) (log(1+x)))p7 = ggplot() + geom_histogram(aes(x = rowSums(otu_table(ps_log))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 5) + theme_bw() + ggtitle(&quot;Histogram for log data&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;)p8 = ggplot() + geom_histogram(aes(x = log10(rowSums(otu_table(pst)))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 0.07) + theme_bw() + ggtitle(&quot;Histogram for log, Log10T&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;)p7 + p8 + plot_layout(ncol = 2 , heights = 8) #VSTvst_count = varianceStabilizingTransformation(object = as(otu_table(ps_rar), &quot;matrix&quot;), fitType = &quot;mean&quot;, blind = TRUE)ps_vst = ps_rarotu_table(ps_vst) &lt;- otu_table(vst_count, taxa_are_rows = TRUE)p9 = ggplot() + geom_histogram(aes(x = rowSums(otu_table(ps_vst))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 10) + theme_bw() + ggtitle(&quot;Histogram for VST data&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;)p10 = ggplot() + geom_histogram(aes(x = log10(rowSums(otu_table(ps_vst)))), fill = &quot;#4a8d78&quot;, color = &quot;#ffffff&quot;, binwidth = 0.07) + theme_bw() + ggtitle(&quot;Histogram for VST, Log10T&quot;) + xlab(&quot;Taxa sum&quot;) + ylab(&quot;Count, all samples&quot;)p9 + p10 + plot_layout(ncol = 2 , heights = 8) msd = meanSdPlot(otu_table(ps_rar), plot = FALSE, ranks = TRUE)p11 = msd$gg + ggtitle(&quot;Mean vs. STD for raw counts of taxa&quot;) +theme_bw() + xlab(&quot;Mean of normalized raw counts&quot;) + ylab(&quot;Standard deviation of counts&quot;) +guides(fill = &quot;none&quot;)msd2 = meanSdPlot(otu_table(ps_vst), plot = FALSE, ranks = TRUE)p12 = msd2$gg + ggtitle(&quot;Mean vs. STD for VST counts of taxa&quot;) +theme_bw() + xlab(&quot;Mean of normalized VST counts&quot;) + ylab(&quot;Standard deviation of VST counts&quot;)p11 + p12 + plot_layout(ncol = 2 , heights = 8) # Standardization (x-mean(x)/sd(x))tb = ps_rar@otu_table %&gt;% data.frame()std = apply(tb, 2, function(x)&#123;scale(x)&#125;)rownames(std) &lt;- rownames(tb)qplot(rowSums(std), bins = 100) pca_result = prcomp(std, scale = FALSE)names(pca_result) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; &quot;x&quot;pca_result$center## BRK01 BRK02 BRK03 BRK04 BRK05 ## -4.841882e-18 -4.516532e-18 -1.260552e-17 3.100055e-18 -3.685399e-18 ## BRK06 BRK07 BRK08 BRK09 BRK10 ## 7.618008e-18 2.040467e-17 3.316007e-18 -1.672069e-17 -6.933211e-19 ## BRK11 BRK12 BRK13 BRK14 BRK15 ## -4.578867e-18 1.868983e-17 -9.308688e-18 -9.223444e-18 6.710865e-18 ## BRK16 BRK17 BRK18 BRK19 BRK20 ## 1.265311e-17 3.233746e-17 2.415258e-19 8.945689e-18 1.014977e-17 ## BRK21 BRK22 BRK23 BRK24 BRK25 ## -3.020138e-18 -2.631637e-17 -1.553153e-17 -1.329742e-17 -1.175662e-18 ## BRK26 BRK27 BRK28 BRK29 BRK30 ## 7.176868e-18 -4.081786e-18 -7.600248e-18 -1.728401e-17 1.236790e-17 ## BRK31 BRK32 BRK33 BRK34 BRK36 ## 1.065555e-17 -5.702317e-18 1.599895e-17 -9.921737e-18 -1.142879e-17 ## BRK37 BRK38 BRK39 BRK40 BRK41 ## -1.132330e-18 4.797839e-18 -1.785160e-17 -9.740593e-18 -1.431395e-18 ## BRK42 BRK43 BRK44 BRK45 BRK46 ## 7.954723e-18 2.809868e-18 9.705075e-18 1.118122e-18 -1.449013e-17 ## BRK47 BRK48 BRK49 BRK50 BRK51 ## -1.050495e-17 3.361471e-18 8.628154e-18 -6.707313e-18 6.816000e-18 ## BRK52 BRK53 BRK54 BRK55 BRK56 ## 1.298627e-17 -3.862992e-18 -2.711482e-18 4.148383e-18 1.683044e-18 ## BRK57 BRK58 BRK59 BRK60 BRK61 ## 1.528432e-17 -1.047796e-17 -2.039188e-17 -1.482258e-17 -1.254087e-17 ## BRK62 BRK63 BRK64 BRK65 BRK66 ## 3.474420e-18 3.078743e-17 -1.463362e-17 -1.260125e-17 -1.770952e-18 ## BRK67 BRK68 BRK69 BRK70 BRK71 ## -2.333565e-18 6.753132e-18 1.052697e-17 8.008267e-18 4.469648e-18 ## BRK72 BRK81 BRK82 BRK83 BRK84 ## 1.414045e-17 1.019807e-17 -3.789824e-18 -3.597314e-18 -9.277432e-18 ## BRK85 BRK86 BRK87 BRK88 BRK89 ## 9.331420e-18 4.660027e-19 1.505984e-17 -7.615166e-18 -1.365615e-17 ## BRK90 BRK91 BRK92 BRK93 BRK94 ## 1.325266e-17 9.757642e-18 -3.472288e-18 -8.391956e-18 -1.005031e-17 ## BRK95 BRK96 ## 6.060166e-18 1.130554e-17 pca_df = data.frame(PC1 = pca_result$rotation[,1], PC2 = pca_result$rotation[,2], group = sample_data(ps_rar)$group)explained_variance &lt;- pca_result$sdev^2 / sum(pca_result$sdev^2)pc_labels &lt;- paste0(&quot;PC&quot;, 1:length(explained_variance))var_df &lt;- data.frame(PC = pc_labels, ExplainedVariance = explained_variance)ggplot(pca_df, aes(x = PC1, y = PC2, color = group)) +geom_point(size = 8, alpha = 0.6) +scale_color_manual(values = colors()[c(190, 40, 30, 15, 8, 90, 35, 80, 42)]) + theme_bw() +xlab(glue(&quot;PC1, &#123;round(var_df$ExplainedVariance[1] * 100,1)&#125;%&quot;)) +ylab(glue(&quot;PC1, &#123;round(var_df$ExplainedVariance[2] * 100,1)&#125;%&quot;)) +guides(color = guide_legend(title = &quot;Group&quot;)) +geom_vline(xintercept = 0, lty = 2, color = alpha(&quot;orange&quot;, alpha = 0.7))+geom_hline(yintercept = 0, lty = 2, color = alpha(&quot;orange&quot;, alpha = 0.7)) 5.1. 排序图cols = colors()[c(190, 20, 30, 15, 8, 90, 35, 120, 42)]#bray #bray PCoA: BC = (Σ|X_i - Y_i|) / (Σ(X_i + Y_i))bray_pcoa=ordinate(ps_log, method=&quot;PCoA&quot;, distance = &quot;bray&quot;)evals&lt;-bray_pcoa$values$Eigenvaluesbray_p = plot_ordination(ps_log, bray_pcoa, title = &quot;Bray-Curtis PCoA plot, Log&quot;, color = &quot;white&quot;) + labs(col=&quot;Group&quot;)+ geom_point( aes(fill = group), alpha = 0.75, pch = 21, size = 8, color = &quot;black&quot;, show.legend = FALSE) + #coord_fixed() + #stat_ellipse(aes(group = group, fill = group), # show.legend = F, type = &quot;t&quot;, level = 0.05, lty = 2, geom = &quot;polygon&quot;, alpha = 0.1) + labs(x = sprintf(&quot;PCo1 [%s%%]&quot;, round(evals/sum(evals)*100,1)[1]), y = sprintf(&quot;PCo2 [%s%%]&quot;, round(evals/sum(evals)*100, 2)[2])) + scale_fill_manual(values = cols) + geom_vline(xintercept = 0, lty = 2, alpha = 0.5, color = &quot;blue&quot;) + geom_hline(yintercept = 0, lty = 2, alpha = 0.5, color = &quot;blue&quot;) + theme_bw()+ theme(axis.title = element_text(face = &quot;bold&quot;), legend.title = element_text(size = 10, face = &quot;bold&quot;), legend.text = element_text(face = &quot;bold&quot;), axis.text = element_text(size = 15))#ggsave(&quot;./Beta/bray.pcoa.dig.vs.muc.jpeg&quot;, dpi = 300)wunifrac = ordinate(ps_log, method=&quot;NMDS&quot;, distance=&#x27;wunifrac&#x27;) ## Run 0 stress 0.04758852 ## Run 1 stress 0.05257593 ## Run 2 stress 0.04758856 ## ... Procrustes: rmse 3.155827e-05 max resid 8.339069e-05 ## ... Similar to previous best## Run 3 stress 0.04758858 ## ... Procrustes: rmse 3.471801e-05 max resid 8.939346e-05 ## ... Similar to previous best## Run 4 stress 0.0475886 ## ... Procrustes: rmse 1.71134e-05 max resid 5.518678e-05 ## ... Similar to previous best## Run 5 stress 0.04758853 ## ... Procrustes: rmse 1.978362e-05 max resid 5.187433e-05 ## ... Similar to previous best## Run 6 stress 0.07429898 ## Run 7 stress 0.05257595 ## Run 8 stress 0.07429908 ## Run 9 stress 0.04758852 ## ... Procrustes: rmse 1.726039e-05 max resid 4.740674e-05 ## ... Similar to previous best## Run 10 stress 0.04758851 ## ... New best solution## ... Procrustes: rmse 1.241057e-05 max resid 2.7403e-05 ## ... Similar to previous best## Run 11 stress 0.04758852 ## ... Procrustes: rmse 4.215754e-06 max resid 1.148943e-05 ## ... Similar to previous best## Run 12 stress 0.07429939 ## Run 13 stress 0.04758853 ## ... Procrustes: rmse 8.518784e-06 max resid 2.341394e-05 ## ... Similar to previous best## Run 14 stress 0.04758852 ## ... Procrustes: rmse 7.077508e-06 max resid 1.774362e-05 ## ... Similar to previous best## Run 15 stress 0.04758853 ## ... Procrustes: rmse 1.264987e-05 max resid 3.6171e-05 ## ... Similar to previous best## Run 16 stress 0.04758853 ## ... Procrustes: rmse 3.895438e-05 max resid 0.0001050886 ## ... Similar to previous best## Run 17 stress 0.04758853 ## ... Procrustes: rmse 4.289572e-05 max resid 0.0001150774 ## ... Similar to previous best## Run 18 stress 0.05257595 ## Run 19 stress 0.04758852 ## ... Procrustes: rmse 1.454469e-06 max resid 7.042092e-06 ## ... Similar to previous best## Run 20 stress 0.04758853 ## ... Procrustes: rmse 6.52229e-06 max resid 2.010471e-05 ## ... Similar to previous best## *** Best solution repeated 9 times wunifrac_p = plot_ordination(ps_log, wunifrac, title = &quot;WUNIFRAC NMDS, LogT&quot;) + geom_point( aes(fill = group), pch = 21, alpha = 0.75, color = &quot;black&quot;, size =8, show.legend = TRUE) + #coord_fixed()+ labs(fill=&quot;Group&quot;)+ scale_fill_manual(values = cols) + geom_vline(xintercept = 0, lty = 2, alpha = 0.5, color = &quot;blue&quot;) + geom_hline(yintercept = 0, lty = 2, alpha = 0.5, color = &quot;blue&quot;) + theme_bw()+ theme(axis.title = element_text(face = &quot;bold&quot;), legend.title = element_text(size = 10, face = &quot;bold&quot;), legend.text = element_text(face = &quot;bold&quot;), axis.text = element_text(size = 15))bray_p + wunifrac_p + plot_layout( heights = 8, widths = 15, ncol = 2) + plot_annotation(&quot;Ordination plots of compostional analysis&quot;, &quot;Bray-Curtis dissimilarity and\\n Weighted Unifrac phylogenetic distance&quot;, &quot;You might know this as Beta diversity&quot;) Gloomer：一个包装“tax_glom()”函数并向分类单元添加简洁唯一名称的函数# Gloomer#A function to create unique names for each ASV. If species is set as the taxa level, it removes any NA in Order level then attempts to use the name of one level higher taxa for those who have similar names, e.g. uncultured_bacteriumgloomer = function(ps = data, taxa_level = taxa_level, NArm = &quot;TRUE&quot;)&#123; rank.names = c(&#x27;Kingdom&#x27;,&#x27;Phylum&#x27;, &#x27;Class&#x27;, &#x27;Order&#x27;, &#x27;Family&#x27;, &#x27;Genus&#x27;, &#x27;Species&#x27;) #====================Sometimes in genus level, we might have multiple uncultured organisms, which if we want to make unique out of them for the species level it won&#x27;t work==== #since adding uncultured to uncultered is sill duplication. therefore if the taxa_level is set to species we first make a unique genus and then we go further to the speices===##Removing unculured Familyps = subset_taxa(ps, !Family %in% c(&quot;uncultured&quot;, &quot;NA&quot;, &quot;uncategorized&quot;, &quot;unassigend&quot;, &quot;&quot;, &quot; &quot;)) if(taxa_level == &quot;Species&quot;) &#123; ps = subset_taxa(ps, !Genus %in% NA)#we remove genus tagged NAtax_table(ps)[, taxa_level] &lt;- ifelse(is.na(tax_table(ps)[, taxa_level]), paste0(&quot;unknown&quot;), paste(tax_table(ps)[, taxa_level]))#convert NA in species into unknown physeq = tax_glom(physeq = ps, taxrank = taxa_level, NArm = NArm) taxdat = tax_table(physeq)[, seq_along(rank.names[1:which(rank.names == taxa_level)])] taxdat = taxdat[complete.cases(taxdat),] %&gt;% as.data.frame otudat = otu_table(physeq) #first take care of the uncultured genustaxdat[,6] = ifelse(taxdat[,6] %in% c(&quot;uncategorized&quot;, NA, &quot;uncultured&quot;, &quot;unassigend&quot;, &quot;&quot;, &quot; &quot;), paste0(&quot;[&quot;, taxdat[,length(rank.names[1:which(rank.names==&quot;Genus&quot;)])-1], &quot;]&quot;, &quot;_&quot;, taxdat[,6]), taxdat[,6]) spec1 = taxdat[, taxa_level] %&gt;% as.vectorspec2 = taxdat[, taxa_level] %&gt;% as.vector uni = matrix(NA, ncol = length(spec2), nrow = length(spec1)) for(i in seq_along(spec1))&#123; for(j in seq_along(spec2))&#123; uni[i, j] = ifelse(spec1[i] == spec2[j] , &quot;TRUE&quot;, &quot;FALSE&quot;) &#125; &#125;rownames(uni) &lt;-spec1colnames(uni) &lt;- spec2 uni[upper.tri(uni, diag = TRUE)] = 0 #get rid of diagonals and upper triangleduplis = uni %&gt;% reshape2::melt() %&gt;% filter(value == &quot;TRUE&quot;) if(dim(duplis)[[1]] &gt; 0) &#123;duplis = uni %&gt;% reshape2::melt() %&gt;% filter(value == &quot;TRUE&quot;) %&gt;% dplyr::select(1) %&gt;% unique() %&gt;% unlist %&gt;% as.vectortaxdat = taxdat %&gt;% mutate( uni= ifelse(taxdat[, taxa_level] %in% duplis, paste0(&quot;[&quot;, taxdat[,length(rank.names[1:which(rank.names==taxa_level)])-1], &quot;]&quot;, &quot;_&quot;, taxdat[,taxa_level]), taxdat[,taxa_level]))#check if all the names are unique at species level, otherwise we will bring family instead of genus dupies &lt;- taxdat[duplicated(taxdat[,&quot;uni&quot;]), &quot;uni&quot;] if(length(dupies)&gt;0) &#123; taxdat = taxdat %&gt;% data.frame %&gt;% mutate( uni2= ifelse(taxdat[, &quot;uni&quot;] %in% dupies, paste0(&quot;[&quot;, taxdat[,length(rank.names[1:which(rank.names==taxa_level)])-2], &quot;]&quot;, &quot;_&quot;, taxdat[,&quot;uni&quot;]), taxdat[,&quot;uni&quot;])) taxdat[, taxa_level] = taxdat[, &quot;uni2&quot;] taxdat[, &quot;uni&quot;] &lt;- NULL taxdat[, &quot;uni2&quot;] &lt;- NULL taxdat &lt;- as(taxdat, &quot;matrix&quot;) rownames(otudat) &lt;- taxdat[rownames(taxdat) %in% rownames(otudat), taxa_level] rownames(taxdat) &lt;- taxdat[, taxa_level] taxdat &lt;- tax_table(taxdat) taxa_names(physeq) &lt;- taxa_names(taxdat) tax_table(physeq) &lt;- taxdat otu_table(physeq) &lt;- otudat &#125; else &#123; taxdat[, taxa_level] = taxdat[, &quot;uni&quot;]taxdat[, &quot;uni&quot;] &lt;- NULLtaxdat &lt;- as(taxdat, &quot;matrix&quot;) rownames(otudat) &lt;- taxdat[rownames(taxdat) %in% rownames(otudat), taxa_level]rownames(taxdat) &lt;- taxdat[, taxa_level]taxdat &lt;- tax_table(taxdat)taxa_names(physeq) &lt;- taxa_names(taxdat)tax_table(physeq) &lt;- taxdatotu_table(physeq) &lt;- otudat &#125; &#125; else &#123; taxdat &lt;- as.matrix(taxdat) taxdat &lt;- tax_table(taxdat)rownames(otudat) &lt;- taxdat[rownames(taxdat) %in% rownames(otudat), taxa_level]rownames(taxdat) &lt;- taxdat[, taxa_level]taxdat &lt;- tax_table(taxdat)taxa_names(physeq) &lt;- taxa_names(taxdat)tax_table(physeq) &lt;- taxdatotu_table(physeq) &lt;- otudat &#125; #==========================================# &#125; else if (taxa_level == &quot;Genus&quot;) &#123; physeq = tax_glom(physeq = ps, taxrank = taxa_level, NArm = NArm) taxdat = tax_table(physeq)[, seq_along(rank.names[1:which(rank.names == taxa_level)])] taxdat = taxdat[complete.cases(taxdat),] %&gt;% as.data.frame otudat = otu_table(physeq) # take care of the uncultured genustaxdat[,6] = ifelse(taxdat[,6] %in% c(&quot;uncategorized&quot;, NA, &quot;uncultured&quot;, &quot;unassigend&quot;, &quot;&quot;, &quot; &quot;), paste0(&quot;[&quot;, taxdat[,length(rank.names[1:which(rank.names==taxa_level)])-1], &quot;]&quot;, &quot;_&quot;, taxdat[,taxa_level]), taxdat[,taxa_level]) gen1 = taxdat[, taxa_level] %&gt;% as.vectorgen2 = taxdat[, taxa_level] %&gt;% as.vector uni = matrix(NA, ncol = length(gen2), nrow = length(gen1)) for(i in seq_along(gen1))&#123; for(j in seq_along(gen2))&#123; uni[i, j] = ifelse(gen1[i] == gen2[j] , &quot;TRUE&quot;, &quot;FALSE&quot;) &#125; &#125;rownames(uni) &lt;-gen1colnames(uni) &lt;- gen2 uni[upper.tri(uni, diag = TRUE)] = 0 #get rid of diagonals and upper triangleduplis = uni %&gt;% reshape2::melt() %&gt;% filter(value == &quot;TRUE&quot;) if(dim(duplis)[[1]] &gt; 0)&#123;#if there is not duplications, we can simply use the taxa names as the row name duplis = uni %&gt;% reshape2::melt() %&gt;% filter(value == &quot;TRUE&quot;) %&gt;% dplyr::select(1)%&gt;% unique() %&gt;% unlist %&gt;% as.vector taxdat = taxdat %&gt;% mutate( uni= ifelse(taxdat[, taxa_level] %in% duplis, paste0(&quot;[&quot;, taxdat[,length(rank.names[1:which(rank.names==taxa_level)])-1], &quot;]&quot;, &quot;_&quot;, taxdat[,taxa_level]), taxdat[,taxa_level])) taxdat[, taxa_level] = taxdat[, &quot;uni&quot;] taxdat[, &quot;uni&quot;] &lt;- NULL taxdat &lt;- as(taxdat, &quot;matrix&quot;) rownames(otudat) &lt;- taxdat[rownames(taxdat) %in% rownames(otudat), taxa_level] rownames(taxdat) &lt;- taxdat[taxdat[,taxa_level] %in% rownames(otudat), taxa_level] taxdat &lt;- as.matrix(taxdat) taxdat &lt;- tax_table(taxdat) taxa_names(physeq) &lt;- taxa_names(taxdat) tax_table(physeq) &lt;- taxdat otu_table(physeq) &lt;- otudat &#125; else &#123; taxdat &lt;- as.matrix(taxdat) taxdat &lt;- tax_table(taxdat) rownames(otudat) &lt;- taxdat[rownames(taxdat) %in% rownames(otudat), taxa_level] rownames(taxdat) &lt;- taxdat[, taxa_level] taxdat &lt;- tax_table(taxdat) taxa_names(physeq) &lt;- taxa_names(taxdat) tax_table(physeq) &lt;- taxdat otu_table(physeq) &lt;- otudat &#125; &#125; else &#123; physeq = tax_glom(physeq = ps, taxrank = taxa_level, NArm = TRUE) taxdat = tax_table(physeq)[, seq_along(rank.names[1:which(rank.names == taxa_level)])] taxdat = taxdat[complete.cases(taxdat),] %&gt;% as.data.frameotudat = otu_table(physeq) spec1 = taxdat[, taxa_level] %&gt;% as.vectorspec2 = taxdat[, taxa_level] %&gt;% as.vector uni = matrix(NA, ncol = length(spec2), nrow = length(spec1)) for(i in seq_along(spec1))&#123; for(j in seq_along(spec2))&#123; uni[i, j] = ifelse(spec1[i] == spec2[j] , &quot;TRUE&quot;, &quot;FALSE&quot;) &#125; &#125;rownames(uni) &lt;-spec1colnames(uni) &lt;- spec2 uni[upper.tri(uni, diag = TRUE)] = 0 #get rid of diagonals and upper triangleduplis = uni %&gt;% reshape2::melt() %&gt;% filter(value == &quot;TRUE&quot;)if(dim(duplis)[[1]] &gt; 0)&#123;#if there is not duplications, we can simply use the taxa names as the row name duplis = uni %&gt;% reshape2::melt() %&gt;% filter(value == &quot;TRUE&quot;) %&gt;% dplyr::select(1)%&gt;% unique() %&gt;% unlist %&gt;% as.vectortaxdat = taxdat %&gt;% mutate( uni= ifelse(taxdat[, taxa_level] %in% duplis, paste(taxdat[,length(rank.names[1:which(rank.names==taxa_level)])-1], &quot;_&quot;, taxdat[,taxa_level]), taxdat[,taxa_level])) taxdat[, taxa_level] = taxdat[, &quot;uni&quot;]taxdat[, &quot;uni&quot;] &lt;- NULLtaxdat &lt;- as.matrix(taxdat) rownames(otudat) &lt;- taxdat[rownames(taxdat) %in% rownames(otudat), taxa_level]rownames(taxdat) &lt;- taxdat[, taxa_level]taxdat &lt;- tax_table(taxdat)taxa_names(physeq) &lt;- taxa_names(taxdat)tax_table(physeq) &lt;- taxdatotu_table(physeq) &lt;- otudat&#125; else &#123;taxdat &lt;- as.matrix(taxdat) taxdat &lt;- tax_table(taxdat)rownames(otudat) &lt;- taxdat[rownames(taxdat) %in% rownames(otudat), taxa_level]rownames(taxdat) &lt;- taxdat[, taxa_level]taxdat &lt;- tax_table(taxdat)taxa_names(physeq) &lt;- taxa_names(taxdat)tax_table(physeq) &lt;- taxdatotu_table(physeq) &lt;- otudat&#125;#ps = phyloseq(otu_table(otudat, taxa_are_rows = T), tax_table(as.matrix(taxdat)), sample_data(physeq)) &#125;return(physeq) &#125; 5.2. 基于图表的 beta 多样性分析library(igraph, verbose = FALSE)library(ggnetwork, verbose = FALSE)library(phyloseqGraphTest, verbose = FALSE)#For total none-rarefied datasetpst.spec &lt;- gloomer(ps = pst, taxa_level = &quot;Species&quot;, NArm = TRUE)ps_total = prune_taxa(taxa_sums(pst.spec)&gt;1000, pst.spec)#filtering the taxa based on total sumsample_data(ps_total)$sampleID &lt;- rownames(sample_data(ps_total))net &lt;- make_network(ps_total, directed = FALSE, max.dist = 0.35, distance = &quot;bray&quot;, type = &quot;samples&quot;)sampledata &lt;- sample_data(ps_total) %&gt;% data.framesampledata$sampleID &lt;- rownames(sampledata)V(net)$id &lt;- sampledata[names(V(net)), &quot;group&quot;] %&gt;% as.vectorV(net)$sample &lt;- rownames(sampledata)[rownames(sampledata) %in% names(V(net))] %&gt;% as.vectorV(net)$sample_type &lt;- sampledata[names(V(net)), &quot;group&quot;] %&gt;% as.vector#graph permutational testset.seed(2023)graph.test &lt;- graph_perm_test(ps_total, sampletype = &quot;group&quot;, max.dist = 0.35, grouping = &quot;sampleID&quot;, distance = &quot;bray&quot;, type = &quot;mst&quot;, nperm = 1000) print(glue(&quot;Test statistic for the graph-based analysis indicates that\\n with p-value = &#123;graph.test$pval&#125;, \\n we REJECT the Null hypothesis that the distribution of taxa in samples from different groiups are similar!&quot;)) ## Test statistic for the graph-based analysis indicates that## with p-value = 0.000999000999000999, ## we REJECT the Null hypothesis that the distribution of taxa in samples from different groiups are similar! V(graph.test$net)$group &lt;- sampledata[names(V(graph.test$net)), &quot;group&quot;] %&gt;% as.vectorplotNet1 = phyloseqGraphTest::plot_test_network(graph.test) + theme(legend.text = element_text(size = 8), legend.title = element_text(size = 9)) + geom_nodes(size = 4, aes(color = sampletype))+scale_color_manual(values = cols)plotPerm1=plot_permutations(graph.test, bins = 40) + geom_text(aes(label = glue(&quot;&#123;ifelse(round(graph.test$pval, 2) == 0, &#x27;P &lt; 0.01&#x27;, round(graph.test$pval, 2))&#125;&quot;), x = 65, y =40), color = &quot;red&quot;) + theme_bw()+ ggtitle(&quot;Permutation test for pure edges&quot;, &quot;Bray-Curtis&quot;)grid.arrange(ncol = 2, plotNet1, plotPerm1) + geom_col(inherit.aes = F, color = &quot;red&quot;) ## NULL#ggsave(&quot;./graph_perm_total.jpeg&quot;, plot = net.grid, device = &quot;jpeg&quot;, width = 15, beight = 10, dpi = 300) 5.3. Beta多样性指数的统计分析：基于距离的冗余分析（dbRDA）#Calculating Bray-Curtis dissimilarity coefficeintsbray_log = phyloseq::distance(ps_log, method = &quot;bray&quot;) 5.3.1. 检查质心周围方差的离散程度（方差同源检验）#test for the disperssion of the variance around the centroidslibrary(vegan)library(permute, verbose = FALSE)#set the age of the animal as the random variableset.seed(1990)h &lt;- with(data = data.frame(sample_data(ps_log)), how(blocks = group, nperm = 999))##total data#Now we do a Homogeneity of dispersion testset.seed(10)bray.disp &lt;- vegan::betadisper(bray_log, group = sample_data(ps_log)$group, type = &quot;centroid&quot;)#if the p-value is significant, it means that there is a significant difference in variance for any of the tested levels. perm.test = permutest(bray.disp, permutation =h, pairwise = T)p.val.perm = perm.test$tab$`Pr(&gt;F)`[[1]]disp.centroid = bray.disp$centroids %&gt;% as.data.framedisp.vectors = bray.disp$vectors %&gt;% as.data.frameeig.vals = bray.disp$eig#jpeg( &quot;./Beta/dispersion of variance_bray_total_sample.type.jpeg&quot;, quality = 100)plot(bray.disp, col = cols, bty = &quot;n&quot;, las = 1, main = &quot;Dispersion of variance around the centroids, \\n bray | LogT dataset&quot;, sub=NULL, xlab = sprintf(&quot;PCo1 [%s%%]&quot;, round(eig.vals/sum(eig.vals)*100,1)[1]), ylab = sprintf(&quot;PCo2 [%s%%]&quot;, round(eig.vals/sum(eig.vals)*100,1)[2])); text(glue(&quot;P = &#123;p.val.perm&#125;&quot;), x = -0.1, y = -0.29, cex = 1.5, col = &quot;red&quot;) 5.3.2. 数据库RDA模型#Whole datasetset.seed(2023)h &lt;- with(data = data.frame(sample_data(ps_log)), how(blocks = group, nperm = 999))bray.dbrda = dbrda(t(otu_table(ps_log)) ~ group, dist = &quot;bray&quot;, permutations=h, data = sample_data(ps_log)%&gt;%data.frame)#Sex not sig, so reduced the modelbray.dbrda ## Call: dbrda(formula = t(otu_table(ps_log)) ~ group, data =## sample_data(ps_log) %&gt;% data.frame, distance = &quot;bray&quot;, permutations =## h)## ## Inertia Proportion Rank RealDims## Total 13.5609 1.0000 ## Constrained 8.3821 0.6181 8 8## Unconstrained 5.1788 0.3819 78 62## Inertia is squared Bray distance ## ## Eigenvalues for constrained axes:## dbRDA1 dbRDA2 dbRDA3 dbRDA4 dbRDA5 dbRDA6 dbRDA7 dbRDA8 ## 4.931 1.071 0.857 0.555 0.470 0.343 0.138 0.017 ## ## Eigenvalues for unconstrained axes:## MDS1 MDS2 MDS3 MDS4 MDS5 MDS6 MDS7 MDS8 ## 1.9737 1.1133 0.6623 0.4507 0.1619 0.1129 0.0996 0.0569 ## (Showing 8 of 78 unconstrained eigenvalues) permutest(x = bray.dbrda, by = &quot;terms&quot;, permutations = h) ## ## Permutation test for dbrda under reduced model ## ## Blocks: group ## Permutation: free## Number of permutations: 999## ## Model: dbrda(formula = t(otu_table(ps_log)) ~ group, data =## sample_data(ps_log) %&gt;% data.frame, distance = &quot;bray&quot;, permutations =## h)## Permutation test for all constrained eigenvalues## Df Inertia F Pr(&gt;F)## group 8 8.3821 15.781 1## Residual 78 5.1788 5.3.3. 绘制模型提取图# Make the plot out of the model, which is the variation explained only by the terms #digestascore.site = vegan::scores(bray.dbrda, display = &quot;sites&quot;) %&gt;% as.data.framescore.centroid = vegan::scores(bray.dbrda, display = &quot;cn&quot;)%&gt;% as.data.framerownames(score.centroid)&lt;- levels(sample_data(ps_log)$group)eig.vals = bray.dbrda$CCA$eiginertia.total = bray.dbrda$tot.chi #total variation (inertia) explained. #this number should be used as the denominator for measuring the amount of variance out of totoal variance wxplained by each dbrda#Digestascore.site %&gt;% ggplot(aes(dbRDA1, dbRDA2)) +geom_point(aes(fill = sample_data(ps_log)$group), color = &quot;black&quot;, pch = 21, alpha = 0.5, size =6 ) + geom_hline(yintercept = 0, lty = 2, alpha =0.5) + geom_vline(xintercept = 0, lty = 2, alpha = 0.5)+scale_fill_manual(values = cols) + theme_bw() + scale_y_continuous(na.value = c(-2, 3), n.breaks = 10) +scale_x_continuous(na.value = c(-1, 1), n.breaks = 10) + labs(fill =&quot;Groups&quot;) + xlab(label = paste(&quot;dbRDA1 [&quot;, round(eig.vals[[1]]/sum(eig.vals)*100, 1), &quot;% of fitted and&quot;, round(eig.vals[[1]]/inertia.total*100, 1), &quot;% of total variation]&quot;)) + ylab(label = paste(&quot;dbRDA2 [&quot;, round(eig.vals[[2]]/sum(eig.vals)*100, 1), &quot;% of fitted and&quot;, round(eig.vals[[2]]/inertia.total*100, 1), &quot;of total variation]&quot;)) + theme(axis.title = element_text(size = 10), text = element_text(size = 13, face = &quot;bold&quot;), axis.text.x =element_text(size =10, face = &quot;bold&quot;),axis.text.y =element_text(size =10, face = &quot;bold&quot;)) + ggtitle(label = &quot;dbRDA plot of Bray&quot;, &quot;LogT data&quot;) #ggsave(&quot;./Beta/bray.dbRDA.dig.jpeg&quot;, height = 8, width = 9, dpi =300) 6. 差异丰度分析：DESeq26.1. 堆积条形图#Aglomerating the taxaphyl = gloomer(ps_rar, taxa_level = &quot;Phylum&quot;, NArm = TRUE)spec = gloomer(ps_rar, taxa_level = &quot;Species&quot;, NArm = TRUE)#Barplot of relative abundance of phylum in digestatrans_phyl &lt;- merge_samples(phyl, &quot;group&quot;)relabund_phyl &lt;- transform_sample_counts(trans_phyl, function(x) x / sum(x)*100)relabund_phyl = prune_taxa( taxa_sums(relabund_phyl)&gt;0.0001, relabund_phyl)#choosing colorsphylcol=c( &quot;deepskyblue&quot;,&#x27;springgreen3&#x27;,&#x27;snow3&#x27;,&#x27;burlywood4&#x27;, &#x27;cadetblue&#x27;, &#x27;darkblue&#x27;, &#x27;cornflowerblue&#x27;,&#x27;deeppink2&#x27;,&#x27;orangered&#x27;, &#x27;dimgrey&#x27;, &#x27;red&#x27;,&#x27;limegreen&#x27;, &#x27;cyan1&#x27;,&#x27;darkmagenta&#x27;, &#x27;purple&#x27;, &#x27;cyan4&#x27;, &#x27;gold&#x27;) phy_col = phylcol[1:7] phyl_p &lt;- plot_bar(relabund_phyl, fill=&quot;Phylum&quot;) + scale_fill_manual(values = phylcol) + xlab(&quot;Groups&quot;) + ylab(&quot;Relative abundance, %&quot;)+ ggtitle(label = &quot;Stacked barplot&quot;, &quot;rarefied, relabund data &gt; 0.01%&quot;) + theme_bw() + theme( legend.position = &quot;right&quot;, text = element_text(size =15, face = &quot;bold&quot;)) #+ #coord_flip() phyl_p 6.2. DESeq2请记住使用原始数据，因为 DESEq2 内部会计算大小因子和色散的差异。 phyl = gloomer(pst, taxa_level = &quot;Phylum&quot;, NArm = TRUE)spec = gloomer(pst, taxa_level = &quot;Species&quot;, NArm = TRUE)#converting phylosq to deseqphyl_dds &lt;- phyloseq_to_deseq2(phyl, design = ~ group) spec_dds &lt;- phyloseq_to_deseq2(spec, design = ~ group)#calculate geometric means prior to estimate size factorsgm.mean = function(x, na.rm= TRUE) &#123; exp(sum(log(x[x&gt;0]), na.rm=na.rm)/length(x))&#125;##Phylum levelgeo.mean = apply(counts(phyl_dds), 1, gm.mean)phyl_dds = estimateSizeFactors(phyl_dds, geoMeans = geo.mean)phyl_dds &lt;-DESeq(phyl_dds, test = &quot;Wald&quot;, fitType = &quot;parametric&quot;)#Species levelgeo.mean = apply(counts(spec_dds), 1, gm.mean)spec_dds = estimateSizeFactors(spec_dds, geoMeans = geo.mean)spec_dds &lt;-DESeq(spec_dds, test = &quot;Wald&quot;, fitType = &quot;parametric&quot;) 6.3. 可视化 DESeq 结果：瀑布图和火山图library(ggrepel, verbose = FALSE)#Waterfall plot for phylumsigtabspec = results(spec_dds, contrast = c(&quot;group&quot;, &quot;Mock&quot;, &quot;A&quot;)) %&gt;% data.frame() %&gt;% filter(padj &lt;=0.05, abs(log2FoldChange) &gt; 7 ) sigtabspec$Order &lt;- tax_table(spec)[,4][rownames(tax_table(spec)) %in% rownames(sigtabspec)] sigtabspec$Species &lt;- rownames(sigtabspec)#plotting for the phylum alonespec_col=c( &quot;deepskyblue&quot;,&#x27;springgreen3&#x27;,&#x27;snow3&#x27;,&#x27;burlywood4&#x27;, &#x27;cadetblue&#x27;, &#x27;darkblue&#x27;, &#x27;cornflowerblue&#x27;,&#x27;deeppink2&#x27;,&#x27;orangered&#x27;, &#x27;dimgrey&#x27;, &#x27;red&#x27;,&#x27;limegreen&#x27;, &#x27;cyan1&#x27;,&#x27;darkmagenta&#x27;, &#x27;purple&#x27;, &#x27;cyan4&#x27;, &#x27;gold&#x27;, &#x27;#470e19&#x27;, &#x27;#124435&#x27;, &#x27;#1d723f&#x27;, &#x27;#57e9ff&#x27;) colindex = data.frame(color = spec_col[1:length(unique(sigtabspec$Order))], Order = sort(unique(sigtabspec$Order)))ords = unique(data.frame(sigtabspec$Order)) %&gt;% pullcolors = c()for(i in ords)&#123; colors[i] = colindex[colindex$Order == i,1]&#125;# Phylum orderx = tapply(sigtabspec$log2FoldChange, sigtabspec$Order, function(x) max(x))x = sort(x, TRUE)sigtabspec$Order = factor(as.character(sigtabspec$Order), levels=names(x))#Species reorderx = tapply(sigtabspec$log2FoldChange, sigtabspec$Species, function(x) max(x))x = sort(x, TRUE)sigtabspec$Species = factor(as.character(sigtabspec$Species), levels=names(x))waterfall_p = ggplot(sigtabspec, aes(y=Species, x=log2FoldChange), stroke = 0.5) + geom_vline(xintercept = 0.0, color = &quot;orange&quot;, size = 0.5, lty = 2) + geom_point(aes(fill = Order), alpha = 0.6, size = 10, color = &quot;black&quot;, shape = 21, stroke = 0.5) + theme_bw() + scale_x_continuous(limits = c(-20, 15), n.breaks = 10) + ggtitle(&quot;Log2FoldChange of Species&quot;, &quot;Mock vs. A&quot;) + scale_fill_manual(values = colors[names(colors) %in% sigtabspec$Order]) #ggsave(&quot;./deseq2/mucus/difabund_muc_DiarNoInfl_vs_NoDiar.jpeg&quot;, device = &quot;jpeg&quot;, dpi = 300)waterfall_p rm(sigtabphyl, alpha, x, colors, colindex, phyla) #Volcano plotalpha = 0.05spec_dat = results(spec_dds, contrast = c(&quot;group&quot;, &quot;Mock&quot;, &quot;A&quot;))%&gt;% data.framespec_dat = spec_dat[complete.cases(spec_dat),]spec_dat$Significant = ifelse(spec_dat$padj &lt;= alpha, paste0(&quot;FDR &lt; &quot;, alpha), &quot;Not Sig&quot;) %&gt;% factor(levels = c(&quot;FDR &lt; 0.05&quot;, &quot;Not Sig&quot;)) spec_taxa = tax_table(spec) %&gt;% as.matrixsigtabspec = cbind(as(spec_dat, &quot;data.frame&quot;), as(spec_taxa[rownames(spec_taxa) %in% rownames(spec_dat),], &quot;matrix&quot;))#a costumized color schemephylcol=c(&#x27;coral4&#x27;, &quot;deeppink&quot;,&#x27;brown2&#x27;,&#x27;antiquewhite4&#x27;, &#x27;cornflowerblue&#x27;, &#x27;plum4&#x27;, &#x27;darkgoldenrod3&#x27;,&#x27;aquamarine4&#x27;, &#x27;yellow&#x27;, &#x27;red&#x27;, &#x27;darkblue&#x27;, &#x27;Maroon&#x27;, &#x27;Gray&#x27;, &#x27;steelblue2&#x27;,&#x27;darkgreen&#x27;, &#x27;tomato1&#x27;, &#x27;cyan4&#x27;, &#x27;magenta&#x27;)colindex = data.frame(color = phylcol[1:length(unique(tax_table(spec)[,2]))], phylum = sort(unique(tax_table(spec)[,2])))phyla = unique(data.frame(tax_table(spec)[,2])) %&gt;% pullcolors = c()for(i in phyla)&#123; colors[i] = colindex[colindex$Phylum == i,1]&#125;#filtering out the taxa below 2 LFC#sigtabspec = sigtabspec[abs(sigtabspec$log2FoldChange)&gt;2,]volc_p = sigtabspec %&gt;% group_by(log2FoldChange) %&gt;% arrange(desc(log2FoldChange)) %&gt;% ggplot(aes(x = log2FoldChange, y = -log10(pvalue), label = Genus)) + geom_hline(yintercept = -log10(sigtabspec[sigtabspec$Significant == &quot;Not Sig&quot;,&quot;pvalue&quot;]) %&gt;% max, color = alpha(&quot;red&quot;,0.5), lty = 2) +geom_vline(xintercept = 0, color = alpha(&quot;black&quot;, 0.3)) +geom_point(data = sigtabspec[sigtabspec$Significant == &quot;Not Sig&quot;,], aes(x = log2FoldChange, y = -log10(pvalue)), color = alpha(&quot;darkgreen&quot;, 0.6), size = 2) + theme_bw(base_size = 12) + theme(legend.position= &quot;right&quot;, text = element_text(size = 15, face = &quot;bold&quot;)) + geom_point(data = sigtabspec[sigtabspec$Significant == &quot;FDR &lt; 0.05&quot;,], aes(x = log2FoldChange, y = -log10(pvalue), fill = Phylum), size = 6, alpha = 0.5, color = &quot;black&quot;, shape = 21, stroke = 0.5) + scale_fill_manual(values = colors[names(colors) %in% sigtabspec[sigtabspec$Significant == &quot;FDR &lt; 0.05&quot;, &quot;Phylum&quot;]]) +geom_text_repel( nudge_y = 0.15, nudge_x = -.5, data= top_n(sigtabspec[sigtabspec$Significant == &quot;FDR &lt; 0.05&quot; &amp; sigtabspec$log2FoldChange &lt; -2,], n = -10, wt = pvalue), aes(label = Genus), size = 2.5, box.padding = unit( 0.4, units =&quot;lines&quot;), point.padding = unit(0.4, &quot;lines&quot;), max.overlaps = 20) + geom_text_repel(nudge_y = 0, nudge_x =0.5, data= top_n(sigtabspec[sigtabspec$Significant == &quot;FDR &lt; 0.05&quot; &amp; sigtabspec$log2FoldChange &gt; 2,], -10, pvalue), aes(label = Genus), size = 2, box.padding = unit( 0.4, units =&quot;lines&quot;), point.padding = unit(0.4, &quot;lines&quot;), max.overlaps = 20)+geom_text( aes(x = 4, y =0, label = &quot;Not Sig&quot;), color = &quot;red&quot;, size = 2.5) +geom_text(aes(x = 4, y =5, label = &quot;FDR &lt; 0.05&quot;), color = &quot;red&quot;, size = 2.5)+ ggtitle (label = &quot;Volcano Plot of the top 10 most significant log2FoldChange&quot;, &quot;Species in Mock vs. A groups&quot;) + scale_y_continuous(limits = c(0, 10), n.breaks = 5) + scale_x_continuous(limits = c(-7.5, 6), n.breaks = 10) + guides(size = &quot;none&quot;) volc_p #ggsave(&quot;./deseq2/mucus/volc_gen_DiarNoInfl_vs_NoDiar_muc.jpeg&quot;, device = &quot;jpeg&quot;, dpi = 300) 7. 生物标志物和分类单元数据之间的热图关联#correlation heatmap between biomarker and the Genusgen &lt;- gloomer(ps_rar, taxa_level = &quot;Genus&quot;, NArm = TRUE)gen_log = filter_taxa(gen, function(x) sum(x&gt;0)&gt;0, TRUE) #filtering the zero counts outgen_log = transform_sample_counts(gen_log, function(x) &#123;log(1+x)&#125;)# Creating dummy variables biodf = sample_data(gen_log)biodf = data.frame(biodf, geneA = rnorm(87, mean = 0, sd = 1), geneB = rnorm(87, mean = 0, sd = 0.5), geneC = rnorm(87, mean = 0, sd = 0.005)) %&gt;% select(-c(1:2)) %&gt;% as.matrix() countdf = as.matrix(otu_table(gen_log)) countdf = countdf[!rownames(countdf) %in% c(&quot;Unknown&quot;, &quot;uncultured&quot;, &quot;Uncultured&quot;, &quot;Unassigned&quot;, &quot;NA&quot;),] countdf = t(countdf) countdf = countdf[rownames(countdf) %in% rownames(biodf),] #cor gene taxacor_main = Hmisc::rcorr(countdf, biodf, type = &quot;spearman&quot;)cors = cor_main$rcors = cors[rownames(cors) %in% colnames(countdf), colnames(cors) %in% colnames(biodf)] #rows as taxa, cols as chemical data#calculating the qvalues for the correlationscor.pval = cor_main$P[rownames(cor_main$P) %in% rownames(cors) , colnames(cor_main$P) %in% colnames(cors)] cor.qval = p.adjust(cor.pval, method = &quot;BH&quot;)q.vals = matrix(cor.qval, ncol = ncol(cor.pval), nrow = nrow(cor.pval), dimnames = list(rownames(cor.pval), colnames(cor.pval))) #Adding significance signes to the qval matrix to be used in the heatmap later onq.vals[cor.qval &lt;0.05] = &quot;*&quot;q.vals[cor.qval &gt;= 0.05] = &quot;&quot;library(Biobase, verbose = FALSE)asvdat = as(cors,&quot;matrix&quot;)#in the aassay dataset, we add our correlation matrix instead of the abundance matrixtaxadat = Biobase::AnnotatedDataFrame(data.frame(cors))#taxa tablepdata = cors %&gt;% data.framex = ExpressionSet(assayData = asvdat, featureData = taxadat )#Adding phenotype datapData(x) &lt;- pdata # Filtering based on row standard deviation and choosing the most variable 50 taxalibrary(matrixStats)sds &lt;- rowSds(Biobase::exprs(x))o &lt;- order(sds, decreasing = TRUE)[1:50]h_1 &lt;- hclust(dist(Biobase::exprs(x)[o,]), method = &quot;ward.D2&quot;)h_2 &lt;- hclust(dist(t(Biobase::exprs(x)[o,])), method = &quot;ward.D2&quot;)#making a phylum annotation and it only accepts one column dataframerow.annot = gen_log@tax_table[rownames(gen_log@tax_table) %in% rownames(Biobase::exprs(x)[o,]),2]#making color index for the phylum annotation phylcol=c(&#x27;coral4&#x27;, &#x27;cyan&#x27;,&#x27;#ff00aa&#x27;, &#x27;tomato1&#x27;, &#x27;cornflowerblue&#x27;, &#x27;plum4&#x27;, &#x27;darkgoldenrod3&#x27;,&#x27;aquamarine4&#x27;, &#x27;cadetblue2&#x27;, &#x27;red&#x27;, &#x27;darkblue&#x27;, &#x27;Maroon&#x27;, &#x27;Gray&#x27;, &#x27;steelblue2&#x27;,&#x27;darkmagenta&#x27;, &#x27;antiquewhite4&#x27;, &quot;darkorange&quot;, &#x27;darkgreen&#x27;)set.seed(2)phylcol = sample(phylcol, size = length(unique(row.annot[,1])), replace = F)phyl.col = data.frame(Phylum = unique(row.annot[,1]), phyl.col = phylcol[1:length(unique(row.annot[,1]))])rownames(phyl.col) &lt;- NULLphyl.col = column_to_rownames(phyl.col, &quot;Phylum&quot;) %&gt;% as.matrixlibrary(RColorBrewer, verbose = FALSE) mat = matrix(NA, ncol = 1, nrow = nrow(row.annot))for(i in 1:nrow(row.annot))&#123; mat[i,] = ifelse(row.annot[i,1][[1]] %in% rownames(phyl.col), phyl.col[rownames(phyl.col) %in% row.annot[i,1][[1]] ,1], &quot;NA&quot;)&#125;colnames(mat) &lt;- &quot;col&quot;row.annot = cbind(row.annot, mat) %&gt;% data.frame()pheatmap(Biobase::exprs(x)[o,], annotation_row = row.annot %&gt;% select(1), cellheight = 12, annotation_colors = list( Phylum = phyl.col[,1]), cellwidth = 15, cutree_rows = 4, border_color = NA, fontsize_number = 15, number_color = &quot;black&quot;, display_numbers = q.vals[rownames(q.vals) %in% rownames(Biobase::exprs(x)[o,]), colnames(q.vals) %in% colnames(exprs(x)[o,])], angle_col = 45, Rowv = as.dendrogram(h_1), Colv = as.dendrogram(h_2), cutcluster_rows = T, cluster_cols = F, col = brewer.pal(9, &quot;Reds&quot;), width = 10, height = 12, main = &quot;Spearman correlation of top 50 Genre \\nand 3 Genes, clustered row &quot;) #ggsave(plot = pheat.chem.scfa, &quot;./heatmap/heatmap.SCFA.gen_50.jpeg&quot;, dpi = 750, height = 10, width = 8) 在原假设下，p 值是具有均匀分布的随机值。因此，我们必须执行调整后的 p 值来校正族错误率 (FWER)。FWER 是指同时进行多个假设检验时至少出现一个 I 类错误（误报）的概率。 # Set the parametersnum_simulations &lt;- 10000 # Number of simulated datasetssample_size &lt;- 50 # Sample size for each group# Initialize a vector to store p-valuessimulated_p_values &lt;- numeric(num_simulations)# Simulate data and compute p-valuesfor (i in 1:num_simulations) &#123; # Simulate data under the null hypothesis for two groups group1 &lt;- rnorm(sample_size) group2 &lt;- rnorm(sample_size) # Perform a two-sample t-test on the simulated data test_result &lt;- t.test(group1, group2) # Store the p-value simulated_p_values[i] &lt;- test_result$p.value&#125;# Plot a histogram of simulated p-valueshist(simulated_p_values, breaks = 30, col = &quot;lightblue&quot;, main = &quot;Distribution of Simulated P-values under Null hypothesis&quot;) print(glue(&quot;&#123;sum(simulated_p_values&lt;=0.05)&#125; of the tests out of 10000 test were significant. \\n This indicates that under the Null hypothesis alpha % of times your test will be rendered false positive (type-I error).&quot;)) ## 515 of the tests out of 10000 test were significant. ## This indicates that under the Null hypothesis alpha % of times your test will be rendered false positive (type-I error). 8. 系统发育树可视化library(phytools)library(TDbook)library(ggimage)library(treeio)library(tidyverse)library(tidytree)spec &lt;- gloomer(ps_rar, taxa_level = &quot;Class&quot;)spec &lt;- transform_sample_counts(spec, function(x)&#123;x/sum(x)*100&#125;)spec &lt;- prune_taxa(taxa_sums(spec)&gt;1, spec)taxadf &lt;- as.data.frame(tax_table(spec))color.index &lt;- c(&quot;#771155&quot;, &quot;#AA4488&quot;, &quot;#CC99BB&quot;, &quot;#114477&quot;, &quot;#4477AA&quot;, &quot;#77AADD&quot;, &quot;#117777&quot;, &quot;#44AAAA&quot;, &quot;#77CCCC&quot;, &quot;#117744&quot;, &quot;#44AA77&quot;, &quot;#88CCAA&quot;, &quot;#777711&quot;, &quot;#AAAA44&quot;, &quot;#DDDD77&quot;, &quot;#774411&quot;, &quot;#AA7744&quot;, &quot;#DDAA77&quot;, &quot;#771122&quot;, &quot;#AA4455&quot;, &quot;#DD7788&quot;)#creating a highlihgt df with a costum functionmrca.wrap &lt;- function(highlight, taxa.df, tree, tax_level=&quot;Phylum&quot;, type.id = &quot;node&quot;)&#123;taxa &lt;- taxa.df %&gt;% data.frame node.id &lt;- list() for(i in phyls)&#123; node.id[i] &lt;- findMRCA(tree, tree$tip.label[taxa[,tax_level] == i], type = type.id) node.df &lt;- data.frame(phyl = names(node.id), node.id = c(node.id[1][[1]]))&#125; node.id = node.id = as(node.id,&quot;matrix&quot;) node.df = data.frame(highlights = rownames(node.id), node.id = unlist(node.id)) return(node.df) &#125;ps_mock = phyloseq::subset_samples(spec, group == &quot;Mock&quot;) phyls = tax_table(ps_mock)[rowSums(ps_mock@otu_table)&gt;0,2] %&gt;% data.frame() %&gt;% distinct() %&gt;% pull()#chosoing phylum that that are in Mock samplesnode.df &lt;- mrca.wrap(highlight = phyls, taxa.df = tax_table(spec), tree = phy_tree(spec), tax_level = &quot;Phylum&quot;, type = &quot;node&quot;) #drwing the treetree_p = ggtree(spec, aes(color = Phylum),branch.length = &quot;none&quot;,layout = &quot;circular&quot;,show.legend =TRUE,open.angle = 5,size = 1.5) + ggtitle(&quot;Phylogenetic tree of different Classes.&quot;, &quot;Mock comunity phyla are highligted!&quot;) +geom_tiplab(aes(label=Class, color = Phylum), check.overlap = FALSE, face = &quot;bold&quot;, size = 3, offset = 0.3) +scale_color_manual(values = sample(color.index, length(unique(taxadf$Phylum)), FALSE), breaks = unique(taxadf$Phylum)) + #to remove the NA from the legendgeom_highlight(data = node.df, lwd = 0.25, lty = 3, alpha = 0.1,aes(node = node.id, fill = highlights), extend =0.05, to.bottom = TRUE, align = &quot;right&quot;, show.legend = TRUE) +geom_nodepoint( pch = 21, color = &quot;black&quot;, size = 2, fill = &quot;white&quot;) + geom_nodepoint(aes(subset = node %in% node.df$node.id), pch = 21, size = 3, color = &quot;black&quot;, fill = c(&quot;#00ffff&quot;, &quot;#ff7700&quot;)) +geom_tippoint(size = 1, pch = 21, color = alpha(colour = &quot;white&quot;, alpha = 0.2)) + geom_label(aes(x = branch, label = round(branch.length,2)),label.padding = unit(0.05, &quot;line&quot;), size = 3, inherit.aes = TRUE) +scale_fill_manual(values = c( &quot;#16f476&quot;, &quot;#fc00fc&quot;)) # geom_cladelab(node = node.df$node.id, align = FALSE,# label = node.df$highlights,# offset.text = 0.25, # offset = 16,# barsize = 0.05, # extend = 0.01,# angle =&quot;auto&quot;,# fontface = 2, size = 7) +#Plotting abundance on treerel.df &lt;- psmelt(spec) %&gt;% select(OTU, Abundance, group) %&gt;% group_by( OTU, group) %&gt;% summarise(rel = mean(Abundance), .groups = &quot;drop&quot;) %&gt;% pivot_wider(values_from = &quot;rel&quot;, id_cols = &quot;OTU&quot;, names_from = &quot;group&quot;) %&gt;% column_to_rownames( &quot;OTU&quot;)rescaled.reldf &lt;- apply(rel.df, 2, function(x) &#123;log(x+1)&#125;)library(ggnewscale, verbose = FALSE)p2 &lt;- tree_p + new_scale_fill()p3 &lt;- gheatmap(p2, font.size = 5, rescaled.reldf, offset=6.04, width=.8, colnames_angle=60, colnames_offset_y = -.45, colnames_position = &quot;top&quot;) + scale_fill_viridis_c(option=&quot;C&quot;, name=&quot;Log relative abundance, %&quot;)p3 原文链接和Github存储库位置","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"},{"title":"用VueJS和Fastapi通过websocket实现进度条追踪","slug":"用VueJS和Fastapi通过websocket实现进度条追踪","date":"2024-09-20T05:09:13.000Z","updated":"2024-10-18T05:33:21.867Z","comments":true,"path":"vue-fastapi-websocket/","permalink":"https://xiaohanys.top/vue-fastapi-websocket/","excerpt":"简介在我们的应用小程序中，我们是前后端分离的。前端页面只负责渲染，而后端需要处理数据。但是如果遇到数据量很大的情况下，我们处理起来就很缓慢，如果我们想通过AJAX的方法追踪后台数据变化的进度，需要用到轮询的方案，这个是非常消耗资源的。这里我们用VueJS和Fastapi的小例子演示前端传递数据，后台用10秒处理数据并实时反应进度给前台的实现。","text":"简介在我们的应用小程序中，我们是前后端分离的。前端页面只负责渲染，而后端需要处理数据。但是如果遇到数据量很大的情况下，我们处理起来就很缓慢，如果我们想通过AJAX的方法追踪后台数据变化的进度，需要用到轮询的方案，这个是非常消耗资源的。这里我们用VueJS和Fastapi的小例子演示前端传递数据，后台用10秒处理数据并实时反应进度给前台的实现。 后端Fastapi因为后端写起来相对简单，我们先写个后端 新建my-fastapi目录，然后新建main.py文件，代码如下所示： from time import sleepfrom fastapi import FastAPI, WebSocket ##导入websocket##创建appapp = FastAPI()##路由端点需要websocket@app.websocket(&quot;/ws&quot;)async def websocket_endpoint(websocket: WebSocket): await websocket.accept() ## 等待链接 while True: data = await websocket.receive_json() ###等待接收Json数据 ### 这是一个10S的任务 for i in range(10): sleep(1) ### 将此刻的循环次数传递给前端 await websocket.send_json(&#123;&quot;data&quot;: &quot;&quot;, &quot;time&quot;: (i+1)*10&#125;) jieguo = data[&#x27;input&#x27;] + &quot; finished!&quot; ### 等待返回数据 await websocket.send_json(&#123;&quot;data&quot;: jieguo, &quot;time&quot;: 100&#125;) 前端前端用Vuejs+NaiveUI实现： 创建一个Vite项目： npm create vite@latest my-vue-app -- --template vue 安装 Naive-UI npm i -D naive-ui vfonts 将src/app.vue 中的内容删除，替换成下面的： &lt;script setup&gt;///UIimport &#123; NButton, NInput, NSpace, NH1, NProgress, useThemeVars &#125; from &#x27;naive-ui&#x27;import &#123; onMounted, ref &#125; from &#x27;vue&#x27;import &#123; changeColor &#125; from &quot;seemly&quot;;/// 绑定数据const data = ref(&#x27;&#x27;)const tm = ref(&#x27;&#x27;)const inputData = ref(&#x27;&#x27;)/// 建立持久通信const connection = new WebSocket(&quot;ws://localhost:8000/ws&quot;)const themeVars = useThemeVars()///这里用来向后端发送Json数据，注意要转换为字符串格式 JSON.stringify()const submit = () =&gt; &#123; connection.send(JSON.stringify(&#123; input: inputData.value &#125;))&#125;/// 这里接收数据，注意要把字符串格式的JSON解析为json对象 JSON.parse()onMounted(() =&gt; &#123; connection.onmessage = function (e) &#123; const backData = JSON.parse(e.data) data.value = backData.data tm.value = backData.time &#125;&#125;)&lt;/script&gt;&lt;template&gt; &lt;n-space justify=&quot;center&quot; vertical&gt; &lt;n-h1 style=&quot;text-align: center;&quot;&gt;Hello &#123;&#123; data &#125;&#125;&lt;/n-h1&gt; &lt;n-space justify=&quot;center&quot;&gt; &lt;n-progress style=&quot;width: 300px;&quot; type=&quot;line&quot; indicator-placement=&quot;inside&quot; :color=&quot;themeVars.errorColor&quot; :rail-color=&quot;changeColor(themeVars.errorColor, &#123; alpha: 0.2 &#125;)&quot; :percentage=&quot;tm&quot; /&gt; &lt;/n-space&gt; &lt;n-space style=&quot;display: flex; margin: 10px auto; justify-content: center;&quot;&gt; &lt;n-input v-model:value=&quot;inputData&quot; type=&quot;text&quot; placeholder=&quot;测试输入&quot; /&gt; &lt;n-button type=&quot;primary&quot; @click=&quot;submit&quot;&gt;Submit&lt;/n-button&gt; &lt;/n-space&gt; &lt;/n-space&gt;&lt;/template&gt; 最终的效果如图所示： 这个动图没有录制完整，最终会到100%。","categories":[{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/categories/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"},{"name":"前端","slug":"前端","permalink":"https://xiaohanys.top/tags/%E5%89%8D%E7%AB%AF/"},{"name":"后端","slug":"后端","permalink":"https://xiaohanys.top/tags/%E5%90%8E%E7%AB%AF/"}],"author":"Xiaoguang Pan"},{"title":"生信小白如何用vuejs简单创建一个交互式网页","slug":"生信小白如何用vuejs简单创建一个交互式网页","date":"2024-09-20T05:08:07.000Z","updated":"2024-10-18T05:32:27.495Z","comments":true,"path":"vue-web-app/","permalink":"https://xiaohanys.top/vue-web-app/","excerpt":"什么是Vue.jsVueJS是一个渐进式的前端框架，所谓渐进式的意思就是你可以用它快速完成原型创作，然后在此基础上逐步完善。他可以足够简单，也可以足够完善，那么对于新手小白来说，这简直就是福利！","text":"什么是Vue.jsVueJS是一个渐进式的前端框架，所谓渐进式的意思就是你可以用它快速完成原型创作，然后在此基础上逐步完善。他可以足够简单，也可以足够完善，那么对于新手小白来说，这简直就是福利！ Vuejs 相当于使用nodejs来渲染一个html模板，从而动态的生成我们想要的界面（DOM），类似的功能我们可能在Python中见过，例如Jinja2 或者 Django template. 就是给定一个基础的模板，然后我们只需要把给定的数据输入，就可以得到想要的页面了！ 创建项目工程目录这里我们使用Vite来开发前端页面。创建一个Vue的模板 npm create vite@latest my-vue-app -- --template vue 安装NaiveUI npm i -D naive-ui vfonts 配置NaiveUI 因为NaiveUI是可以任意引用的(Tree shaking),所以我们直接饮用想使用的code即可，无需配置。 创建单页面应用在SRC目录中新建一个Components页面，或者Views 或者 Pages,什么名字都可以，我们把该页面的逻辑放进去。 例如，我们创建了/Components/upload.vue. &lt;script setup&gt;import &#123; ref &#125; from &quot;vue&quot;;import &#123; lyla &#125; from &quot;lyla&quot;;import &#123; useMessage &#125; from &quot;naive-ui&quot;;const message = useMessage();const DataFromServer = ref([]);const sampleName = ref(4);const barcode = ref(23);const chip = ref(27);const lane = ref(28);const datapath = ref(48);const path = ref(&quot;10.2.100.1:/pakpox/pob8d1/&quot;);const code1 = ref(&quot;&quot;);const code2 = ref(&quot;&quot;);const tableUpload = ref();const params = ref(&#123; sampleName: sampleName, barcode: barcode, chip: chip, lane: lane, datapath: datapath, path: path,&#125;);const params2 = ref(&#123; file: &quot;&quot;, sampleName: sampleName, barcode: barcode, chip: chip, lane: lane, datapath: datapath, path: path,&#125;);const customUpload = (&#123; file, data, headers, withCredentials, action, onFinish, onError, onProgress,&#125;) =&gt; &#123; const formData = new FormData(); params2.value.file = file.file; if (data) &#123; Object.keys(data).forEach((key) =&gt; &#123; formData.append(key, data[key]); &#125;); &#125; formData.append(&quot;file&quot;, file.file); lyla .post(action, &#123; withCredentials, headers, body: formData, onUploadProgress: (&#123; percent &#125;) =&gt; &#123; onProgress(&#123; percent: Math.ceil(percent) &#125;); &#125;, &#125;) .then((&#123; json &#125;) =&gt; &#123; if (json.hasBlank == 1) &#123; message.warning(&quot;Upload success, but some blank in table&quot;); &#125; if (json.hasDup == 1) &#123; message.warning(&quot;Upload success, but some duplicate in table&quot;); &#125; DataFromServer.value = json.rawData; code1.value = json.cmd1; code2.value = json.cmd2; onFinish(); &#125;) .catch((error) =&gt; &#123; message.success(error.message); onError(); &#125;);&#125;;const customUpload2 = (&#123; file, data, headers, withCredentials, action, onFinish, onError, onProgress,&#125;) =&gt; &#123; const formData = new FormData(); params2.value.file2 = file.file; if (data) &#123; Object.keys(data).forEach((key) =&gt; &#123; formData.append(key, data[key]); &#125;); &#125; formData.append(&quot;file2&quot;, file.file); lyla .post(action, &#123; withCredentials, headers, body: formData, onUploadProgress: (&#123; percent &#125;) =&gt; &#123; onProgress(&#123; percent: Math.ceil(percent) &#125;); &#125;, &#125;) .then((&#123; json &#125;) =&gt; &#123; if (json.hasBlank == 1) &#123; message.warning(&quot;Upload success, but some blank in table&quot;); &#125; if (json.hasDup == 1) &#123; message.warning(&quot;Upload success, but some duplicate in table&quot;); &#125; DataFromServer.value = json.rawData; code1.value = json.cmd1; code2.value = json.cmd2; onFinish(); &#125;) .catch((error) =&gt; &#123; message.success(error.message); onError(); &#125;);&#125;;const columns = [ &#123; title: &quot;sampleName&quot;, key: &quot;sampleName&quot;, &#125;, &#123; title: &quot;barcode&quot;, key: &quot;barcode&quot;, &#125;, &#123; title: &quot;chip&quot;, key: &quot;chip&quot;, &#125;, &#123; title: &quot;lane&quot;, key: &quot;lane&quot;, &#125;, &#123; title: &quot;dataPath&quot;, key: &quot;dataPath&quot;, width: 100, ellipsis: &#123; tooltip: true, &#125;, &#125;,];const submitdata = () =&gt; &#123; tableUpload.value?.submit();&#125;;&lt;/script&gt;&lt;template&gt; &lt;n-layout&gt; &lt;n-layout-header bordered&gt; &lt;div style=&quot;margin: 10px 12px;&quot;&gt; &lt;span style=&quot;font-size: 1.3rem;font-weight: 500;&quot;&gt;Glims to copy&lt;/span&gt; &lt;/div&gt; &lt;/n-layout-header&gt; &lt;n-layout-content&gt; &lt;div style=&quot;margin-top: 20px;&quot;&gt; &lt;n-grid x-gap=&quot;12&quot; :cols=&quot;12&quot;&gt; &lt;n-gi :span=&quot;1&quot;&gt; &lt;/n-gi&gt; &lt;n-gi :span=&quot;3&quot;&gt; &lt;n-card hoverable&gt; &lt;n-space vertical&gt; &lt;n-p&gt;Excel from Glims...&lt;/n-p&gt; &lt;n-upload ref=&quot;tableUpload&quot; :default-upload=&quot;false&quot; action=&quot;https://glims2excel-1-w4936186.deta.app/upload&quot; response-type=&quot;json&quot; :custom-request=&quot;customUpload&quot; :data=&quot;params&quot; &gt; &lt;n-button style=&quot;height: 35px&quot;&gt;Upload..&lt;/n-button&gt; &lt;/n-upload&gt; &lt;n-p&gt;Index for sampleName&lt;/n-p&gt; &lt;n-input-number v-model:value=&quot;sampleName&quot; clearable /&gt; &lt;n-p&gt;Index for barcode&lt;/n-p&gt; &lt;n-input-number v-model:value=&quot;barcode&quot; clearable /&gt; &lt;n-p&gt;Index for chip&lt;/n-p&gt; &lt;n-input-number v-model:value=&quot;chip&quot; clearable /&gt; &lt;n-p&gt;Index for lane&lt;/n-p&gt; &lt;n-input-number v-model:value=&quot;lane&quot; clearable /&gt; &lt;n-p&gt;Index for dataPath&lt;/n-p&gt; &lt;n-input-number v-model:value=&quot;datapath&quot; clearable /&gt; &lt;n-p&gt;Remoth path&lt;/n-p&gt; &lt;n-input v-model:value=&quot;path&quot; type=&quot;text&quot; /&gt; &lt;n-button type=&quot;error&quot; @click=&quot;submitdata&quot;&gt; Submit! &lt;/n-button&gt; &lt;n-divider dashed&gt; Change Names &lt;/n-divider&gt; &lt;n-upload action=&quot;https://glims2excel-1-w4936186.deta.app/changenames&quot; response-type=&quot;json&quot; :custom-request=&quot;customUpload2&quot; :data=&quot;params2&quot; &gt; &lt;n-button round type=&quot;primary&quot;&gt;Upload..&lt;/n-button&gt; &lt;/n-upload&gt; &lt;/n-space&gt; &lt;/n-card&gt; &lt;/n-gi&gt; &lt;n-gi :span=&quot;7&quot;&gt; &lt;n-space vertical style=&quot;margin-top: 50px&quot;&gt; &lt;n-tabs type=&quot;segment&quot;&gt; &lt;n-tab-pane name=&quot;table&quot; tab=&quot;Table Preview&quot;&gt; &lt;n-data-table :columns=&quot;columns&quot; :data=&quot;DataFromServer&quot; max-height=&quot;600px&quot; /&gt; &lt;/n-tab-pane&gt; &lt;n-tab-pane name=&quot;cmd&quot; tab=&quot;Shell cmd&quot;&gt; &lt;n-tabs type=&quot;line&quot; animated&gt; &lt;n-tab-pane name=&quot;shell1&quot; tab=&quot;Shell1&quot;&gt; &lt;n-card style=&quot;overflow-y: scroll; max-height: 700px&quot;&gt; &lt;n-code :code=&quot;code1&quot; language=&quot;bash&quot; word-wrap&gt; &lt;/n-code&gt; &lt;/n-card&gt; &lt;/n-tab-pane&gt; &lt;n-tab-pane name=&quot;shell2&quot; tab=&quot;Shell2&quot;&gt; &lt;n-card style=&quot;overflow-y: scroll; max-height: 700px&quot;&gt; &lt;n-code :code=&quot;code2&quot; language=&quot;bash&quot; word-wrap&gt; &lt;/n-code&gt; &lt;/n-card&gt; &lt;/n-tab-pane&gt; &lt;/n-tabs&gt; &lt;/n-tab-pane&gt; &lt;/n-tabs&gt; &lt;/n-space&gt; &lt;/n-gi&gt; &lt;n-gi :span=&quot;1&quot;&gt; &lt;/n-gi&gt; &lt;/n-grid&gt; &lt;/div&gt; &lt;/n-layout-content&gt; &lt;n-layout-footer&gt; &lt;div style=&quot;height: 90px;text-align: center;padding-top: 30px;&quot;&gt; &lt;p&gt;built by XiaoguangPan&lt;/p&gt; &lt;p&gt;Based on Vue.js &amp; Naive UI&lt;/p&gt; &lt;/div&gt; &lt;/n-layout-footer&gt; &lt;/n-layout&gt;&lt;/template&gt; 然后我们把该组件引入App.vue, &lt;script setup&gt;import upload from &quot;./components/upload.vue&quot;;import hljs from &#x27;highlight.js/lib/core&#x27;import bash from &#x27;highlight.js/lib/languages/bash&#x27;hljs.registerLanguage(&#x27;bash&#x27;, bash)&lt;/script&gt;&lt;template&gt; &lt;n-message-provider&gt; &lt;n-config-provider :hljs=&quot;hljs&quot;&gt; &lt;upload /&gt; &lt;/n-config-provider&gt; &lt;/n-message-provider&gt;&lt;/template&gt; 就这么简单，短短几行代码，一个前端页面完成。完整的项目可以在github访问。 部署单页面应用到github page因为我们没有后端，完全的就是一个客户端渲染页面，所以可以按照纯静态去部署。 在当前目录创建.github/workflows/deploy.yml文件 将以下流程代码写入： # 将静态内容部署到 GitHub Pages 的简易工作流程name: Deploy static content to Pageson: # 仅在推送到默认分支时运行。 push: branches: [&#x27;main&#x27;] # 这个选项可以使你手动在 Action tab 页面触发工作流 workflow_dispatch:# 设置 GITHUB_TOKEN 的权限，以允许部署到 GitHub Pages。permissions: contents: read pages: write id-token: write# 允许一个并发的部署concurrency: group: &#x27;pages&#x27; cancel-in-progress: truejobs: # 单次部署的工作描述 deploy: environment: name: github-pages url: $&#123;&#123; steps.deployment.outputs.page_url &#125;&#125; runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 - name: Set up Node uses: actions/setup-node@v3 with: node-version: 18 cache: &#x27;npm&#x27; - name: Install dependencies run: npm install - name: Build run: npm run build - name: Setup Pages uses: actions/configure-pages@v3 - name: Upload artifact uses: actions/upload-pages-artifact@v1 with: # Upload dist repository path: &#x27;./dist&#x27; - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v1 总结我们借助Vuejs 和 NaiveUI, 完全不需要我们知道Html和CSS的知识，我们只需要把需要的组件调用即可。同时，我们的后端API已经在上一篇文章写好部署，我们直接调用即可。","categories":[{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/categories/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"前端","slug":"前端","permalink":"https://xiaohanys.top/tags/%E5%89%8D%E7%AB%AF/"}],"author":"Xiaoguang Pan"},{"title":"把Fastapi部署到免费的Deta Space上","slug":"把Fastapi部署到免费的DetaSpace上","date":"2024-09-20T05:06:48.000Z","updated":"2024-10-18T05:31:55.220Z","comments":true,"path":"fastapi-dataspace/","permalink":"https://xiaohanys.top/fastapi-dataspace/","excerpt":"上一篇文章我们写了一个Streamlit的程序来全栈的执行我们的任务，但是我们也看到了它的一个缺点：前端界面非异步，UI定制缺乏灵活性。那么，我们接下来尝试采用前后端分离的方式来完成上次的任务。","text":"上一篇文章我们写了一个Streamlit的程序来全栈的执行我们的任务，但是我们也看到了它的一个缺点：前端界面非异步，UI定制缺乏灵活性。那么，我们接下来尝试采用前后端分离的方式来完成上次的任务。 前端：Vue.js + Naive-UI + Vite + github page 后端：Fastapi 那么我们可以看到，我们是需要前端通过AJAX的方式从后端获取数据，后端异步的执行我们表格的处理任务，并生成脚本，推送给前端。要想执行后端任务，必然需要服务器。我们这么个小的程序完全没必要那么做，那么，我们接下来就去 “白嫖” Deta Space的功能来完成后端的部署，这样我们的前端在任何地方都可以调取该API，前端就是个客户端APP而已。 我们先把Fastapi的业务逻辑完成很简单，我们只需要在main.app中写就可以了： 首先导入需要的模块： from fastapi import FastAPI, File, UploadFile, Form, Dependsimport pandas as pdimport os 因为我们读取表格文件，然后生成shell脚本这个任务的返回结果，是我们的一个依赖（一直要用的东西。 所以我们先把这个依赖写出来，可以看到，依赖函数其实就是一个去掉了路由的路由函数！ async def load_file(file: UploadFile = File(...), sampleName: int = Form(...), barcode: int = Form(...), chip: int = Form(...), lane: int = Form(...), dataPath: int = Form(...))-&gt;pd.DataFrame: contents = await file.read() df = pd.read_excel(contents) df = df.fillna(&#x27;&#x27;) df = df.iloc[:,[sampleName-1,barcode-1,chip-1,lane-1,dataPath-1]] df.columns = [&quot;sampleName&quot;, &quot;barcode&quot;, &quot;chip&quot;, &quot;lane&quot;, &quot;dataPath&quot;] return df 然后是第一个路由函数： 就像前面说的，我们的冗余处理，简单的加后缀即可。 ## 将冗余列加后缀def makenames(df:pd.DataFrame,col:str)-&gt;pd.DataFrame: s=&#x27;_&#x27;+df.groupby(col).cumcount().add(1).astype(str) df.loc[:,col]+=s.mask(s==&quot;_1&quot;,&quot;&quot;) return df @app.post(&quot;/upload&quot;)async def process(path:str = Form(...),df:pd.DataFrame = Depends(load_file)): isBlank = 0 isDuplicate = 0 remotePath = path ## 判断dataPath列书否为&quot;&quot; if &quot;&quot; in df.dataPath.tolist(): isBlank = 1 df = df[df.dataPath!=&quot;&quot;] if df.sampleName.duplicated().sum() &gt; 0: isDuplicate = 1 df = makenames(df,&quot;sampleName&quot;) df = df.assign(filename1=df.apply(lambda row : &quot;_&quot;.join([row[&#x27;chip&#x27;],row[&#x27;lane&#x27;],str(row[&#x27;barcode&#x27;]),&quot;1.fq.gz&quot;]),axis=1)) df = df.assign(filename2=df.apply(lambda row : &quot;_&quot;.join([row[&#x27;chip&#x27;],row[&#x27;lane&#x27;],str(row[&#x27;barcode&#x27;]),&quot;2.fq.gz&quot;]),axis=1)) cmd1 = &quot;\\n&quot;.join(df.apply(lambda row: &quot;scp &quot; + os.path.join(row[&#x27;dataPath&#x27;],row[&#x27;filename1&#x27;]) + &quot; &quot; + os.path.join(remotePath,row[&#x27;sampleName&#x27;] + &quot;_R1.fastq.gz&quot;),axis=1).tolist()) cmd2 = &quot;\\n&quot;.join(df.apply(lambda row: &quot;scp &quot; + os.path.join(row[&#x27;dataPath&#x27;],row[&#x27;filename2&#x27;]) + &quot; &quot; + os.path.join(remotePath,row[&#x27;sampleName&#x27;] + &quot;_R2.fastq.gz&quot;),axis=1).tolist()) return &#123;&quot;rawData&quot;:df.to_dict(orient=&quot;records&quot;),&quot;cmd1&quot;:cmd1,&quot;cmd2&quot;:cmd2,&quot;HasBlank&quot;:isBlank,&quot;hasDup&quot;:isDuplicate&#125; 最后是第二个函数通过提交样本名对应文件，修改样本名为真实数据。 @app.post(&quot;/changenames&quot;)async def change(file2:UploadFile= File(...),path:str = Form(...),df2:pd.DataFrame = Depends(load_file)): remotePath = path isDuplicate = 0 df = pd.read_csv(file2.file,header=None,delimiter=&quot;\\t&quot;) df.columns = [&quot;sampleName&quot;,&quot;trueName&quot;] df2 = df2.merge(df,how=&quot;left&quot;,on=&quot;sampleName&quot;) df2 = df2.drop(columns=[&quot;sampleName&quot;]) df2 = df2.dropna() df2 = df2.rename(columns=&#123;&quot;trueName&quot;:&quot;sampleName&quot;&#125;) if df.sampleName.duplicated().sum() &gt; 0: isDuplicate = 1 df2 = makenames(df,&quot;sampleName&quot;) df2 = df2.assign(filename1=df2.apply(lambda row : &quot;_&quot;.join([row[&#x27;chip&#x27;],row[&#x27;lane&#x27;],str(row[&#x27;barcode&#x27;]),&quot;1.fq.gz&quot;]),axis=1)) df2 = df2.assign(filename2=df2.apply(lambda row : &quot;_&quot;.join([row[&#x27;chip&#x27;],row[&#x27;lane&#x27;],str(row[&#x27;barcode&#x27;]),&quot;2.fq.gz&quot;]),axis=1)) cmd1 = &quot;\\n&quot;.join(df2.apply(lambda row: &quot;scp &quot; + os.path.join(row[&#x27;dataPath&#x27;],row[&#x27;filename1&#x27;]) + &quot; &quot; + os.path.join(remotePath,row[&#x27;sampleName&#x27;] + &quot;_R1.fastq.gz&quot;),axis=1).tolist()) cmd2 = &quot;\\n&quot;.join(df2.apply(lambda row: &quot;scp &quot; + os.path.join(row[&#x27;dataPath&#x27;],row[&#x27;filename2&#x27;]) + &quot; &quot; + os.path.join(remotePath,row[&#x27;sampleName&#x27;] + &quot;_R2.fastq.gz&quot;),axis=1).tolist()) return &#123;&quot;rawData&quot;:df2.to_dict(orient=&quot;records&quot;),&quot;cmd1&quot;:cmd1,&quot;cmd2&quot;:cmd2,&quot;HasBlank&quot;:0,&quot;hasDup&quot;:isDuplicate&#125; 跨域CORS问题：别忘了，我们要随处访问该API，所以跨域问题需要暴力移除：添加几行代码在头部： from fastapi.middleware.cors import CORSMiddleware## 1. Create the FastAPI instanceapp = FastAPI()## 2. Add the CORS middlewareorigins = [ &quot;*&quot;]app.add_middleware( CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=[&quot;*&quot;], allow_headers=[&quot;*&quot;],) 注册Deta Space帐户创建免费的Deta Space帐户接下来，创建一个免费帐户Deta Space，您只需要一个电子邮件和密码。 您甚至不需要信用卡，但请确保在注册时启用开发者模式。 安装space cli工具curl -fsSL https://get.deta.dev/space-cli.sh | sh 登陆cli为了使用 Deta Space 验证您的 CLI，您将需要一个访问令牌。 要获取此令牌，请打开您的Deta Space canvas，打开Teletype（画布底部的命令栏），然后单击“设置”。从那里，选择生成令牌并复制生成的令牌。 现在从 Space CLI 运行space login。将令牌粘贴到 CLI 提示符并按 Enter 键后，您应该会看到一条确认消息。 $ space loginTo authenticate the Space CLI with your Space account, generate a new access token in your Space settings and paste it below:*****************************************👍 Login Successful! 在空间中创建新的工程并提交创建工程现在您已经使用 Space CLI 进行了身份验证，可以使用它来创建一个新的工程: $ space new# What is your project&#x27;s name? &gt;$ Glims2Excel Space CLI 会要求您为项目命名，我们将称之为Glims2Excel。 然后，它会尝试自动检测您正在使用的框架或语言，并向您展示它找到的内容。在我们的例子中，它将通过以下消息识别 Python 应用程序，提示您确认： ⚙️ No Spacefile found, trying to auto-detect configuration ...👇 Deta detected the following configuration:Micros:name: Glims2Excel L src: . L engine: python3.9# Do you want to bootstrap &quot;Glims2Excel&quot; with this configuration? (y/n)$ y 确认后，您的项目将在 Deta Space 中创建。Builder 是一个工具箱，可帮助您在 Deta Space 中创建和管理应用程序。 CLI 还将在本地Glims2Excel目录中创建一个Spacefile。Spacefile是一个配置文件，告诉 Deta Space 如何运行您的应用程序。您的应用程序的内容Spacefile如下： v: 0micros: - name: Glims2Excel src: . engine: python3.9 它是一个yaml文件，您可以使用它来添加计划任务等功能或修改应用程序的运行方式，我们稍后将执行此操作。 在Spacefile中定义运行命令Spacefile 中的命令告诉 Space 应执行什么命令来启动您的应用程序。在这种情况下，它应该是uvicorn main:app。注意，我还添加了public_routes选项，目的是令我的API可以公开访问。 v: 0micros: - name: Glims2Excel src: . engine: python3.9 run: uvicorn main:app public_routes: - &quot;/*&quot; 添加requirements.txt在正式上传之前，务必添加依赖文件，该依赖文件应该和main.py在同一个位置，同一个目录下，否则无法运行： # requirements.txtfastapiuvicorn[standard]python-multipart ##处理Formdatapandas ##分析表格数据openpyxl ## 读取excel 正式部署运行space push即可。你可以在你的Canvas中获得一个app，本文的app地址为 https://glims2excel-1-w4936186.deta.app/docs 。 总结经过以上步骤，我们成功的将自己的api部署到了容器中，我们可以在任何地方采用任何方式来和该app交互。下一篇文章，我们将把前端页面写出来。","categories":[{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/categories/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"},{"name":"后端","slug":"后端","permalink":"https://xiaohanys.top/tags/%E5%90%8E%E7%AB%AF/"}],"author":"Xiaoguang Pan"},{"title":"Julia语言编写Needleman Wunsch全局比对算法","slug":"Julia语言编写NeedlemanWunsch全局比对算法","date":"2024-09-20T05:05:46.000Z","updated":"2024-10-18T05:34:04.469Z","comments":true,"path":"jl-global-alignment/","permalink":"https://xiaohanys.top/jl-global-alignment/","excerpt":"输入是两个字符串，输出是对齐后的两个字符串。","text":"输入是两个字符串，输出是对齐后的两个字符串。 function needleman_wunsch(seq1::String, seq2::String; gap_penalty::Int64=-1, match_score::Int64=1, mismatch_penalty::Int64=-1) # Check if the sequences are valid if !all(isuppercase(i) for i in seq1) || !all(isuppercase(j) for j in seq2) throw(ArgumentError(&quot;The sequences must contain only upper-case letters.&quot;)) end # Calculate the length of the two sequences len1, len2 = length(seq1), length(seq2) # Initialize the matrix matrix = zeros(Int64, len1 + 1, len2 + 1) # Initialize the first column first_col = @views matrix[2:end, 1] first_col[:] .= [i * gap_penalty for i in 1:(length(seq1))] # Initialize the first row first_row = @views matrix[1, 2:end] first_row[:] .= [j*gap_penalty for j in 1:(length(seq2))] # Fill the matrix for i in 2:(len1+1) for j in 2:(len2+1) match = seq1[i-1] == seq2[j-1] ? matrix[CartesianIndex(i - 1, j - 1)] + match_score : matrix[CartesianIndex(i - 1, j - 1)] + mismatch_penalty delete = matrix[CartesianIndex(i - 1, j)] + gap_penalty insert = matrix[CartesianIndex(i, j - 1)] + gap_penalty matrix[CartesianIndex(i, j)] = max(match, delete, insert) end end # Initialize the aligned sequences aligned_seq1 = &quot;&quot; aligned_seq2 = &quot;&quot; # Initialize the current index i = len1 + 1 j = len2 + 1 # Traceback while i &gt; 1 || j &gt; 1 if i &gt; 1 &amp;&amp; j &gt; 1 &amp;&amp; matrix[i, j] == matrix[i-1, j-1] + (seq1[i-1] == seq2[j-1] ? match_score : mismatch_penalty) aligned_seq1 = string(seq1[i-1], aligned_seq1) aligned_seq2 = string(seq2[j-1], aligned_seq2) i -= 1 j -= 1 elseif i &gt; 1 &amp;&amp; matrix[i, j] == matrix[i-1, j] + gap_penalty aligned_seq1 = string(seq1[i-1], aligned_seq1) aligned_seq2 = string(&#x27;-&#x27;, aligned_seq2) i -= 1 else aligned_seq1 = string(&#x27;-&#x27;, aligned_seq1) aligned_seq2 = string(seq2[j-1], aligned_seq2) j -= 1 end end (aligned_seq1, aligned_seq2)end 例如： @time needleman_wunsch(&quot;AGTACGCAGTCAGGTGACGTA&quot;,&quot;AGTACCCAGTCAGGGACGTA&quot;) 0.000017 seconds (129 allocations: 8.109 KiB)(&quot;AGTACGCAGTCAGGTGACGTA&quot;, &quot;AGTACCCAGTCAGG-GACGTA&quot;) 整个算法主要是矩阵操作。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"Julia语言模仿BAM文件的pileup类似操作","slug":"Julia语言模仿BAM文件的pileup类似操作","date":"2024-09-20T05:05:01.000Z","updated":"2024-10-18T05:34:07.454Z","comments":true,"path":"jl-bam-pileup/","permalink":"https://xiaohanys.top/jl-bam-pileup/","excerpt":"简介使用过pysam和samtools的小伙伴肯定了解 pileup的操作，如果把BAM文件看作表格的话，那么通常我们是按行去解析它的record，进而获得一些信息，例如比对到哪条染色体，比对的开始位置和结束位置等. 另一种情况下，我们想要按照列去循环解析，得到这个列上的具体信息，典型的就是这个列上比对序列的碱基是什么？比对序列的位置是什么？以及是Match or Mismatch or indel 等。那么，该操作就需要引入pileup操作了。","text":"简介使用过pysam和samtools的小伙伴肯定了解 pileup的操作，如果把BAM文件看作表格的话，那么通常我们是按行去解析它的record，进而获得一些信息，例如比对到哪条染色体，比对的开始位置和结束位置等. 另一种情况下，我们想要按照列去循环解析，得到这个列上的具体信息，典型的就是这个列上比对序列的碱基是什么？比对序列的位置是什么？以及是Match or Mismatch or indel 等。那么，该操作就需要引入pileup操作了。 indel 等。那么，该操作就需要引入pileup操作了。 pysam包中分别有pileup，PileupColumn以及PuleupRead 对象来承担上述任务, 那么我们简单的在julia使用，不需要构建对象这么复杂的操作，写几个函数即可 引入需要的包using XAMusing BioAlignmentsusing GenomicFeaturesusing BioGenerics 获取比对到某个位点和某个区间的read 数量function pileup(bam::BAM.Reader,contig::String, pos::Int64) ### get the intervealCollection site = Interval(contig, pos,pos) ### get the pileup GenomicFeatures.eachoverlap(bam,site)end## return number of segments in the pileupfunction nsegments(bam::BAM.Reader,contig::String, start::Int64, stop::Int64) interval = Interval(contig, start, stop) num = 0 for i in eachoverlap(bam,interval) num+=1 end numendfunction nsegments(bam::BAM.Reader,contig::String, pos::Int64) interval = Interval(contig, pos, pos) num = 0 for i in eachoverlap(bam,interval) num+=1 end numend 获取比对到某个位点的Query 序列的位置和比对情况function get_query_position(bam::BAM.Reader, contig::String, pos::Int) # get the pileup at the contig position PileupReads = pileup(bam, contig, pos) # get the reference base Iterators.map(x-&gt;(ref2seq(BAM.alignment(x),pos)[1]),PileupReads)end function get_query_operation(bam::BAM.Reader,contig::String, pos::Int) # Get pileup reads PileupReads = pileup(bam,contig, pos) # Map each read to the base at the given position Iterators.map(x-&gt;(ref2seq(BAM.alignment(x),pos)[2]),PileupReads)end 该get_query_position函数返回一个生成器，可以迭代获取比对到某个参考位点的record上的具体位置，例如 bam = open(BAM.Reader,&quot;45T.cs.rmdup.sort.bam&quot;,index=&quot;45T.cs.rmdup.sort.bam.bai&quot;)pos = get_query_position(bam,&quot;chr6&quot;,36434531)for p in pos println(p)end 当然，你可以做一个双重循环，计算很多位点的具体位置。 同样，该get_query_operation函数返回的是具体比对类型，我们都知道，比对有M(match),I(insertion),D(deletion)组成，这个就可以打印对应的行为。 bam = open(BAM.Reader,&quot;45T.cs.rmdup.sort.bam&quot;,index=&quot;45T.cs.rmdup.sort.bam.bai&quot;)pos = get_query_operation(bam,&quot;chr6&quot;,36434531)for p in pos println(p)endMMMMM...","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"Python编写拆分Barcode的脚本，并用Codon编译为Native code","slug":"Python编写拆分Barcode的脚本","date":"2024-09-20T05:04:10.000Z","updated":"2024-10-18T05:34:10.194Z","comments":true,"path":"py-barcode-split/","permalink":"https://xiaohanys.top/py-barcode-split/","excerpt":"摘要成对的reads中，read_2的开头包含两份barcode序列，分别长10bp,中间有一段固定长度为15bp的序列分割，例如 ATCTATGACATGTTACGTTAACTCCNATCTATCACTTAGCGCTGNCCCTGTCCTCTACACTCCACCCCCTCCCCACCAGACTAAACAACGCCCTTTCCCC 该序列中ATTTATGACA及AATCTATCAA为barcode序列。要注意，barcode因为测序的原因存在一定的错配，需要对其有一定的容纳。","text":"摘要成对的reads中，read_2的开头包含两份barcode序列，分别长10bp,中间有一段固定长度为15bp的序列分割，例如 ATCTATGACATGTTACGTTAACTCCNATCTATCACTTAGCGCTGNCCCTGTCCTCTACACTCCACCCCCTCCCCACCAGACTAAACAACGCCCTTTCCCC 该序列中ATTTATGACA及AATCTATCAA为barcode序列。要注意，barcode因为测序的原因存在一定的错配，需要对其有一定的容纳。 具体过程1. 编写识别Barcode容错的程序Barcode和序列本质上是字符串，而最简单的容错就是使用汉明距离来计算出两个等长字符串的错配个数，并加以限制。 def hamming_distance(s1: str, s2: str) -&gt; int: &quot;&quot;&quot;Return the Hamming distance between equal-length sequences&quot;&quot;&quot; if len(s1) != len(s2): raise ValueError(&quot;Undefined for sequences of unequal length&quot;) return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2)) 根据标题所示，我们需要将Python代码编译为本地代码，因此需要对一些变量加以标注，以生成高性能代码。Codon中对变量类型的声明s1:str，在最新版本的python中是存在的。 2. 编写一个解析压缩格式fastq文件的程序这里因为测序数据量很大，不建议直接读入内存，我们写一个生成器。另外，因为Codon不支持BioPython包，因此需要我们自己来写。 import gzip@tupleclass FastqRecord: id: str sequence: str quality: strdef parse_gzip_fastq(filename:str): with gzip.open(filename, &#x27;rt&#x27;) as f: while True: try: lines = [next(f).strip(&quot;\\n&quot;) for _ in range(4)] except: break if not all(lines): break id = lines[0][1:] sequence = lines[1] quality = lines[3] yield FastqRecord(id, sequence, quality) 3. 读取Barcode序列Barcode是任意两个序列的2V2组合，而且在read_2上，所以需要反向互补。 import itertoolsdef reverse_complement(seq: str) -&gt; str: &quot;&quot;&quot;Compute reverse complement of a sequence&quot;&quot;&quot; basecomplement = &#123;&#x27;A&#x27;: &#x27;T&#x27;, &#x27;C&#x27;: &#x27;G&#x27;, &#x27;G&#x27;: &#x27;C&#x27;, &#x27;T&#x27;: &#x27;A&#x27;&#125; letters = list(seq) letters = [basecomplement[base] for base in letters] return &#x27;&#x27;.join(letters[::-1])def read_barcode(barcode: str): &quot;&quot;&quot;Read barcode from file&quot;&quot;&quot; codes = List[str]() with open(barcode, &#x27;r&#x27;) as f: codes = [reverse_complement(line.strip(&quot;\\n&quot;)) for line in f] return [cc for cc in itertools.product(codes, codes)] 最终拆分Barcode的Main函数如下def split_barcodes(fq1:str,fq2:str, barcodes: List[Tuple[str,str]]): &quot;&quot;&quot;make files for every barcode&quot;&quot;&quot; read1_files = &#123;barcode:open(f&quot;&#123;barcode[0]&#125;_&#123;barcode[1]&#125;_1.fastq&quot;,&quot;a+&quot;) for barcode in barcodes&#125; read2_files = &#123;barcode:open(f&quot;&#123;barcode[0]&#125;_&#123;barcode[1]&#125;_2.fastq&quot;,&quot;a+&quot;) for barcode in barcodes&#125; &quot;&quot;&quot;read raw fastq files&quot;&quot;&quot; for record1, record2 in zip(parse_gzip_fastq(fq1), parse_gzip_fastq(fq2)): for barcode in barcodes: barcode_l, barcode_r = barcode if hamming_distance(record2.sequence[:len(barcode_l)], barcode_l) &lt;= 1 and hamming_distance(record2.sequence[len(barcode_r)+15:len(barcode_r)*2+15], barcode_r) &lt;= 3: read1_files[barcode].write(f&quot;&#123;record1.id&#125;\\n&quot;) read1_files[barcode].write(record1.sequence+&quot;\\n&quot;) read1_files[barcode].write(&quot;+\\n&quot;) read1_files[barcode].write(record1.quality+&quot;\\n&quot;) read2_files[barcode].write(f&quot;&#123;record2.id&#125;\\n&quot;) read2_files[barcode].write(record2.sequence+&quot;\\n&quot;) read2_files[barcode].write(&quot;+\\n&quot;) read2_files[barcode].write(record2.quality+&quot;\\n&quot;) [f.close() for f in read1_files.values()] [f.close() for f in read2_files.values()] 编译成本地代码下载codon编译器 /bin/bash -c &quot;$(curl -fsSL https://exaloop.io/install.sh)&quot; 然后执行编译 codon build -release -o split_barcode split_barcodes.codon 总结Codon作为一款高性能的兼容Python语法的编译器，可以在一定程度上提升Python的执行速度到C/C++的水平，原则上只要会写Python，那么就掌握了99%的Codon了，但是在整个过程中我们也发现了，目前Codon只支持部分的Python标准库，有些库函数还不健全，例如GZfile的readline()函数就不存在，需要我们用next(iter)来替代，并添加额外的EOF 检测。在读取fastq文件时，我们也无法调用第三方库，需要自行“造轮子”。虽然提升了执行速度，但是编写代码的速度依旧降低了。 但是，不要忘了现在我们有强大的chatGPT了，针对一些功能，你完全可以让chatGPT帮你写好代码，这样重复造轮子的工作便可以由AI 取代，这在一定程度上算是减少了Codon的缺陷了。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"}],"author":"Xiaoguang Pan"},{"title":"单细胞数据如何绘制stacked violin?","slug":"单细胞数据如何绘制stackedviolin","date":"2024-09-20T05:03:11.000Z","updated":"2024-10-18T05:32:03.082Z","comments":true,"path":"scrna-stack-violin/","permalink":"https://xiaohanys.top/scrna-stack-violin/","excerpt":"Python的Scanpy包和Seurat包一样，是单细胞数据处理的利器，其中，Scanpy中有一种堆积的小提琴图，可以很好的展示marker的表达情况，但是在Seurat中并没有内置命令。因此，我自己尝试提取数据并用ggplot2包来画该图。 首先来展示以下画图的成果，如图","text":"Python的Scanpy包和Seurat包一样，是单细胞数据处理的利器，其中，Scanpy中有一种堆积的小提琴图，可以很好的展示marker的表达情况，但是在Seurat中并没有内置命令。因此，我自己尝试提取数据并用ggplot2包来画该图。 首先来展示以下画图的成果，如图 首先来展示以下画图的成果，如图 那么直接上命令吧！ ###载入需要的R包library(Seurat)library(dplyr)library(tidyr)library(ggplot2)##load测试数据data&lt;-readRDS(&quot;cluster_1.afterclu.rds&quot;) ##第一个函数从Seurat对象中获取表达值并转化为需要的格式tidygotData&lt;-function(seurat.obj,features,groups)&#123; mat&lt;-GetAssayData(data,assay = &quot;RNA&quot;,slot = &quot;data&quot;)[features,] plotData&lt;-mat%&gt;% as.data.frame()%&gt;% tibble::rownames_to_column(var=&quot;gene&quot;)%&gt;% as_tibble()%&gt;% tidyr::pivot_longer(names_to = &quot;cell&quot;,values_to=&quot;exp&quot;,cols=2:(ncol(mat)+1)) cellmeta&lt;-data@meta.data%&gt;% tibble::rownames_to_column(var=&quot;cell&quot;)%&gt;% as_tibble()%&gt;% select(cell,sym(groups)) plotData&lt;-plotData%&gt;% left_join(cellmeta,by=&quot;cell&quot;)%&gt;% setNames(c(&quot;gene&quot;,&quot;cell&quot;,&quot;value&quot;,&quot;cellID&quot;)) plotData&#125;##第二个函数画图plot_stacked_violin&lt;-function(plotData,xlab,ylab,cols)&#123; ggplot(plotData,aes(y=cellID,x=value,fill=cellID))+ geom_violin()+ facet_wrap(.~gene)+ theme_test()+ xlab(xlab)+ ylab(ylab)+ scale_fill_manual(values = cols)+ theme(panel.spacing=unit(0,&quot;cm&quot;), strip.background = element_rect(fill=&quot;transparent&quot;,color = &quot;white&quot;), axis.text.x = element_blank(), axis.ticks.x = element_blank(), panel.border = element_rect(size=0.7,colour = &quot;black&quot;), strip.text = element_text(size=10,face = &quot;italic&quot;), axis.text.y = element_text(size = 11.5,face=&quot;bold&quot;), axis.title.y = element_text(size = 13))+ NoLegend()&#125; 下面是使用方法 plot_stacked_violin(gotData(data,c(&quot;CD7&quot;,&quot;KLRB1&quot;,&quot;NKG7&quot;),&quot;integrated_snn_res.0.1&quot;),&quot;&quot;,&quot;Cell cluster&quot;,c(&quot;#8dd3c7&quot;,&quot;#ffffb3&quot;,&quot;#bebada&quot;,&quot;#fb8072&quot;))","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"单细胞测序","slug":"单细胞测序","permalink":"https://xiaohanys.top/tags/%E5%8D%95%E7%BB%86%E8%83%9E%E6%B5%8B%E5%BA%8F/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"},{"title":"不同的语言处理gzip压缩文件的时间对比","slug":"不同的语言处理gzip压缩文件的时间对比","date":"2024-09-20T05:01:02.000Z","updated":"2024-10-18T05:31:59.362Z","comments":true,"path":"gzip-decompress-eff/","permalink":"https://xiaohanys.top/gzip-decompress-eff/","excerpt":"首先在shell中测试如下命令#!/bin/shtime gzip -d -c risearch_chr1:143971112-143971134:+:FAM72C.out.gz &gt; risearch_chr1:143971112-143971134:+:FAM72C.out","text":"首先在shell中测试如下命令#!/bin/shtime gzip -d -c risearch_chr1:143971112-143971134:+:FAM72C.out.gz &gt; risearch_chr1:143971112-143971134:+:FAM72C.out 0.28s user 0.02s system 99% cpu 0.297 total 然后测试pythonimport gzipdef parse_gzip_py(ris_file): inf = gzip.open(ris_file, &#x27;rt&#x27;) with open(&quot;test_py_gz.txt&quot;,&quot;w&quot;) as f: for line in inf: f.write(line+&quot;\\n&quot;) inf.close() %timeit parse_gzip_py(&quot;risearch_chr1:143971112-143971134:+:FAM72C.out.gz&quot;)399 ms +/- 1.91 ms per loop Julia 里面有两个解析Gzip的包，分别是GZip.jl 和 CodecZlib.jl。 我们分别来测试一下 using GZipfunction parse_gzip_file(filename::String) out = open(&quot;test_gzipjl.txt&quot;, &quot;w&quot;) zips = GZip.open(filename) while !eof(zips) line = readline(zips) println(out, line) end close(out)end @time parse_gzip_file(&quot;risearch_chr1:143971112-143971134:+:FAM72C.out.gz&quot;)0.388812 seconds 然后是codeczlib,必须用另一个包调用它using TranscodingStreamsusing CodecZlibfunction parse_gzi_trans(filename::String) out = open(&quot;test_trans.txt&quot;, &quot;w&quot;) stream = GzipDecompressorStream(open(filename)) for line in eachline(stream) println(out, line) end close(out)end @time parse_gzi_trans(&quot;risearch_chr1:143971112-143971134:+:FAM72C.out.gz&quot;)0.280360 seconds 结论：Julia语言使用GZip包的时候，速度要慢于shell，快于python；使用CodecZlib的时候，速度快于shell 和 python。但是整体时间来看，最快的比最慢的也就快0.1s，这也就意味着，即使是要解压10000个文件，Julia也就比python快16分钟而已。这个在巨大的解压用时面前，并不算什么。 数据文件可从我的github获取link","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"},{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"},{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"}],"author":"Xiaoguang Pan"},{"title":"使用R语言实现bedtools求交集的功能?","slug":"使用R语言实现bedtools求交集的功能","date":"2024-09-20T05:00:06.000Z","updated":"2024-10-18T05:33:02.069Z","comments":true,"path":"r-bedtools-intersect/","permalink":"https://xiaohanys.top/r-bedtools-intersect/","excerpt":"Bedtools作为基因组研究的 “ 瑞士军刀 ”， 功能强大且易于操作，是生信行业不可多得的好软件。通常对bed区间的注释，我们使用其中“ 求交集 ”的功能（bedtools intersect) ，但是有一个很不方便的地方，我们通常要生成对应的bed文件，再注释完成后还需要用R语言等读入才能继续分析，所以整合度不是很好，本文希望提供R语言的思路来解决该问题。","text":"Bedtools作为基因组研究的 “ 瑞士军刀 ”， 功能强大且易于操作，是生信行业不可多得的好软件。通常对bed区间的注释，我们使用其中“ 求交集 ”的功能（bedtools intersect) ，但是有一个很不方便的地方，我们通常要生成对应的bed文件，再注释完成后还需要用R语言等读入才能继续分析，所以整合度不是很好，本文希望提供R语言的思路来解决该问题。 什么是bedtools?bedtools：一个强大的基因组算法工具集总的来说，bedtools实用程序是用于广泛基因组学分析任务的瑞士军刀。最广泛使用的工具支持基因组算法：即基因组的集合论。例如，bedtools允许从广泛使用的基因组文件格式（如 BAM、BED、GFF&#x2F;GTF、VCF）的多个文件中交叉、合并、计数、补充和洗牌基因组区间。虽然每个单独的工具都旨在完成一项相对简单的任务（例如， 相交两个间隔文件），可以通过在 UNIX 命令行上组合多个 bedtools 操作来进行相当复杂的分析。 bedtools是在犹他大学的昆兰实验室开发的，受益于世界各地科学家的杰出贡献。你可以从该链接https://bedtools.readthedocs.io/en/latest/content/bedtools-suite.html获取它所有的所有功能。 第一个问题：获取bed区间开始&#x2F;结束位点落在基因区间上的结果这个问题用bedtools也不好解决，因为实际上我们需要获取交集的子集。需要首先把bed区间转变为只有1bp的区间，这增加了操作步骤，但是在R语言中我们可以很灵活实现该方案。 需要先导入R包 library(GenomicRanges)library(readr)library(dplyr) 然后写一个函数，负责将bed文件转化为GRanges对象。 makeGranges&lt;-function(x,meta.name)&#123; df&lt;-read_tsv(x,col_names=F) if(stringr::str_detect(df$X1[1],&quot;chr&quot;))&#123; df$X1&lt;-stringr::str_remove(df$X1,&quot;chr&quot;) &#125; names(df)&lt;-c(&quot;seqname&quot;,&quot;start&quot;,&quot;end&quot;,meta.name) makeGRangesFromDataFrame(df,keep.extra.columns = T,ignore.strand = T)&#125; 然后就是实现该题目功能的函数了 anno_start&lt;-function(x,y)&#123; x_tmp&lt;-narrow(x,start=1,width = 1) res&lt;-findOverlaps(x_tmp,y,ignore.strand=T) jiaoji&lt;-x[queryHits(res)] mcols(jiaoji)$anno&lt;-mcols(y[subjectHits(res)])[[1]] ## assume that the first meta column should be anno info jiaoji&#125; 其实就是通过缩短bed区间到起始位置1bp，然后求交集即可。 第二个问题：如何实现百分比交集我们通常不仅仅想知道交集，还想知道交集之间相交部分 的长度占自身的百分比来控制结果的输出。 get_overlap_percentage&lt;-function(x,y,query.restrict=T,pct=0.2)&#123; hits &lt;- findOverlaps(x,y,ignore.strand=T) ints&lt;-pintersect(x[queryHits(hits)],y[subjectHits(hits)]) if(query.restrict==TRUE)&#123; int_p&lt;-width(ints)/width(x[queryHits(hits)]) &#125;else&#123; int_p&lt;-width(ints)/width(y[subjectHits(hits)]) &#125; hits &lt;- hits[int_p&gt;=pct] jiaoji&lt;-x[queryHits(hits)] mcols(jiaoji)$anno&lt;-mcols(y[subjectHits(hits)])[[1]] jiaoji&#125; 虽然也不难，但是这个功能实现起来，还是比第一个问题要复杂的，因为交集会存在一对多的情况，所以我们需要两次求交，分别计算长度以及百分比，然后控制第一次的输出。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"},{"title":"Julia短小代码批量检测BAM文件的完整性","slug":"Julia短小代码批量检测BAM文件的完整性","date":"2024-09-20T04:59:18.000Z","updated":"2024-10-18T05:33:50.310Z","comments":true,"path":"jl-bam-check/","permalink":"https://xiaohanys.top/jl-bam-check/","excerpt":"我们在运行bwa mem比对的时候，由于某些不明的原因会造成程序中断，例如内存超了，IO错误，计算节点崩溃等，然而BAM是否完整很难察觉，最终导致后续流程无法运行。这里，我们通过一段简短的代码来检查BAM文件的完整性，代码如下：","text":"我们在运行bwa mem比对的时候，由于某些不明的原因会造成程序中断，例如内存超了，IO错误，计算节点崩溃等，然而BAM是否完整很难察觉，最终导致后续流程无法运行。这里，我们通过一段简短的代码来检查BAM文件的完整性，代码如下： function check_eofs(fs::String) open(fs, &quot;r&quot;) do IO seekend(IO) eof_positions = position(IO) - 28 seek(IO, eof_positions) eof_markers = UInt8[0x1f, 0x8b, 0x08, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0x06, 0x00, 0x42, 0x43, 0x02, 0x00, 0x1b, 0x00, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00] actual_markers = Vector&#123;UInt8&#125;(undef, 28) read!(IO, actual_markers) if isequal(eof_markers, actual_markers) @info &quot;file have BGZF EOF Markers and is corrected!&quot; filename = fs else @warn &quot;file maybe truncted,please check your file!&quot; filename = fs end endendfunction main(ARGS) if length(ARGS) &gt; 1 check_eofs.(ARGS) else check_eofs(ARGS[1]) endendmain(ARGS) 原理很简单，根据官方对SAM文件的描述，其使用了一种兼容gzip的压缩格式，但是这个格式可以通过建立索引最终达到随机访问的目的，而每个文件的末尾都存在EOF-marker, 其为1f 8b 08 04 00 00 00 00 00 ff 06 00 42 43 02 00 1b 00 03 00 00 00 00 00 00 00 00 00的28位比特字符，我们直接读取文件末尾的28比特和该标志符对比即可检查其完整性了。 使用方法 julia check_BGZF.jl BAM_1 BAM_2 BAM_3 ... 同理，所有使用BGZF压缩格式的文件都可以采用这一方法检查完整，例如压缩的vcf文件vcf.gz。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"julia计算为ASCAT创建GC矫正文件","slug":"julia计算为ASCAT创建GC矫正文件","date":"2024-09-20T04:58:23.000Z","updated":"2024-10-18T05:33:55.805Z","comments":true,"path":"jl-gc-correct/","permalink":"https://xiaohanys.top/jl-gc-correct/","excerpt":"如题，官方已经提供了一个R的版本createGCcontentFile.R ，但是根据代码就能看出这个版本非常占内存了，首先要把基因组整个序列都load入内存中去，每次计算出的矫正数据也是储存dataframe中。为了降低内存占用，也为了提高计算速度，我写了一个julia版本的。代码如下：","text":"如题，官方已经提供了一个R的版本createGCcontentFile.R ，但是根据代码就能看出这个版本非常占内存了，首先要把基因组整个序列都load入内存中去，每次计算出的矫正数据也是储存dataframe中。为了降低内存占用，也为了提高计算速度，我写了一个julia版本的。代码如下： using BioSequencesusing FASTXfunction cal(sequence::LongDNASeq) nGC = count(x -&gt; ((x == DNA_G) || (x == DNA_C)), sequence) nAT = count(x -&gt; ((x == DNA_A) || (x == DNA_T)), sequence) (nGC, nAT)endfunction getPosGCs(IO::IOStream, CHRseq::BioSequence, totalLen::Int64, Chr::SubString, POS::Int64, WINDOW::Vector&#123;Int64&#125;, THRESH::Int64, idx::SubString) print(IO, idx, &quot;\\t&quot;, Chr, &quot;\\t&quot;, POS, &quot;\\t&quot;) for window in WINDOW window = window % 2 == 0 ? window + 1 : window startPOS = POS - Int64(floor(window / 2)) tailPOS = POS + Int64(floor(window / 2)) tailPOS = tailPOS &gt; totalLen ? totalLen : tailPOS startPOS = startPOS &lt;= 0 ? 1 : startPOS gc, at = cal(CHRseq[startPOS:tailPOS]) if gc + at &gt; THRESH print(IO, round(gc / (gc + at); digits=6), &quot;\\t&quot;) else print(IO, &quot;NA&quot;, &quot;\\t&quot;) end end print(IO, &quot;\\n&quot;)endfunction main(snpLoci::String, fasta::String) open(&quot;GC_correct.file.txt&quot;, &quot;w&quot;) do io println(&quot;Loding reference genome!&quot;) Genome = open(FASTA.Reader, fasta, index=string(fasta, &quot;.fai&quot;)) WINDOWS = Int64[25, 50, 100, 200, 500, 1e3, 2e3, 5e3, 1e4, 2e4, 5e4, 1e5, 2e5, 5e5, 1e6] println(io, &quot;\\tChr\\tPosition\\t25bp\\t50bp\\t100bp\\t200bp\\t500bp\\t1kb\\t2kb\\t5kb\\t10kb\\t20kb\\t50kb\\t100kb\\t200kb\\t500kb\\t1Mb&quot;) chrMarker = &quot;&quot; seq = dna&quot;NNNNN&quot; totalLen = 0 for line in eachline(snpLoci) if startswith(line, &quot;\\t&quot;) continue else idx, chr, pos = split(line, &quot;\\t&quot;) pos = parse(Int64, pos) if chr != chrMarker seq = FASTA.sequence(Genome[chr]) totalLen = length(seq) println(&quot;processing chromosome &quot;, chr, &quot;...&quot;) end chrMarker = chr getPosGCs(io, seq, totalLen, chr, pos, WINDOWS, 20, idx) end end endendmain(ARGS[1], ARGS[2]) 代码使用了Biojulia中的FASTX.jl包快速读取基因组，使用BioSequences.jl中的函数计算GC含量和索引序列。因此，这两个包需要提前安装好，然后用法就很简单， julia createGCcontentFile.jl snp_loci.txt hg38.fa","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"用julia语言计算测序数据的Insert Size?","slug":"用julia语言计算测序数据的InsertSize","date":"2024-09-20T04:57:17.000Z","updated":"2024-10-18T05:33:16.777Z","comments":true,"path":"jl-insert-size/","permalink":"https://xiaohanys.top/jl-insert-size/","excerpt":"Julia读取BAM的库想要计算Insert size，需要提供一个基因组比对后的文件，sam也好，bam也罢。那么，使用julia语言计算该值的第一步便是了解如何读取和解析BAM文件格式。","text":"Julia读取BAM的库想要计算Insert size，需要提供一个基因组比对后的文件，sam也好，bam也罢。那么，使用julia语言计算该值的第一步便是了解如何读取和解析BAM文件格式。 我们使用BioJulia提供的XAM包来读取BAM文件。所以我们需要首先安装该包。打开julia，输入]进入Pkg模式 add XAM 使用XAM读取和解析BAM文件的一般格式reader = open(BAM.Reader, &quot;data.bam&quot;)record = BAM.Record()while !eof(reader) empty!(record) read!(reader, record) # 做一些事情，例如解析和计算end 上面的流程总体上就是 使用BAM.Reader读取文件 定义一个空的Record对象 从头至尾循环整个BAMfile 将每行bam读入Record 这样的操作方式可以节省内存，避免循环很大的bam文件爆内存。 什么是Insert Size？通俗的讲，Insert 长度就是指双端序列比对后，模版的长度。所以我们要计算需要保证如下条件 reads是成对，最好是Proper paired 只需要计算read1即可，不然就算重了 即使Proper paired也会有负的Insert，需要移除 代码如下 using XAMusing Statisticsfunction insert_size_dist(Reader::XAM.BAM.Reader) insert_length = Int64[] record = BAM.Record() while !eof(Reader) empty!(record) read!(Reader, record) if BAM.flag(record) &amp; 0x2 != 0 ## paired if BAM.flag(record) &amp; 0x40 != 0 ## first in pair t_len = BAM.templength(record) if t_len &gt; 0 push!(insert_length, t_len) end end end end (mean(insert_length), std(insert_length))end","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"julia多线程","slug":"julia多线程","date":"2024-09-20T04:56:28.000Z","updated":"2024-10-18T05:33:53.124Z","comments":true,"path":"jl-multiprocess/","permalink":"https://xiaohanys.top/jl-multiprocess/","excerpt":"julia本身是一门很快速的语言，但是现代计算机往往具有多核心多线程设计，因此，充分发挥硬件，能进一步提高效率","text":"julia本身是一门很快速的语言，但是现代计算机往往具有多核心多线程设计，因此，充分发挥硬件，能进一步提高效率 多线程的启动julia从1.5开始新添加了命令行参数-t num_procs，例如，你想启动一个10个线程的julia，那么就可以执行： julia -t 10 进入REPL后，可以查看当前线程数： Threads.nthreads()10 如何多线程？julia提供了一个简单的宏函数，来快速的包装for循环，使其可以将任务拆分为n份，每分独立执行 Threads.@threads 例如： a = zeros(10)Threads.@threads for i = 1:10 a[i] = Threads.threadid()end 非常简单的，可以快速将你的单线程函数转化为多线程。 线程安全举个例子： acc = Ref(0)@threads for i in 1:1000 acc[] += 1end 正常情况下，该流程计算的是从累加1000个1，结果应该是1000才对，但是因为多线程调度，导致acc最终的结果并非为1000，即得到了错误的计算结果。 第一个解决办法是使用原子操作 acc = Atomic&#123;Int64&#125;(0)@threads for i in 1:1000 atomic_add!(acc, 1)end 另一种方法是在变量上添加线程锁，使其同一时间有且只有一个线程可以操作该变量 acc = Ref(0)l = ReentrantLock()@threads for i=1:1000 lock(l) do acc[]+=1 endend 当然，举例只是为了说明多线程的安全问题，该变量的累积添加了线程锁后其实和单线程操作没有任何区别，反而因为线程调度影响了性能。因此，需要综合考虑你的项目是否需要多线程。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"基于Julia语言的多线程barcode拆分","slug":"基于Julia语言的多线程barcode拆分","date":"2024-09-20T04:55:29.000Z","updated":"2024-10-18T05:32:17.350Z","comments":true,"path":"jl-barcode-split/","permalink":"https://xiaohanys.top/jl-barcode-split/","excerpt":"拆分原理 软件的逻辑是首先获取barcode列表。然后采用多线程分别在fastq文件中并行提取对应barcode的reads。 WGS的下机数据经常出现在fastq2里。所以程序会从fastq中自动查找是否存在对应barcode。 程序可以自动检测barcode始于开始还是末尾，计算hanming距离，运行1bp的mismatch。","text":"拆分原理 软件的逻辑是首先获取barcode列表。然后采用多线程分别在fastq文件中并行提取对应barcode的reads。 WGS的下机数据经常出现在fastq2里。所以程序会从fastq中自动查找是否存在对应barcode。 程序可以自动检测barcode始于开始还是末尾，计算hanming距离，运行1bp的mismatch。 程序可以自动检测barcode始于开始还是末尾，计算hanming距离，运行1bp的mismatch。 方法julia -t number threads split_barcode.jl -b barcode list -r read2 file -l read1 file -o outpath 代码举例using BioSequencesusing FASTXusing CodecZlibusing StringDistancesusing Base.Threadsusing Getoptfunction read_barcode(barcode_fs::String) labels = String[] barcodes = String[] if isfile(barcode_fs) for line in eachline(barcode_fs) label, sequence = split(line) push!(labels, label) push!(barcodes, sequence) end else println(&quot;no barcode file detected!&quot;) end labels, barcodesendfunction detective_barcode(seq::LongDNASeq, barcode::LongDNASeq) barcode_l = length(barcode) starter = seq[1:barcode_l] ender = seq[end-barcode_l:end] dist = Hamming()(starter, barcode) if dist == 0 || dist == 1 ## if barcode in title return 1, 1 else dist = Hamming()(ender, barcode)# calular hamming distance allow 1bp mismatch if dist == 0 || dist == 1 return 1, 2 else return -1, 0 end endendfunction process(fq_file_1::String, fq_file_2::String, barcode::LongDNASeq, label::String, outpath::String) reader_2 = FASTQ.Reader(GzipDecompressorStream(open(fq_file_2))) reader_1 = FASTQ.Reader(GzipDecompressorStream(open(fq_file_1))) wt_1 = FASTQ.Writer(open(&quot;$(outpath)/barcode_$(label)_1.fq&quot;, lock=true, &quot;a&quot;)) ## lock for multiple threads safety wt_2 = FASTQ.Writer(open(&quot;$(outpath)/barcode_$(label)_2.fq&quot;, lock=true, &quot;a&quot;)) for (record_1, record_2) in zip(reader_1, reader_2) fm, weizhi = detective_barcode(FASTQ.sequence(record_2), barcode) if fm == 1 write(wt_1, record_1) if weizhi == 1 write(wt_2, FASTQ.Record(FASTQ.identifier(record_2), FASTQ.sequence(record_2)[11:end], FASTQ.quality(record_2)[11:end])) elseif weizhi == 2 write(wt_2, FASTQ.Record(FASTQ.identifier(record_2), FASTQ.sequence(record_2)[1:100], FASTQ.quality(record_2)[1:100])) end end end close(reader_1) close(reader_2) close(wt_1) close(wt_2)endfunction split_barcode(barcodes_fs::String, fq_file_2::String, fq_file_1::String, outpath::String) labels, barcodes = read_barcode(barcodes_fs) barcodes = LongDNASeq.(barcodes) duiying = Dict(zip(barcodes, labels)) @threads for barcode in barcodes label = duiying[barcode] process(fq_file_1, fq_file_2, barcode, label, outpath) endendfunction Argparse() lst = Dict&#123;String,String&#125;() for (opt, arg) in getopt(ARGS, &quot;hb:r:l:o:&quot;, [&quot;help&quot;,&quot;barcodes=&quot;, &quot;read2=&quot;,&quot;read1=&quot;,&quot;output=&quot;]) opt = replace(opt, &quot;-&quot; =&gt; &quot;&quot;) arg = replace(arg, &quot;=&quot; =&gt; &quot;&quot;) if opt == &quot;help&quot; || opt == &quot;h&quot; println(&quot;this program is for spliting fastq file into different part according barcodes!&quot;) println(&quot;-h\\t--help\\tshow this message&quot;) println(&quot;-b\\t--barcodes\\tinput your barcodes file, should be negetive sequennce!&quot;) println(&quot;-r2\\t--read2\\tinput your fastq file read_2&quot;) println(&quot;-r1\\t--read1\\tinput your fastq file read_1&quot;) println(&quot;-o\\t--output\\tinput output file path&quot;) elseif opt == &quot;barcodes&quot; || opt == &quot;b&quot; lst[&quot;barcodes&quot;] = arg elseif opt == &quot;read2&quot; || opt == &quot;r&quot; lst[&quot;read2&quot;] = arg elseif opt == &quot;read1&quot; || opt == &quot;l&quot; lst[&quot;read1&quot;] = arg elseif opt == &quot;output&quot; || opt == &quot;o&quot; lst[&quot;output&quot;] = arg else println(&quot;please check your parameter!&quot;) end end lstendargs = Argparse()if length(args) &gt; 1 split_barcode(args[&quot;barcodes&quot;], args[&quot;read2&quot;], args[&quot;read1&quot;], args[&quot;output&quot;])end","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"Julia计算相关性检验","slug":"Julia计算相关性检验","date":"2024-09-20T04:54:29.000Z","updated":"2024-10-18T05:33:58.522Z","comments":true,"path":"jl-correlation/","permalink":"https://xiaohanys.top/jl-correlation/","excerpt":"众所周知，计算相关性非常的简单，因为R 语言中有函数cor.test(),该函数可以计算多种方法的相关性检验，返回相关性，Pvalue等检验值，但是这个函数在Julia中并不存在，让Julia作为一门科学计算语言显得并不完美。","text":"众所周知，计算相关性非常的简单，因为R 语言中有函数cor.test(),该函数可以计算多种方法的相关性检验，返回相关性，Pvalue等检验值，但是这个函数在Julia中并不存在，让Julia作为一门科学计算语言显得并不完美。 首先，我们来看一下R语言计算线性相关的结果是什么样子的？ a&lt;-c(1,3,4,5,7)b&lt;-c(2,4,6,8,9)c&lt;-cor.test(a,b)c Pearson&#x27;s product-moment correlationdata: a and bt = 7.7771, df = 3, p-value = 0.004423alternative hypothesis: true correlation is not equal to 095 percent confidence interval: 0.6757774 0.9984873sample estimates: cor 0.976086 首先，我们生成了两个示例的向量，这两个向量必须等长 然后，我们使用了cor.test，因为默认就是皮尔森，因此我们不再使用method参数，得到的结果也很丰富。 但是，我们比较Julia呢？ 首先，julia标准库并没有计算相关性的函数，这里我们可以调用Statistics包来计算相关性 a = [1,3,4,5,7]b = [2,4,6,8,9]using Statisticsc = cor(a,b)0.9760860118037878 然后，我们参考R 语言的定义计算p-value和95%的置信区间。 using Distributions ##载入包，用来计算T统计和t分布的cdf值using Statistics ##计算cor## 根据r和样本量n计算置信区间function interval(r::Float64, n::Int64) z = 0.5 * log(exp(1), (1 + r) / (1 - r)) s = 1 / (sqrt(n - 3)) zl = z - 1.96 * s zh = z + 1.96 * s low = (exp(2 * zl) - 1) / (exp(2 * zl) + 1) high = (exp(2 * zh) - 1) / (exp(2 * zh) + 1) (low, high)end## 输入两个向量，返回各个参数function cor_test(x::AbstractVector, y::AbstractVector) if length(x) == length(y) r = cor(x, y) #相关性值 n = length(x) df = n - 2 ## 自由度 t = r * sqrt((df) / (1 - r^2)) ##T统计量 tdist = cdf(TDist(df), t) ##t分布的累计 if r &gt; 0 pvalue = (1 - tdist) * 2 else pvalue = (tdist) * 2 end else println(&quot;请输入等长的两个向量&quot;) end fin = (r = r, pvalue = pvalue, df = df, t = t, cfi = interval(r, n))end 测试以下函数的返回是否和R语言一致 cor_test(a,b)(r = 0.9760860118037878, pvalue = 0.004423314933107214, df = 3, t = 7.777137710478182, cfi = (0.6757635765936858, 0.9984873283114327))c = cor_test(a,b)c.r0.9760860118037878c.pvalue0.004423314933107214","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"用julia实现bedtools的intersect-bed功能","slug":"用julia实现bedtools的intersect-bed功能","date":"2024-09-20T04:53:40.000Z","updated":"2024-10-18T05:33:14.314Z","comments":true,"path":"jl-intersect/","permalink":"https://xiaohanys.top/jl-intersect/","excerpt":"前言自己写的好几种算法企图实现bedtools的功能，虽然julia性能足够好，但都难以在效率上达到bedtools的性能，于是最后只能借助轮子了。","text":"前言自己写的好几种算法企图实现bedtools的功能，虽然julia性能足够好，但都难以在效率上达到bedtools的性能，于是最后只能借助轮子了。 代码using CSVusing DataFramesusing Tablesusing GenomicFeaturesfunction fs2NameTuple(fs::String) query = DataFrame(CSV.File(fs, delim=&quot;\\t&quot;, header=0)) if size(query)[2] == 5 query = rename!(query, :Column1 =&gt; :Chromosome, :Column2 =&gt; :Start, :Column3 =&gt; :End, :Column4 =&gt; :Name, :Column5 =&gt; :Score) # query = sort!(query, [:Chromosome,:Start]) query = Tables.rowtable(query) result = IntervalCollection([Interval(qy.Chromosome, qy.Start, qy.End, &#x27;?&#x27;, qy.Name) for qy in query], true) result elseif size(query)[2] == 4 query = rename!(query, :Column1 =&gt; :Chromosome, :Column2 =&gt; :Start, :Column3 =&gt; :End, :Column4 =&gt; :Name) # query = sort!(query, [:Chromosome,:Start]) query = Tables.rowtable(query) result = IntervalCollection([Interval(qy.Chromosome, qy.Start, qy.End, &#x27;?&#x27;, qy.Name) for qy in query], true) result else println(&quot;please confirm your bed files&quot;) endendfunction getInterval(A::IntervalCollection, B::IntervalCollection) for i in eachoverlap(A, B) println(i[1].seqname, &quot;\\t&quot;, i[1].first, &quot;\\t&quot;, i[1].last, &quot;\\t&quot;, i[1].metadata, &quot;\\t&quot;, i[2].seqname, &quot;\\t&quot;, i[2].first, &quot;\\t&quot;, i[2].last, &quot;\\t&quot;, i[2].metadata) endendgetInterval(fs2NameTuple(&quot;test1.bed&quot;),fs2NameTuple(&quot;test2.bed&quot;)) 原理首先通过CSV将bed文件读入内存，然后将dataFrame转化为nametuple，最后在转化为intervalCollection。通过内置的overlap功能可以几乎瞬间获取所有的交集，性能已经达到了c语言的程度。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"julia 入门指难","slug":"julia入门指难","date":"2024-09-20T04:52:36.000Z","updated":"2024-10-18T05:34:01.839Z","comments":true,"path":"jl-intro/","permalink":"https://xiaohanys.top/jl-intro/","excerpt":"如何安装Julia有很多方法，其中最简单的就是去各大景象站点下载编译好的二进制包，例如 清华大学开源软件镜像站 北京外国语大学开源软件镜像站 上海交通大学软件源镜像服务 另外，可以使用包管理工具jill下载安装，","text":"如何安装Julia有很多方法，其中最简单的就是去各大景象站点下载编译好的二进制包，例如 清华大学开源软件镜像站 北京外国语大学开源软件镜像站 上海交通大学软件源镜像服务 另外，可以使用包管理工具jill下载安装， 安装&#x2F;更新 jill： pip install jill --user -U (需要 Python 3.6 或更新的版本) 安装 Julia：jill install [VERSION] [--upstream UPSTREAM] [--confirm] 查询现存的上游镜像：jill upstream 帮助文档：jill [COMMAND] --help 利用 jill 安装完成后即可通过在命令行执行 julia/julia-1/julia-1.4 来启动不同版本的 Julia 当然，你也可以自己从源码构建安装，但是会很慢，并不推荐。 如何安装包julia 包管理器类似于R，可以进入REEL中，然后输入]，此时进入了安装包模式，输入add &#123;packages&#125;即可完成安装 安装包的速度很慢？因为Julia的包托管在github上，慢是很正常的，此时你需要一份镜像代理。 具体使用方法可以参考清华镜像julia使用指南 如何查找有哪些包？我列举几个网站，可以从这些站点寻找已经注册的包，小白用户就不要考虑未注册的包了。 julia-hub julia-observer julia-packages 有没有比较系统的教程？有，网易云课堂有免费的教学视频，相对全面且新手友好！版本也较新 julia可以使用jupyter notebook 吗？可以，在julia中安装IJulia包即可 可以买书学习吗？不推荐，因为julia在1.0版本前和版本后就像两个语言，有很大的变化，买的书很可能不起作用反而给你带来困扰。即使要买，也要看好版本，尽量选择1.0版本后的。 julia可以用来做生信分析吗？完全可以，BioJulia了解一下! 但是目前转录组的相关包还没有出现。转录组还是用R包吧！ 有问题去哪里问？julia有官方中文论坛，可以去那里提问问题，地址是julia中文社区你也可以加入官方QQ群：316628299 什么情况下我要学习Julia当你再也无法忍受python的蜗牛速度，而且无法通过算法优化来提升性能，你可以果断尝试Julia，或许会得到意想不到的速度提升。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"}],"author":"Xiaoguang Pan"},{"title":"再说转录组数据标准化（TPM，RPKM，FPKM）","slug":"再说转录组数据标准化","date":"2024-09-20T04:51:44.000Z","updated":"2024-10-18T05:33:24.846Z","comments":true,"path":"transcriptome-normlize/","permalink":"https://xiaohanys.top/transcriptome-normlize/","excerpt":"基础概念讲解在RNA-Seq的分析中，我们常用RPKM、FPKM和TPM作为转录组数据定量的表示方法。 它们都是对表达量进行标准化的方法，为何不直接用read数表示，而选标准化呢?","text":"基础概念讲解在RNA-Seq的分析中，我们常用RPKM、FPKM和TPM作为转录组数据定量的表示方法。 它们都是对表达量进行标准化的方法，为何不直接用read数表示，而选标准化呢? 因为落在一个基因区域内的read数目取决于基因长度和测序深度。基因越长read数目越多，测序深度越高,则一个基因对应的read数目也相对越多。所以必须要标准化，而标准化的对象就是基因长度与测序深度。 RPKM:Reads Per Kilobase of exon model per Million mapped reads (每千个碱基的转录每百万映射读取的reads)，主要用来对单端测序（single-end RNA-seq）进行定量的方法。 RPKM(推荐软件，Range) 的计算公式： $$RPKM&#x3D;\\frac{total\\ exon\\ reads}{(mapped\\ reads\\ (Millions)\\ \\times\\ exon\\ length(Kb))}$$ total exon reads：某个样本mapping到特定基因的外显子上的所有的reads； mapped reads ( Millions ) :某个样本的所有reads总和； **exon length( KB )**：某个基因的长度（外显子的长度的总和，以KB为单位）。 你可以用这个公式计算基因，外显子，转录本的表达，这里以基因的表达为例进行说明。在一个样本中一个基因的RPKM等于落在这个基因上的总的read数(total exon reads)与这个样本的总read数(mapped reads (Millions))和基因长度(exon length( KB )) 的乘积的比值。 FPKM:Fragments Per Kilobase of exon model per Million mapped fragments (每千个碱基的转录每百万映射读取的fragments)，主要是针对pair-end测序表达量进行计算。 FPKM (推荐软件，cufflinks) 和RPKM 的计算方法基本一致。 FPKM和RPKM的区别就是一个是fragment，一个是read。 对于单末端测序数据，由于Cufflinks计算的时候是将一个read当做一个fragment来算的，故而FPKM等同于RPKM。 对于双末端测序而言，如果一对paired-read都比对上了，那么这一对paired-read称之为一个fragment，而如果一对paired-Read中只有一个比对上了，另外一个没有比对上，那么就将这个比对上的read称之为一个fragment.而计算RPKM时，如果一对paired-read都比对上了会当成两个read计算，而如果一对paired-read中只有一个比对上了，另外一个没有比对上，那么就计read数为1。 故而即使是理论上将各个参数都设置成一样的，也并不能说FPKM&#x3D;2RPKM。对于单末端测序，虽然理论上FPKM等同于RPKM, 但是实际上即使是使用同一个mapping软件得到的mapping结果，然后再分别去计算同一个基因的RPKM (自己人工计算，或者用现成的一些软件都能算)和FPKM(用Cufflinks计算)，结果却仍然是不同，因为Cufflinks有自己的模型和自己的一些内在算法。 RPM&#x2F;CPM:Reads&#x2F;Counts of exon model per Million mapped reads (每百万映射读取的reads). RPM的计算公式： $$RPM&#x3D;\\ \\frac{total\\ exon\\ reads}{\\ mapped\\ reads\\ (Millions)}$$ total exon reads：某个样本mapping到特定基因的外显子上的所有的reads； mapped reads (Millions) :某个样本的所有reads总和； 这就是个占比统计，忽视了转录本长度的影响 TPM：Transcripts Per Kilobase of exonmodel per Million mapped reads (每千个碱基的转录每百万映射读取的Transcripts)，优化的RPKM计算方法，可以用于同一物种不同组织的比较。 TPM (推荐软件，RSEM) 的计算公式： $$TPM&#x3D;\\frac{N_i&#x2F;L_i*10^6}{sum(N_1&#x2F;L_1+N_2&#x2F;L_2+··+N_n&#x2F;L_n)}$$ Ni：mapping到基因i上的read数； Li：基因i的外显子长度的总和。 在一个样本中一个基因的TPM：先对每个基因的read数用基因的长度进行校正，之后再用校正后的这个基因read数(Ni&#x2F;Li)与校正后的这个样本的所有read数（sum(Ni&#x2F;Li+……..+ Nm&#x2F;Lm)）求商。由此可知，TPM概括了基因的长度、表达量和基因数目。TPM可以用于同一物种不同组织间的比较，因为sum值总是唯一的。 该选择哪个作为我的标准化方法？TPM一定是万金油？从概念上我们可以知道，RPKM和FPKM可以优化样本内的基因比较，但是在样本之间比较时，会存在很大的偏见。这个显而易见，因为他们在不同样本之间的总和都不一致。 因此，为了可以在多个样本之间比较基因表达量的差异，TPM应运而出，但是TPM真就是万能的吗？ 我们可以举一个简单的例子，假设有4个基因，长度都是3bp,readcounts也都是3个，那么，每个基因的TPM&#x3D;1&#x2F;4. 假设另一个样本也有这4个基因，但是最后一个基因由于表达量升高而变成了15，那么，前三个基因的TPM&#x3D;1&#x2F;8，而最后一个差异基因的TPM&#x3D;5&#x2F;8. 这里就引入了一个偏见，因为TPM值是相对表达量，本质上仍属于比例的一种，那么这个相对表达会在总值（分母）改变时发生变化，那么，我们再比较基因的时候，发现这四个基因都发生了表达量的改变，而实际上只有最后一个是差异基因。 那为什么很多文献说TPM可以用来比较多个样本之间的差异呢？我认为他们是默认了样本之间不存在批次效应，即测序深度是一致的或者说相近的，这样保证了分母不会有巨大的变化，那么相对表达就是在样本之间比较差异的金标准。 但是，如果样本之间存在测序深度的差异，那么使用TPM比较样本差异必然会引入偏见，这也是为什么Deseq2等差异分析软件不会去选择TPM作为输入了。 样本间的标准化方法假设组间差异成分很大，或者存在很大的批次效应。我们不能用TPM去比较差异，而我们的差异分析软件就会假设我们的Raw Read Counts符合负二项分布的模式，假设大部分基因都不是差异基因，差异基因发生在少数基因身上，因此，只有readcount才能符合假设，我们才能用组间矫正的方法例如edger包的TMM方法。 一个理想的方法便是对TPM执行组间矫正，但是TPM本身并不符合负二项分布的模式，因此会引入偏见。 TPM如何计算？ Resem Salmon FeatureCounts的定量结果如何计算TPM？ 答案也很简单，我们可以按照公式计算即可，例如这样： library(dplyr)df=tibble(gene=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),length=c(12,21,33,45), readCount=c(120,13,26,390))df&lt;-df%&gt;%mutate(Ratio=readCount/length)%&gt;% mutate(Sum=sum(Ratio),TPM=Ratio/Sum*1e6) 这样就可以简单的计算TPM了，但是这里的并非准确的TPM值，因为我们需要的TPM的length为有效长度而非转录本的长度，而 $$EffLength&#x3D;feature Length - average fragment length +1$$ 当然这里的有效长度依然是估计，所以我们需要使用额外的软件去计算插入长度，从而求得片段长度。例如picard 最后，我们可以从比对后的BAM文件直接获取基因的定量TPM，通过TPMCalculator软件 文献：TPMCalculator: One-Step Software to Quantify mRNA Abundance of Genomic Features脚本：Github:TPMCalculator","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"},{"name":"转录组","slug":"转录组","permalink":"https://xiaohanys.top/tags/%E8%BD%AC%E5%BD%95%E7%BB%84/"}],"author":"Xiaoguang Pan"},{"title":"单细胞数据如何混合亚类和大类做点图分析Marker基因","slug":"单细胞数据如何混合亚类和大类做点图分析Marker基因","date":"2024-09-20T04:50:55.000Z","updated":"2024-10-18T05:32:06.207Z","comments":true,"path":"scrna-markplot/","permalink":"https://xiaohanys.top/scrna-markplot/","excerpt":"单细胞数据数据量很大，加重了分析的负担，但只要掌握好的方法和工具，就可以无往而不利。今年要说的这个如题，是因为在区分亚类的时候，提取了大类型并调整分辨率重新聚类计算的亚类。针对这种情况，该如何实现呢？","text":"单细胞数据数据量很大，加重了分析的负担，但只要掌握好的方法和工具，就可以无往而不利。今年要说的这个如题，是因为在区分亚类的时候，提取了大类型并调整分辨率重新聚类计算的亚类。针对这种情况，该如何实现呢？ 问题分析其实该问题可以简化为把每个亚类的分类信息提取出来并给大类进行赋值，然后使用Seurat内置的DotPlot功能进行作图，样式可以微调。 解决方法有了方案，解决起来就简单了！ 首先，markers基因先输入，然后把大类读入内存并操作一下library(RColorBrewer)library(ggplot2)library(Seurat)markers&lt;-c(&quot;PDGFRB&quot;,&quot;DCN&quot;,&quot;LUM&quot;,&quot;DKK1&quot;,&quot;ESR1&quot;,&quot;PGR&quot;,&quot;NCAM1&quot;,&quot;ITGA1&quot;,&quot;FCGR3A&quot;,&quot;CD3E&quot;,&quot;TRAC&quot;,&quot;CD8A&quot;,&quot;ACTA2&quot;,&quot;RGS5&quot;,&quot;TAGLN&quot;,&quot;ADGRL4&quot;,&quot;DIPK2B&quot;,&quot;CD34&quot;,&quot;KDR&quot;,&quot;GP1BA&quot;,&quot;CD14&quot;,&quot;PROM1&quot;,&quot;MCAM&quot;,&quot;CD36&quot;,&quot;VCAM1&quot;,&quot;VWF&quot;,&quot;DEFB1&quot;,&quot;GPX3&quot;,&quot;FOXJ1&quot;,&quot;SNTN&quot;,&quot;CD14&quot;,&quot;CD68&quot;,&quot;CD4&quot;,&quot;CD163&quot;,&quot;CD19&quot;,&quot;IGKC&quot;,&quot;JCHAIN&quot;,&quot;CD1C&quot;,&quot;THBD&quot;,&quot;CLEC9A&quot;,&quot;CBR3&quot;)endo&lt;-readRDS(&quot;all_20_samples_after_cluster_0.09.rds&quot;)#这里读取大类###给大类重命名new.cluster.ids &lt;- c(&quot;Stroma Cell&quot;, &quot;NK and T Cell&quot;, &quot;Luminal Epithelial Cell&quot;, &quot;Macrophages and Dendritic&quot;, &quot;Progenitor Cell&quot;, &quot;Endothelial Cell&quot;, &quot;Smooth Muscle Cell&quot;, &quot;Ciliated Epithelial Cell&quot;)names(new.cluster.ids) &lt;- levels(endo)endo &lt;- RenameIdents(endo, new.cluster.ids)all_cell&lt;-Idents(endo)need&lt;-as.character(all_cell)names(need)&lt;-names(all_cell) 然后，把需要的三个亚类读入内容并处理亚类和大类之间的映射关系for (i in c(1,3,4))&#123; fs_name&lt;-paste0(&quot;./cluster_&quot;,i,&quot;/cluster_&quot;,i,&quot;.afterclu.rds&quot;) cls&lt;-readRDS(fs_name) cell.type&lt;-Idents(cls) cls_cluster&lt;-paste(new.cluster.ids[i+1],cell.type,sep=&quot;_&quot;) names(cls_cluster)&lt;-names(cell.type) need[which(need==new.cluster.ids[i+1])]&lt;-cls_cluster rm(cls)&#125; 把新的到的分组信息传入到大类中并设置好因子顺序，将其定义为Identsendo$new_group&lt;-needendo$new_group&lt;-factor(endo$new_group,levels=c(&quot;Stroma Cell&quot;,&quot;NK and T Cell_0&quot;,&quot;NK and T Cell_1&quot;,&quot;NK and T Cell_2&quot;,&quot;NK and T Cell_3&quot;,&quot;Smooth Muscle Cell&quot;,&quot;Endothelial Cell&quot;,&quot;Progenitor Cell_0&quot;,&quot;Progenitor Cell_1&quot;,&quot;Progenitor Cell_2&quot;,&quot;Progenitor Cell_3&quot;,&quot;Luminal Epithelial Cell&quot;,&quot;Ciliated Epithelial Cell&quot;,&quot;Macrophages and Dendritic_0&quot;,&quot;Macrophages and Dendritic_1&quot;,&quot;Macrophages and Dendritic_2&quot;,&quot;Macrophages and Dendritic_3&quot;))Idents(endo)&lt;-&quot;new_group&quot; 最后作图微调并保存DefaultAssay(endo)&lt;-&quot;RNA&quot;DotPlot(endo,features=unique(markers),cols=c(&quot;white&quot;,&quot;red&quot;))+ RotatedAxis()+ theme_test()+ theme(axis.text=element_text(size=6,face=&quot;bold&quot;),axis.title=element_blank(),legend.position=&quot;bottom&quot;,legend.text=element_text(size=5.5),legend.title=element_text(size=6,face=&quot;bold&quot;),axis.text.x=element_text(angle = 90, hjust = 1, vjust = .5)) 图形如下： 另外的图？针对以上的方法，我们可以把每个亚类的关系都映射到大类的umap图上，以尽量展示所有的关系 library(RColorBrewer)library(ggplot2)library(Seurat)endo&lt;-readRDS(&quot;all_20_samples_after_cluster_0.09.rds&quot;)#这里读取大类###给大类重命名new.cluster.ids &lt;- c(&quot;Stroma Cell&quot;, &quot;NK and T Cell&quot;, &quot;Luminal Epithelial Cell&quot;, &quot;Macrophages and Dendritic&quot;, &quot;Progenitor Cell&quot;, &quot;Endothelial Cell&quot;, &quot;Smooth Muscle Cell&quot;, &quot;Ciliated Epithelial Cell&quot;)names(new.cluster.ids) &lt;- levels(endo)endo &lt;- RenameIdents(endo, new.cluster.ids)all_cell&lt;-Idents(endo)need&lt;-as.character(all_cell)names(need)&lt;-names(all_cell)for (i in c(1:7))&#123; fs_name&lt;-paste0(&quot;./cluster_&quot;,i,&quot;/cluster_&quot;,i,&quot;.afterclu.rds&quot;) cls3&lt;-readRDS(fs_name)#这里读取亚类 cell.type&lt;-Idents(cls3) cls3_cluster&lt;-paste(new.cluster.ids[i+1],cell.type,sep=&quot;_&quot;) names(cls3_cluster)&lt;-names(cell.type) need[which(need2==new.cluster.ids[i+1])]&lt;-cls3_cluster rm(cls3)&#125;endo$new_id&lt;-needDimPlot(endo,reduction=&quot;umap&quot;,group.by=&quot;new_id&quot;,cols=colorRampPalette(brewer.pal(29,&quot;Set1&quot;))(29))+theme(legend.text=element_text(size=6))ggsave(&quot;dimplot_for_newlable.png&quot;,width=7,height=4,dpi=300)","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"单细胞测序","slug":"单细胞测序","permalink":"https://xiaohanys.top/tags/%E5%8D%95%E7%BB%86%E8%83%9E%E6%B5%8B%E5%BA%8F/"},{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"},{"title":"ClusterProfiler:真的不只是富集分析","slug":"ClusterProfiler真的不只是富集分析","date":"2024-09-20T04:48:58.000Z","updated":"2024-10-18T05:33:32.114Z","comments":true,"path":"clusterprofiler-not-enrichment/","permalink":"https://xiaohanys.top/clusterprofiler-not-enrichment/","excerpt":"网上很多教程都在讲Y叔的clusterprofile富集分析的教程，但是查阅了官方文档后才知道，这个包真的不仅仅只有这个功能，其他功能也很强大。","text":"网上很多教程都在讲Y叔的clusterprofile富集分析的教程，但是查阅了官方文档后才知道，这个包真的不仅仅只有这个功能，其他功能也很强大。 做ID转换ID转换应该是基因下游分析的敲门砖了，因为一般注释用的是ENSMEBL ID，但是这个ID是人类无法识别的一串数字，下游分析功能都需要转换成基因名称或者基因ID，有这个bitr功能就方便了很多。 x &lt;- c(&quot;GPX3&quot;, &quot;GLRX&quot;, &quot;LBP&quot;, &quot;CRYAB&quot;, &quot;DEFB1&quot;, &quot;HCLS1&quot;, &quot;SOD2&quot;, &quot;HSPA2&quot;, &quot;ORM1&quot;, &quot;IGFBP1&quot;, &quot;PTHLH&quot;, &quot;GPC3&quot;, &quot;IGFBP3&quot;,&quot;TOB1&quot;, &quot;MITF&quot;, &quot;NDRG1&quot;, &quot;NR1H4&quot;, &quot;FGFR3&quot;, &quot;PVR&quot;, &quot;IL6&quot;, &quot;PTPRM&quot;, &quot;ERBB2&quot;, &quot;NID2&quot;, &quot;LAMB1&quot;, &quot;COMP&quot;, &quot;PLS3&quot;, &quot;MCAM&quot;, &quot;SPP1&quot;, &quot;LAMC1&quot;, &quot;COL4A2&quot;, &quot;COL4A1&quot;, &quot;MYOC&quot;, &quot;ANXA4&quot;, &quot;TFPI2&quot;, &quot;CST6&quot;, &quot;SLPI&quot;, &quot;TIMP2&quot;, &quot;CPM&quot;, &quot;GGT1&quot;, &quot;NNMT&quot;, &quot;MAL&quot;, &quot;EEF1A2&quot;, &quot;HGD&quot;, &quot;TCN2&quot;, &quot;CDA&quot;, &quot;PCCA&quot;, &quot;CRYM&quot;, &quot;PDXK&quot;, &quot;STC1&quot;, &quot;WARS&quot;, &quot;HMOX1&quot;, &quot;FXYD2&quot;, &quot;RBP4&quot;, &quot;SLC6A12&quot;, &quot;KDELR3&quot;, &quot;ITM2B&quot;)eg = bitr(x, fromType=&quot;SYMBOL&quot;, toType=&quot;ENTREZID&quot;, OrgDb=&quot;org.Hs.eg.db&quot;)head(eg) ## SYMBOL ENTREZID## 1 GPX3 2878## 2 GLRX 2745## 3 LBP 3929## 4 CRYAB 1410## 5 DEFB1 1672## 6 HCLS1 3059 想知道你可以进行哪些ID 转换？ library(org.Hs.eg.db)keytypes(org.Hs.eg.db) ## [1] &quot;ACCNUM&quot; &quot;ALIAS&quot; &quot;ENSEMBL&quot; ## [4] &quot;ENSEMBLPROT&quot; &quot;ENSEMBLTRANS&quot; &quot;ENTREZID&quot; ## [7] &quot;ENZYME&quot; &quot;EVIDENCE&quot; &quot;EVIDENCEALL&quot; ## [10] &quot;GENENAME&quot; &quot;GO&quot; &quot;GOALL&quot; ## [13] &quot;IPI&quot; &quot;MAP&quot; &quot;OMIM&quot; ## [16] &quot;ONTOLOGY&quot; &quot;ONTOLOGYALL&quot; &quot;PATH&quot; ## [19] &quot;PFAM&quot; &quot;PMID&quot; &quot;PROSITE&quot; ## [22] &quot;REFSEQ&quot; &quot;SYMBOL&quot; &quot;UCSCKG&quot; ## [25] &quot;UNIGENE&quot; &quot;UNIPROT&quot; Note：虽然GO分析支持很多ID，例如symbol完全可以直接运行，但还是建议都转换为ENTREZID，毕竟，我们很少直接得到symbol啊。 GSEA分析还在使用官方的GSEA java包进行富集分析吗，你需要准备gct文件（表达矩阵），要转换为symbol或者entrezid,还要准备cls文件（样本特征），还要下载好gmt文件，最后出来的分析结果，各种图片都是不好看的，准确说不够灵活，用R包呢？ GSEA_GO ego3 &lt;- gseGO(geneList = geneList, OrgDb = org.Hs.eg.db, ont = &quot;CC&quot;, nPerm = 1000, minGSSize = 100, maxGSSize = 500, pvalueCutoff = 0.05, verbose = FALSE) geneList 是什么？ 类似于做富集（过表达）分析，geneList是一列基因id，而GSEA分析一般需要基因表达量来衡量富集分数，但是从原理上来讲，还是根据表达量计算Fordchange,然后给他排序，看这个排序在基因集中的富集程度。这样就不会因为传统分析中先筛选差异基因而过滤掉的低差异基因信息（详情参考GSEA的原理），所以，这个基因列表是指一列排序后的以基因名称为名字的log2FC值向量。 假设你有一个两列的文件，第一列为名字，第二列为logFC,你可以这样： d &lt;- read.csv(your_csv_file)##第一列为基因ID##第二列为差异值## 获取log2FCgeneList &lt;- d[,2]## 命名names(geneList) &lt;- as.character(d[,1])## 降序geneList &lt;- sort(geneList, decreasing = TRUE)head(geneList) 剩余都是一些限制参数，请执行?clusterProfiler::gseGO ## 4312 8318 10874 55143 55388 991 ## 4.572613 4.514594 4.418218 4.144075 3.876258 3.677857 GSEA-KEGG kk2 &lt;- gseKEGG(geneList = geneList, organism = &#x27;hsa&#x27;, nPerm = 1000, minGSSize = 120, pvalueCutoff = 0.05, verbose = FALSE)head(kk2) ## ID Description setSize## hsa04510 hsa04510 Focal adhesion 188## hsa04151 hsa04151 PI3K-Akt signaling pathway 322## hsa03013 hsa03013 RNA transport 131## hsa05152 hsa05152 Tuberculosis 162## hsa04062 hsa04062 Chemokine signaling pathway 165## hsa04218 hsa04218 Cellular senescence 143## enrichmentScore NES pvalue p.adjust## hsa04510 -0.4188582 -1.706291 0.001430615 0.02322097## hsa04151 -0.3482755 -1.497042 0.002614379 0.02322097## hsa03013 0.4116488 1.735751 0.003095975 0.02322097## hsa05152 0.3745153 1.630500 0.003154574 0.02322097## hsa04062 0.3754101 1.633635 0.003184713 0.02322097## hsa04218 0.4153718 1.772207 0.003194888 0.02322097## qvalues rank leading_edge## hsa04510 0.01576976 2183 tags=27%, list=17%, signal=23%## hsa04151 0.01576976 1997 tags=23%, list=16%, signal=20%## hsa03013 0.01576976 3383 tags=40%, list=27%, signal=29%## hsa05152 0.01576976 2823 tags=34%, list=23%, signal=27%## hsa04062 0.01576976 1298 tags=21%, list=10%, signal=19%## hsa04218 0.01576976 1155 tags=17%, list=9%, signal=16%## core_enrichment## hsa04510 5595/5228/7424/1499/4636/83660/7059/5295/1288/23396/3910/3371/3082/1291/394/3791/7450/596/3685/1280/3675/595/2318/3912/1793/1278/1277/1293/10398/55742/2317/7058/25759/56034/3693/3480/5159/857/1292/3908/3909/63923/3913/1287/3679/7060/3479/10451/80310/1311/1101## hsa04151 627/2252/7059/92579/5563/5295/6794/1288/7010/3910/3371/3082/1291/4602/3791/1027/90993/3441/3643/1129/2322/1975/7450/596/3685/1942/2149/1280/4804/3675/595/2261/7248/2246/4803/3912/1902/1278/1277/2846/2057/1293/2247/55970/5618/7058/10161/56034/3693/4254/3480/4908/5159/1292/3908/2690/3909/8817/9223/4915/3551/2791/63923/3913/9863/3667/1287/3679/7060/3479/80310/1311/5105/2066/1101## hsa03013 10460/1978/55110/54913/9688/8894/11260/10799/9631/4116/5042/8761/6396/23165/8662/10248/55706/79833/9775/29107/23636/5905/9513/5901/10775/10557/4927/79902/1981/26986/11171/10762/8480/8891/11097/26019/10940/4686/9972/81929/10556/3646/9470/387082/1977/57122/8563/7514/79023/3837/9818/56000## hsa05152 820/51806/6772/64581/3126/3112/8767/3654/1054/1051/3458/1520/11151/1594/50617/54205/91860/8877/3329/637/3689/7096/2207/3929/4360/5603/929/533/3452/6850/7124/1509/3569/7097/1378/8772/64170/3119/843/2213/8625/3920/2215/3587/5594/3593/9103/3592/6300/9114/10333/3109/3108/1432/3552## hsa04062 3627/10563/6373/4283/6362/6355/2921/6364/3576/6352/10663/1230/6772/6347/6351/3055/1237/1236/4067/6354/114/3702/6361/1794/1234/6367/6375/6374/2919/409/4793/2792/6360/5880## hsa04218 结果里面包含了所有GSEA计算的结果数值 setSize:基因集的大小enrichmentScore:富集打分NES：标准化后的富集打分pvalue,p.ajust.qvalues:各种显著性检验rank:log2FC的排序位置 根据官方提供的gmt文件或者自己做gmt文件进行分析 假设你从官网下载了Hallmark基因集 wp2gene &lt;- read.gmt(&quot;h.all.v7.0.entrez.gmt&quot;)em2 &lt;- GSEA(geneList, TERM2GENE = wp2gene) 这样就可以直接得到富集分析的结果，非常方便 另外，也可以通过msigdbr包直接获取基因集信息，但是感觉灵活性不高。 对GSEA的结果可视化anno &lt;- edo2[1, c(&quot;NES&quot;, &quot;pvalue&quot;, &quot;p.adjust&quot;)]lab &lt;- paste0(names(anno), &quot;=&quot;, round(anno, 3), collapse=&quot;\\n&quot;)gseaplot2(edo2, geneSetID = 1, title = edo2$Description[1])+annotate(&quot;text&quot;, 0.7, edo2[i, &quot;enrichmentScore&quot;] * .9, label = lab, hjust=0, vjust=0) pp &lt;- lapply(1:3, function(i) &#123; anno &lt;- edo2[i, c(&quot;NES&quot;, &quot;pvalue&quot;, &quot;p.adjust&quot;)] lab &lt;- paste0(names(anno), &quot;=&quot;, round(anno, 3), collapse=&quot;\\n&quot;) gsearank(edo2, i, edo2[i, 2]) + xlab(NULL) +ylab(NULL) + annotate(&quot;text&quot;, 0, edo2[i, &quot;enrichmentScore&quot;] * .9, label = lab, hjust=0, vjust=0)&#125;)plot_grid(plotlist=pp, ncol=1) 使结果可读性提升针对分析结果，GO富集可以设置参数readable = TRUE，但是对KEGG无法使用，因此，可以使用setReadable library(org.Hs.eg.db)library(clusterProfiler)data(geneList, package=&quot;DOSE&quot;)de &lt;- names(geneList)[1:100]x &lt;- enrichKEGG(de)## The geneID column is ENTREZIDhead(x, 3) ## ID Description GeneRatio BgRatio## hsa04110 hsa04110 Cell cycle 8/48 124/7932## hsa04218 hsa04218 Cellular senescence 7/48 160/7932## hsa04114 hsa04114 Oocyte meiosis 6/48 128/7932## pvalue p.adjust qvalue## hsa04110 6.356283e-07 7.182599e-05 6.490099e-05## hsa04218 4.377944e-05 2.473538e-03 2.235055e-03## hsa04114 1.105828e-04 4.165285e-03 3.763695e-03## geneID Count## hsa04110 8318/991/9133/890/983/4085/7272/1111 8## hsa04218 2305/4605/9133/890/983/51806/1111 7## hsa04114 991/9133/983/4085/51806/6790 6 y &lt;- setReadable(x, OrgDb = org.Hs.eg.db, keyType=&quot;ENTREZID&quot;)## The geneID column is translated to symbolhead(y, 3) ## ID Description GeneRatio BgRatio## hsa04110 hsa04110 Cell cycle 8/48 124/7932## hsa04218 hsa04218 Cellular senescence 7/48 160/7932## hsa04114 hsa04114 Oocyte meiosis 6/48 128/7932## pvalue p.adjust qvalue## hsa04110 6.356283e-07 7.182599e-05 6.490099e-05## hsa04218 4.377944e-05 2.473538e-03 2.235055e-03## hsa04114 1.105828e-04 4.165285e-03 3.763695e-03## geneID## hsa04110 CDC45/CDC20/CCNB2/CCNA2/CDK1/MAD2L1/TTK/CHEK1## hsa04218 FOXM1/MYBL2/CCNB2/CCNA2/CDK1/CALML5/CHEK1## hsa04114 CDC20/CCNB2/CDK1/MAD2L1/CALML5/AURKA## Count## hsa04110 8## hsa04218 7## hsa04114 6 你甚至可以用它来对单细胞聚类结果进行注释如果我们有一个大的细胞marker库，然后我们有显著差异的基因marker,那寻找细胞类型就和过表达分析是一种情况了，都是采用超几何分布计算概率，所以： cell_markers &lt;- vroom::vroom(&#x27;http://bio-bigdata.hrbmu.edu.cn/CellMarker/download/Human_cell_markers.txt&#x27;) %&gt;% tidyr::unite(&quot;cellMarker&quot;, tissueType, cancerType, cellName, sep=&quot;, &quot;) %&gt;% dplyr::select(cellMarker, geneID) %&gt;% dplyr::mutate(geneID = strsplit(geneID, &#x27;, &#x27;))cell_markers 你甚至可以从cellmarker官网直接下载所有的细胞marker ## # A tibble: 2,868 x 2## cellMarker geneID ## &lt;chr&gt; &lt;list&gt; ## 1 Kidney, Normal, Proximal tubular cell &lt;chr [1…## 2 Liver, Normal, Ito cell (hepatic stellate cell) &lt;chr [1…## 3 Endometrium, Normal, Trophoblast cell &lt;chr [1…## 4 Germ, Normal, Primordial germ cell &lt;chr [1…## 5 Corneal epithelium, Normal, Epithelial cell &lt;chr [1…## 6 Placenta, Normal, Cytotrophoblast &lt;chr [1…## 7 Periosteum, Normal, Periosteum-derived progenit… &lt;chr [4…## 8 Amniotic membrane, Normal, Amnion epithelial ce… &lt;chr [2…## 9 Primitive streak, Normal, Primitive streak cell &lt;chr [2…## 10 Adipose tissue, Normal, Stromal vascular fracti… &lt;chr [1…## # … with 2,858 more rows y &lt;- enricher(gene, TERM2GENE=cell_markers, minGSSize=1)DT::datatable(as.data.frame(y)) 这样就找到了细胞类型，你可以过滤占比最高，且p值显著的结果。","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"单细胞测序","slug":"单细胞测序","permalink":"https://xiaohanys.top/tags/%E5%8D%95%E7%BB%86%E8%83%9E%E6%B5%8B%E5%BA%8F/"},{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"},{"title":"python, perl 和julia的性能对比","slug":"和julia的性能对比","date":"2024-09-20T04:44:08.000Z","updated":"2024-10-18T05:32:11.134Z","comments":true,"path":"jl-py-pr-compair/","permalink":"https://xiaohanys.top/jl-py-pr-compair/","excerpt":"在生物信息学中经常用到的脚本语言主要是python和perl，他们被用来处理文本，大量统计，流程控制等等，其自身也是各有优势。比如说perl天生就为了处理文本而生，但是python确是有名的胶水语言，特别在整合C代码时显示出巨大的优势，其语法简洁易懂，易于维护更让其成为仅次于C和JAVA的第三大语言，但其糟糕的性能在处理大量循环时会让人忍不住抓狂。因此，Julia语言应运而生，其控制了python中没必要的动态性，加之使用JIT技术让其能够保有高性能的同时具备简洁的语法。","text":"在生物信息学中经常用到的脚本语言主要是python和perl，他们被用来处理文本，大量统计，流程控制等等，其自身也是各有优势。比如说perl天生就为了处理文本而生，但是python确是有名的胶水语言，特别在整合C代码时显示出巨大的优势，其语法简洁易懂，易于维护更让其成为仅次于C和JAVA的第三大语言，但其糟糕的性能在处理大量循环时会让人忍不住抓狂。因此，Julia语言应运而生，其控制了python中没必要的动态性，加之使用JIT技术让其能够保有高性能的同时具备简洁的语法。 Codon是一个高性能的Python编译器，它将Python代码编译为本地机器代码，而不需要任何运行时开销。Python上的典型加速在单个线程上大约为10-100x或更多。Codon的性能通常与C&#x2F;C++不相上下。与Python不同，Codon支持本机多线程，这会导致速度提高很多倍。Codon可通过插件基础设施进行扩展，使您能够合并新的库、编译器优化甚至关键字。 现在，让我们测试codon是否能给python提速，在此之前，我们需要修改以下python的代码 import sysdef calculateGC(sequence:str)-&gt;Tuple[int,int]: &quot;&quot;&quot;Calculate the GC content of a DNA sequence&quot;&quot;&quot; gc = 0 allnumber = 0 for i in sequence: if i != &#x27;N&#x27; and i != &#x27;n&#x27;: allnumber += 1 if i == &#x27;G&#x27; or i == &#x27;C&#x27; or i == &#x27;g&#x27; or i == &#x27;c&#x27;: gc += 1 return gc, allnumberdef main(file:str): &quot;&quot;&quot;Main function&quot;&quot;&quot; gcNum = 0 allNum = 0 with open(file,&#x27;r&#x27;) as f: for line in f: line=line.strip() if line.startswith(&#x27;&gt;&#x27;): continue else: gc, allnumber = calculateGC(line) gcNum += gc allNum += allnumber print(f&#x27;GC content is: &#123;gcNum/allNum:.3f&#125;&#x27;)main(sys.argv[1]) 然后运行 codon build --release -o calGC calGC.py 最后速度为： time ./calGC ../../Project/DataBase/hg38.fa GC content is: 0.410./calGC ../../Project/DataBase/hg38.fa 22.30s user 2.75s system 111% cpu 22.421 total 速度还不错，已经可以超越不用Biojulia包的julia函数了。 2022&#x2F;11&#x2F;14更新： 最近python3.11出来了，据说性能有很大的提升, 一位国外的小哥(Dennis Bakhuis)采用简单的蒙特卡洛预测圆周率的方式测试循环的性能，发现Python3.11性能确实是突飞猛进，同一时间，Julia也已经更新到V1.8了，于是我在他的github下贡献了julia版本的代码，希望继续比较多个语言的计算特性。 总而言之，虽然python性能进一步优化，但和julia相比，速度依旧不够打，1000000个循环，python3.11用时6秒，而julia仅需要0.033秒。具体可以看githubhttps://github.com/dennisbakhuis/python3.11_speedtest 在生物信息学中经常用到的脚本语言主要是python和perl，他们被用来处理文本，大量统计，流程控制等等，其自身也是各有优势。比如说perl天生就为了处理文本而生，但是python确是有名的胶水语言，特别在整合C代码时显示出巨大的优势，其语法简洁易懂，易于维护更让其成为仅次于C和JAVA的第三大语言，但其糟糕的性能在处理大量循环时会让人忍不住抓狂。因此，Julia语言应运而生，其控制了python中没必要的动态性，加之使用JIT技术让其能够保有高性能的同时具备简洁的语法。 说了那么多，在生物信息上我们经常需要处理大量的文本文件，例如Fasta格式的序列文件，那么三者又是谁快呢？ 版本控制 python3 &#x3D; 3.8.3 perl &#x3D; 5.26.2 julia &#x3D; 1.5.0-beta system &#x3D; centos 8 计算内容从UCSC上下载人类参考基因组 hg38.fa.gz 并解压，计算基因组GC含量，N碱基不算在总长中。 代码perl #!/usr/bin/perl -wuse strict;if(@ARGV &lt; 1)&#123; die &quot;Usage : perl $0 &lt;genome.fa&gt;\\n&quot;;&#125;my $input = shift @ARGV;my ($sum,$G_num,$C_num,$N_num)=(0,0,0,0);my $id;open IN, &quot;&lt; $input&quot; or die $!;while(my $line = &lt;IN&gt;)&#123; chomp $line; if($line =~ /&gt;([^\\s]+)/)&#123; $id = $1; &#125;else&#123; $sum += length($line); $G_num += ($line =~ tr/Gg/Gg/); $C_num += ($line =~ tr/Cc/Cc/); $N_num += ($line =~ tr/Nn/Nn/); &#125;&#125;close IN;my $GC_rate = ($G_num+$C_num)/($sum-$N_num);printf &quot;GC content: %.3f \\n&quot;,$GC_rate; julia function lineGC(seq::String) GCnumber=count(x-&gt;(x==&#x27;G&#x27;||x==&#x27;C&#x27;||x==&#x27;g&#x27;||x==&#x27;c&#x27;),seq) lineNum=count(x-&gt;(x!=&#x27;N&#x27; &amp;&amp; x!=&#x27;n&#x27;),seq) (GCnumber,lineNum)endfunction calGC(fs) GCnumber=zero(Int) lineNum=zero(Int) open(fs,&quot;r&quot;) do IOstream for line in eachline(IOstream) if startswith(line,&quot;&gt;&quot;) continue else GC,all=lineGC(line) GCnumber+=GC lineNum+=all end end end round(GCnumber/lineNum;digits=3)endprintln(&quot;GC content: &quot;,calGC(ARGS[1])) python import sysdef lineGC(seq): tmp=[base for base in seq if base ==&quot;G&quot; or base ==&quot;g&quot; or base == &quot;C&quot; or base == &quot;c&quot;] gcNumber=len(tmp) tmp2=[base for base in seq if base !=&quot;N&quot; and base !=&quot;n&quot;] allNumber=len(tmp2) return (gcNumber,allNumber)with open(sys.argv[1],&#x27;r&#x27;) as f: gcNum=0 allNum=0 for line in f: if line.startswith(&quot;&gt;&quot;): continue else: gc,alln=lineGC(line.strip(&quot;\\n&quot;)) gcNum=gcNum+gc allNum=allNum+allnprint(&quot;GC content: &#123;:.3f&#125;&quot;.format(gcNum/allNum)) 运行时间测试python julia perl 总结结果令人咋舌，可以从sys时间看出来python和perl都是立马启动，而julia在函数的即时编译上花了一点时间（一半时间）。 总体用时上，julia仅比perl快了1秒，而python却用了惊人的9分钟，😭 后记python 也不是这么不堪，想要提速还是可以有很多办法的，比如切换pypy, 或者也用正则表达式，例如： import sysimport redef lineGC(seq): pattern_1 = re.compile(r&quot;G|C&quot;,re.I) pattern_2 = re.compile(r&quot;N&quot;,re.I) gcNumber=len(pattern_1.findall(seq)) allNumber=len(seq)-len(pattern_2.findall(seq)) return (gcNumber,allNumber)with open(sys.argv[1],&#x27;r&#x27;) as f: gcNum=0 allNum=0 for line in f: if line.startswith(&quot;&gt;&quot;): continue else: gc,alln=lineGC(line.strip(&quot;\\n&quot;)) gcNum=gcNum+gc allNum=allNum+allnprint(&quot;GC content: &#123;:.3f&#125;&quot;.format(gcNum/allNum)) 这样计算下来，大概需要6分20秒，提速了一半 另外，我们也可以使用NumPy的向量化运算来提速 import sysfrom pyfaidx import Fastaimport numpy as npdef lineGC(seq): gc_number = np.where((seq==b&#x27;G&#x27;)|(seq==b&#x27;C&#x27;)|(seq==b&#x27;g&#x27;)|(seq==b&#x27;c&#x27;))[0].shape[0] n_number = np.where((seq==b&#x27;N&#x27;)|(seq==b&#x27;n&#x27;))[0].shape[0] allnumber = seq.shape[0] - n_number return (gc_number,allnumber)def calGC(fs): GC = 0 all = 0 hg38 = Fasta(fs) for record in hg38: seq = np.asarray(record) gc_number,all_number=lineGC(seq) GC = GC + gc_number all = all + all_number return (GC, all)if __name__ == &quot;__main__&quot;: gcNum, allNum = calGC(sys.argv[1]) print(&quot;GC content: &#123;:.3f&#125;&quot;.format(gcNum/allNum)) 这样的话，就只需要2分22秒了，已经是非常快的了，但是和perl还是有差距的。 最后，难道julia真的速度和perl就相差无几吗? 答案是否定的，因为julia设计是为了科学计算的，但是其字符串的性能并算不上优秀，我们可以调用BioSequence来处理生物序列 using BioSequencesusing FASTXfunction lineGC(seq) GCnumber=count(x-&gt;(x==DNA_G||x==DNA_C),seq) lineNum=length(seq)-count(isambiguous,seq) GCnumber,lineNumendfunction calGC(fs) GCnumber=zero(Int) lineNum=zero(Int) reader=open(FASTA.Reader,fs) for record in reader GC,all=lineGC(FASTA.sequence(record)) GCnumber+=GC lineNum+=all end close(reader) round(GCnumber/lineNum;digits=3)endprintln(&quot;GC content: &quot;,calGC(ARGS[1])) 这样就只需要11秒就可以计算出答案了","categories":[{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"},{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"},{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"}],"author":"Xiaoguang Pan"},{"title":"用streamlit搭建数据交互式app","slug":"用streamlit搭建数据交互式app","date":"2024-09-20T01:50:21.000Z","updated":"2024-10-18T05:33:19.179Z","comments":true,"path":"streamlit-app/","permalink":"https://xiaohanys.top/streamlit-app/","excerpt":"streamlit的有趣特点 所有的程序，只要是前端交互页面发生变动或者说交互，代码就会从头到尾执行一遍 提供了非常多数据交互的组件，每个组件都可以返回数值，用来和别的组件交流 有特殊的缓存系统，防止长时间运行的程序成为瓶颈 因为程序从头至尾的顺序执行，异步的支持较差","text":"streamlit的有趣特点 所有的程序，只要是前端交互页面发生变动或者说交互，代码就会从头到尾执行一遍 提供了非常多数据交互的组件，每个组件都可以返回数值，用来和别的组件交流 有特殊的缓存系统，防止长时间运行的程序成为瓶颈 因为程序从头至尾的顺序执行，异步的支持较差 搭建一个小型demo主页1 主页2 主要功能下机数据的文件名字往往都是通过barcode区分的，而我们分析的时候需要按照样本为其命名，这样更方便，也不容易搞错。一个方便的方法便是用列表中的样本自动生成新的文件名，并自动提供数据拷贝指令。 这里有一个坑：如果在一个lane上测序量不足的情况下，会出现加测的情况，也就意味着，一个样本名会对应多个文件。换句话说，样本名列有冗余，需要我们注意。 所以搭建这个app的目的是：通过upload一个包含测序下机路径数据的excel表格，设置需要获取的信息所在的列索引，本app自动把提取后的数据展示在网页表格中。然后，在第二个tab中自动把数据所需要的拷贝指令生成出来。 这里对待冗余样本用了很简单的逻辑，既自动生成_1,_2…后缀。 直接上代码### streamlit_app/main.pyimport streamlit as stimport pandas as pdimport os st.set_page_config( page_title=&quot;Excel toolkits&quot;, layout=&quot;wide&quot;)@st.cache_datadef read_data(upload_file): # Read the uploaded file into a dataframe df = pd.read_excel(upload_file) # Rename the columns df.columns = [&quot;n&quot;+str(i+1) for i in range(df.shape[1])] # Fill missing values with empty string df = df.fillna(&#x27;&#x27;) return dfdef read_data2(upload_file): # Read the file into a Pandas dataframe duiying = pd.read_excel(upload_file) # Rename the columns duiying.columns = [&quot;sampleName&quot;,&quot;trueName&quot;] return duiying# This code makes sure that all of the names in the dataframe are unique by adding a number at the end of each name that is not unique.# The code also adds a number at the end of each name that is unique, but not the first one.def makenames(df,col): s=&#x27;_&#x27;+df.groupby(col).cumcount().add(1).astype(str) df.loc[:,col]+=s.mask(s==&quot;_1&quot;,&quot;&quot;) return df## Add a titlest.title(&quot;Excel tools for Glims&quot;)## Split the page into 2 columnscol1, col2 = st.columns([0.35,0.65],gap=&quot;medium&quot;)## 这个是左边的栏目with col1: ###组件数据交互 upload_data = st.file_uploader(label=&quot;Excel file from Glims...&quot;, type=[&#x27;xlsx&#x27;,&#x27;xls&#x27;]) SM_index = st.number_input(label=&quot;Index for sampleName:&quot;, min_value=1, max_value=62, value=4) BC_index = st.number_input(label=&quot;Index for barcode:&quot;, min_value=1, max_value=62, value=23) CP_index = st.number_input(label=&quot;Index for chip:&quot;, min_value=1, max_value=62, value=27) LN_index = st.number_input(label=&quot;Index for lane:&quot;, min_value=1, max_value=62, value=28) Path_index = st.number_input(label=&quot;Index for DataPath:&quot;, min_value=1, max_value=62, value=48) Remote_path = st.text_input(label=&quot;Remote path:&quot;, value=&quot;10.2.100.1:/pakpox/pob8d1/&quot;, max_chars=1000) isSubmit = st.button(&quot;Submit!&quot;) st.divider() placeholder = st.empty() upload_data2 = st.file_uploader(label=&quot;Input Barcode with sampleName:&quot;, type=[&#x27;xlsx&#x27;,&#x27;xls&#x27;])### 这个是右边的栏目with col2: ### 分两个tab tab1, tab2 = st.tabs([&quot;ViewData&quot;, &quot;Scipts&quot;]) tab21, tab22 = tab2.tabs([&quot;Script1&quot;, &quot;Script2&quot;]) ### container是容器占位符，作用是可以把后面运行的命令得到的结果放到一个容器里面渲染在前面去 upcontent = tab1.container() upcontent.subheader(&quot;View of the data:&quot;) if upload_data is not None: ###这个判断很重要，不然会报错 ###这是一个缓存函数 dataframes = read_data(upload_data) df = dataframes.iloc[:,[SM_index-1,BC_index-1,CP_index-1,LN_index-1,Path_index-1]] df.columns = [&quot;sampleName&quot;, &quot;barcode&quot;, &quot;chip&quot;, &quot;lane&quot;, &quot;dataPath&quot;] ## if sampleName is duplicated, fix the name if df.sampleName.duplicated().any(): tab1.warning(&quot;Duplicated sampleName found, fixing...&quot;) df = makenames(df,&quot;sampleName&quot;) ### upload2的作用是，有的excel文件并没有样本名列，它提供了额外的样本名和barcode对应关系，需要我们提交后再次上传 if upload_data2 is not None: placeholder.info(&quot;After upload data, please click the button again!&quot;) df2 = read_data2(upload_data2) df = df.merge(df2,how=&quot;left&quot;,on=&quot;sampleName&quot;) df = df.drop(columns=[&quot;sampleName&quot;]) df = df.rename(columns=&#123;&quot;trueName&quot;:&quot;sampleName&quot;&#125;) df = df[~df[&quot;sampleName&quot;].isna()] ### 这个判断导致延迟执行，每次更新数据后，只有重新点击提交按钮才会重新执行 if isSubmit: upcontent.dataframe(df,hide_index=True,height=600) df = df.assign(filename1=df.apply(lambda row : &quot;_&quot;.join([row[&#x27;chip&#x27;],row[&#x27;lane&#x27;],str(row[&#x27;barcode&#x27;]),&quot;1.fq.gz&quot;]),axis=1)) df = df.assign(filename2=df.apply(lambda row : &quot;_&quot;.join([row[&#x27;chip&#x27;],row[&#x27;lane&#x27;],str(row[&#x27;barcode&#x27;]),&quot;2.fq.gz&quot;]),axis=1)) cmd1 = df.apply(lambda row: &quot;scp &quot; + os.path.join(row[&#x27;dataPath&#x27;],row[&#x27;filename1&#x27;]) + &quot; &quot; + os.path.join(Remote_path,row[&#x27;sampleName&#x27;] + &quot;_R1.fastq.gz&quot;),axis=1).tolist() cmd2 = df.apply(lambda row: &quot;scp &quot; + os.path.join(row[&#x27;dataPath&#x27;],row[&#x27;filename2&#x27;]) + &quot; &quot; + os.path.join(Remote_path,row[&#x27;sampleName&#x27;] + &quot;_R2.fastq.gz&quot;),axis=1).tolist() tab21.code(&quot;\\n&quot;.join(cmd1)) tab22.code(&quot;\\n&quot;.join(cmd2)) 总结这个小demo用到了不少知识点 函数前面加@st.cache_data可以起到缓存作用 因为streamlit是从头到尾的执行，所以if判断很重要，例如upload2其实从逻辑上是最后执行的，但是因为这个特性加在了代码前面 通常我们加载数据后，都会有信息的提示，例如“数据加载成功”，“检测到冗余！”等。然后streamlit难以实现异步，导致要么该提示一闪而过，要么就永久留在页面上，不然会对后面运行的程序带来影响。所以container可以当作占位符，把想要渲染的组件先于逻辑放在前端 下一篇预告streamlit是一个快速搭建网页小程序的工具，但是我们看到，目前一个很简单的需求，实现起来都有重重限制。而且其异步的难以实现更是让用户体验非常糟糕。 下一篇，我们将使用基于next.js和Chakra UI的reflex来再次搭建该网页小程序。看看性能会有提升么？","categories":[{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/categories/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"},{"name":"前端","slug":"前端","permalink":"https://xiaohanys.top/tags/%E5%89%8D%E7%AB%AF/"}],"author":"Xiaoguang Pan"},{"title":"猪的stop数据库芯片构建","slug":"猪的stop数据库芯片构建","date":"2020-05-13T06:25:13.000Z","updated":"2024-10-18T05:33:29.700Z","comments":true,"path":"pig-stop-chip/","permalink":"https://xiaohanys.top/pig-stop-chip/","excerpt":"项目目的 利用CBE的碱基编辑能将正常氨基酸密码子转换成终止密码子的性能，设计出针对人类、猪、小鼠的全部基因的CBE-STOP芯片。通过TRAP系统的细胞内测试，检测分析所有gRNA介导的STOP效率，最终建立人类、猪、小鼠的CBE-STOP的gRNA效率数据库，供做base-editing相关研究的科研人员使用。","text":"项目目的 利用CBE的碱基编辑能将正常氨基酸密码子转换成终止密码子的性能，设计出针对人类、猪、小鼠的全部基因的CBE-STOP芯片。通过TRAP系统的细胞内测试，检测分析所有gRNA介导的STOP效率，最终建立人类、猪、小鼠的CBE-STOP的gRNA效率数据库，供做base-editing相关研究的科研人员使用。 算法 找出human、pig、mouse的所有已知基因及其全长序列，建立3个基因数据集（所有protein coding gene，不包含非编码RNA基因）,这里直接使用物种的exon序列即可； 提取所有基因的前3个exon，如果只有1-3个exon的，保留所有exon(需要过滤掉在UTR上的外显子，还需要过滤掉外显子长度小于23bp的)； 找出前3个外显子中所有正义和反义链上的gRNA（20 bp序列). 做一定程度的过滤： 去除23bp中有BsmBI（5‘-CGTCTC-3‘，5‘-GAGACG-3‘）酶切位点的gRNA 去除23bp中有4个及以上连续T的gRNA GC content(20bp)在40%~90%之间 记录脱靶数目 在每条剩余的gRNA（4-8）bp序列上寻找CGA，CAG，CAA，TGG（反向互补），并判断其是否为潜在的综止密码子 组装 具体实现步骤step_1过滤外显子,从Ensembl上获取所有编码基因的所有外显子序eg:AllExonSeq.fa,外显子信息eg:AllExonInfo.txt首先从信息文件入手，每个外显子去除UTR部分同时要保证序列长度是大于23bp的并且只保留前3个外显子并和序列merge在一起 首先，用R简单过滤信息表格较为方便 library(readr)library(dplyr)exon &lt;- read_delim( &quot;AllExonInfo.txt&quot;, delim = &quot;\\t&quot;, col_names = FALSE, col_types = cols(&quot;n&quot;, &quot;n&quot;, &quot;c&quot;, &quot;n&quot;, &quot;n&quot;, &quot;c&quot;, &quot;n&quot;, &quot;n&quot;, &quot;c&quot;) )names(exon) &lt;- c( &quot;Exon region start (bp)&quot;, &quot;Exon region end (bp)&quot;, &quot;Gene name&quot;, &quot;5&#x27; UTR start&quot;, &quot;5&#x27; UTR end&quot;, &quot;Strand&quot;, &quot;3&#x27; UTR start&quot;, &quot;3&#x27; UTR end&quot;, &quot;Chromosome&quot; )##先去掉UTR,在过滤掉长度小于32bp的，去重，按照每个基因保留前3个exonneed &lt;- exon %&gt;% filter(is.na(`5&#x27; UTR start`) | is.na(`3&#x27; UTR end`)) %&gt;% mutate( seqLength = `Exon region end (bp)` - `Exon region start (bp)` + 1, seqName = paste0( &quot;&gt;&quot;, `Gene name`, &quot;|&quot;, `Exon region start (bp)`, &quot;|&quot;, `Exon region end (bp)`, &quot;|&quot;, Strand ) ) %&gt;% filter(seqLength &gt; 32) %&gt;% distinct(seqName, .keep_all = TRUE) %&gt;% group_by(`Gene name`) %&gt;% slice(1:3) %&gt;% select(3, 9, 1, 2, 6, 11) %&gt;% arrange(Chromosome) %&gt;% ungroup()write.table( need, &quot;AllExonInfoTop3.txt&quot;, sep = &quot;\\t&quot;, row.names = FALSE, quote = FALSE) 然后，用python脚本读取fasta序列，并利用pandas库和信息表整合 import pandas as pdname_lst = []allseq_lst = []seq_lst = [&quot;test&quot;]with open(&quot;AllExonSeq.fa&quot;, &quot;r&quot;) as f: for line in f: if line.startswith(&quot;&gt;&quot;): allseq_lst.append(&quot;&quot;.join(seq_lst)) seq_lst = [] name = line.strip(&quot;\\n&quot;) name_lst.append(name) else: seq_lst.append(line.strip(&quot;\\n&quot;))allseq_lst.remove(&quot;test&quot;)allseq_lst.append(seq_lst)df = pd.DataFrame(&#123;&quot;seqName&quot;: name_lst, &quot;sequence&quot;: allseq_lst&#125;)df2 = pd.read_csv(&quot;AllExonInfoTop3.txt&quot;, sep=&quot;\\t&quot;, header=0)df = df.set_index(&quot;seqName&quot;)df2 = df2.set_index(&quot;seqName&quot;)df3 = df2.join(df, how=&quot;inner&quot;)df3 = df3.reset_index()df3.drop_duplicates(&quot;seqName&quot;, keep=&quot;first&quot;, inplace=True)df3.to_csv(&quot;exonWithAnno.txt&quot;, sep=&quot;\\t&quot;, index=None)ss = df3[&quot;seqName&quot;].tolist()sn = df3[&quot;sequence&quot;].tolist()with open(&quot;AllGeneTop3Exon.fa&quot;, &quot;a&quot;) as l: for name, seq in zip(ss, sn): l.write(&quot;&#123;&#125;\\n&#123;&#125;\\n&quot;.format(name, seq)) step_2exon序列小调这里获取的序列如果是反向序列的话，就会造成后期寻找gRNA的不便，为此，我们先把所有的exon序列转换为统一的方向 import pandas as pddef complement(seq): return seq.translate(str.maketrans(&quot;ACGT&quot;, &quot;TGCA&quot;))def revcomp(seq): return complement(seq)[::-1]df = pd.read_csv(&quot;exonWithAnno.txt&quot;, sep=&quot;\\t&quot;, header=0)df2 = df[df[&quot;Strand&quot;] &lt; 0]df1 = df[df[&quot;Strand&quot;] &gt; 0]df2[&quot;sequence&quot;] = df2[&quot;sequence&quot;].apply(revcomp)fin = pd.concat([df1, df2], axis=0, ignore_index=True)ss = fin[&quot;seqName&quot;].tolist()nn = fin[&quot;sequence&quot;].tolist()with open(&quot;allexoninzhengliantop3.fa&quot;, &quot;a&quot;) as f: for name, seq in zip(ss, nn): f.write(f&quot;&#123;name&#125;\\n&#123;seq&#125;\\n&quot;) 接着，外显子序列数目巨大，很难放在一起寻找gRNA，因此，我们先将该序列拆分，每分留6000条序列 seqkit split2 allexoninzhengliantop3.fa -s 6000 -f 事实证明，做这一步反而让后面的分析变得麻烦了 step_3运行FlashFry软件发现所有的gRNA并打分，首先，要在ENSEMBL上下载完整的HG38基因组并构建索引 axel -n 50 ftp://ftp.ensembl.org/pub/release-100/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gzmv Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz hg38.fajava -Xmx4g -jar /dellfsqd2/ST_LBI/USER/panxiaoguang/app/FlashFry/bin/flashfry \\ index \\ --tmpLocation ./GranTmp \\ --database hg38 \\ --reference ./hg38.fa \\ --enzyme spcas9ngg 然后，分别对10份序列设计gRNA并打分 java -Xmx10g -jar /dellfsqd2/ST_LBI/USER/panxiaoguang/app/FlashFry/bin/flashfry \\ discover \\ --database hg38 \\ --fasta ./test/allexoninzhengliantop3.fa.split/allexoninzhengliantop3.part_001.fa \\ --output hg38_exon.part_001.outjava -Xmx10g -jar /dellfsqd2/ST_LBI/USER/panxiaoguang/app/FlashFry/bin/flashfry \\ score \\ --input hg38_exon.part_001.out \\ --output hg38_exon.part_001.scored \\ --scoringMetrics doench2016cfd,moreno2015,rank,minot \\ --database hg38 step_4对打分结果做初步过滤，主要包括GC含量，BSMB1位点，4个以上连续T 用R语言内置函数对数据表过滤很方便,这里额外获取了gRNA的基因，正负链以及在基因组上的位置信息 library(readr)library(dplyr)library(pystr)args = commandArgs(T)name_header&lt;-args[1]df &lt;- read_delim( pystr_format(&quot;&#123;1&#125;.scored&quot;,name_header), delim = &quot;\\t&quot;, col_types = cols( &quot;c&quot;, &quot;d&quot;, &quot;d&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot; ) )df2 &lt;- read_delim(&quot;./test/exonWithAnno.txt&quot;, delim = &quot;\\t&quot;)df2 &lt;- df2 %&gt;% select(`Gene name`, Chromosome) %&gt;% distinct(`Gene name`, .keep_all = TRUE)%&gt;%setNames(c(&quot;gene&quot;,&quot;chromosome&quot;))calGC &lt;- function(x) &#123; x &lt;- stringr::str_sub(x, 1, 20) stringr::str_count(x, &quot;[GC]&quot;) / 20&#125;df &lt;- df %&gt;% select(contig, start, stop, target, orientation, `0-1-2-3-4_mismatch`) %&gt;%##记录0-4bp mismatchtidyr::separate( `0-1-2-3-4_mismatch`, into = c(&quot;mis0&quot;, &quot;mis1&quot;, &quot;mis2&quot;, &quot;mis3&quot;, &quot;mis4&quot;), sep = &quot;,&quot; ) %&gt;%##计算gRNA在基因组的起始和结束位点mutate( genomestart = purrr::map_dbl(contig, function(x) as.numeric(unlist(strsplit( x, &quot;\\\\|&quot; ))[2])), genomestart = genomestart + start, genomeend = genomestart + 22 ) %&gt;%##标记正负链mutate( strand = purrr::map_dbl(contig, function(x) as.numeric(unlist(strsplit( x, &quot;\\\\|&quot; ))[4])), strand = if_else(strand &gt; 0, &quot;+&quot;, &quot;-&quot;) ) %&gt;% mutate(gene = purrr::map_chr(contig, function(x) unlist(strsplit(x, &quot;\\\\|&quot;))[1])) %&gt;%##记录是否具有BSMB1位点mutate(bsmb1 = case_when( stringr::str_detect(target, &quot;CGTCTC&quot;) ~ TRUE, stringr::str_detect(target, &quot;GAGACG&quot;) ~ TRUE, TRUE ~ FALSE )) %&gt;%##记录是否具有4个以上的连续Tmutate(nt = if_else(stringr::str_detect(target, &quot;TTTT+&quot;), TRUE, FALSE)) %&gt;%##记录GC含量mutate(gcContent = purrr::map_dbl(target, function(x) calGC(x))) %&gt;% filter(bsmb1 == FALSE &amp; nt == FALSE &amp; gcContent &gt;= 0.4 &amp; gcContent &lt;= 0.9) %&gt;% select(-2, -3)df &lt;- df %&gt;% left_join(df2, by = &quot;gene&quot;)write.table( df, pystr_format(&quot;./tmp/&#123;1&#125;.first_filter.txt&quot;,name_header), sep = &quot;\\t&quot;, row.names = FALSE, quote = FALSE) step_5最后一步过滤，要求在gRNA4-8bp空间内具有stop位点，这里采用python脚本去下载好的ATG开头的CDS基因集中反向MAP，正反向 ** (这里的正反向不是基因的正反向，而是gRNA和cds方向的一致性，如果基因在正链上，那么gRNA正向设计的为顺，反向设计为逆；如果基因在负链上，那么gRNA正向设计的为逆，反向设计为顺) ** gRNA要分别运算，最后获取可用的gRNA和其对应的stop 位点。为了加速最后一步过滤，分成三步完成，最核心的一步采用pypy step_1.py import pandas as pdimport sysname_header = sys.argv[1]gRNA_filer = pd.read_csv(&quot;./tmp/&#123;&#125;.first_filter.txt&quot;.format(name_header), sep=&quot;\\t&quot;, header=0)gRNA_Z = gRNA_filer[((gRNA_filer[&quot;orientation&quot;] == &quot;FWD&quot;) &amp; (gRNA_filer[&quot;strand&quot;] == &quot;+&quot;)) | ((gRNA_filer[&quot;orientation&quot;] == &quot;RVS&quot;) &amp; (gRNA_filer[&quot;strand&quot;] == &quot;-&quot;))].reset_index()[[&#x27;gene&#x27;,&#x27;target&#x27;]]gRNA_F = gRNA_filer[((gRNA_filer[&quot;orientation&quot;] == &quot;RVS&quot;)&amp;(gRNA_filer[&quot;strand&quot;] == &quot;+&quot;)) | ((gRNA_filer[&quot;orientation&quot;] == &quot;FWD&quot;)&amp;(gRNA_filer[&quot;strand&quot;] == &quot;-&quot;))].reset_index()[[&#x27;gene&#x27;,&#x27;target&#x27;]]gRNA_Z.to_csv(&quot;./tmp/&#123;&#125;.part_1.txt&quot;.format(name_header), sep=&quot;\\t&quot;, index=None, header=None)gRNA_F.to_csv(&quot;./tmp/&#123;&#125;.part_2.txt&quot;.format(name_header), sep=&quot;\\t&quot;, index=None, header=None) step_2.py import reimport sysname_header = sys.argv[1]def read_cds(fs): name_lst = [] seq_lst=[] with open(fs, &#x27;r&#x27;) as f: for line in f: if line.startswith(&quot;&gt;&quot;): name = line.strip(&quot;\\n&quot;).replace(&quot;&gt;&quot;, &quot;&quot;) name_lst.append(name) else: seq_lst.append(line.strip(&quot;\\n&quot;)) fin=[(key,value) for key,value in zip(name_lst,seq_lst) if &#x27;unavailable&#x27; not in value] return findef complement(seq): return seq.translate(str.maketrans(&#x27;ACGT&#x27;, &#x27;TGCA&#x27;))def revcomp(seq): return complement(seq)[::-1]def motif_Z(gRNA, site): if gRNA[:20][site: site + 3] == &quot;CAG&quot;: return &quot;CAG&quot; elif gRNA[:20][site: site + 3] == &quot;CGA&quot;: return &quot;CGA&quot; elif gRNA[:20][site: site + 3] == &quot;CAA&quot;: return &quot;CAA&quot; else: return 0def stop_test_for_Z(n, cds_lst, gRNA): new_cds_lst = [cds for cds in cds_lst if gRNA in cds] score_lst = [] if len(new_cds_lst) &gt; 0: for haha in new_cds_lst: m = haha.find(gRNA) + 1 score = (m - 1 + n - 1) score_lst.append(score) if len([new_score for new_score in score_lst if score % 3 == 0]) &gt; 0: return 1 else: return 0 else: return 0def motif_F(gRNA, site): if gRNA[:20][site: site + 3] == &quot;CCA&quot;: return 1 elif gRNA[:20][site-1: site + 2] == &quot;CCA&quot;: return 2 else: return 0def stop_test_for_F(n, cds_lst, gRNA): gRNA = revcomp(gRNA) new_cds_lst = [cds for cds in cds_lst if gRNA in cds] score_lst = [] if len(new_cds_lst) &gt; 0: for haha in new_cds_lst: m = haha.find(gRNA) + 1 - 1 + 23 score = (m - n) score_lst.append(score) if len([new_score for new_score in score_lst if score % 3 == 0]) &gt; 0: return 1 else: return 0 else: return 0def detective_Z(gRNA, Gene): #cds_lst=all_cds[all_cds[&#x27;Gene&#x27;]==Gene][&#x27;seq&#x27;].tolist() cds_lst=[value for (key,value) in all_cds if key==Gene] stopSitelst=[] for m in re.finditer(&quot;C&quot;, gRNA[:20]): if m.start() + 1 &gt;= 4 and m.start() + 1 &lt;= 8: if motif_Z(gRNA, m.start()): if stop_test_for_Z((m.start() + 1), cds_lst, gRNA): stopSitelst.append(str(m.start() + 1)) if len(stopSitelst) &gt; 0: fin=&quot;/&quot;.join(stopSitelst) else: fin=&quot;None&quot; return findef detective_F(gRNA, Gene): #cds_lst=all_cds[all_cds[&#x27;Gene&#x27;]==Gene][&#x27;seq&#x27;].tolist() cds_lst=[value for (key,value) in all_cds if key==Gene] stopSitelst=[] for m in re.finditer(&quot;C&quot;, gRNA[:20]): if m.start() + 1 &gt;= 4 and m.start() + 1 &lt;= 8: if motif_F(gRNA, m.start()): if stop_test_for_F(((gRNA[:20][m.start() - 1 : m.start() + 3]).find(&quot;CCA&quot;) + 1 + 2), cds_lst, gRNA): stopSitelst.append(str(m.start() + 1)) if len(stopSitelst) &gt; 0: fin=&quot;/&quot;.join(stopSitelst) else: fin=&quot;None&quot; return finall_cds = read_cds(&quot;AllGeneCDS2.fa&quot;)shunshi=[]with open(&quot;./tmp/&#123;&#125;.part_1.txt&quot;.format(name_header),&#x27;r&#x27;) as f: for line in f: gene,target=line.split(&quot;\\t&quot;) target=target.strip(&quot;\\n&quot;) shunshi.append((gene,target))new_lst=[(key,value,detective_Z(value,key)) for (key,value) in shunshi]with open(&quot;./tmp/&#123;&#125;.shunshi.out&quot;.format(name_header),&quot;a&quot;) as f: for (gene,target,site) in new_lst: f.write(&quot;&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\n&quot;.format(gene,target,site))fanshi=[]with open(&quot;./tmp/&#123;&#125;.part_2.txt&quot;.format(name_header),&#x27;r&#x27;) as f: for line in f: gene,target=line.split(&quot;\\t&quot;) target=target.strip(&quot;\\n&quot;) fanshi.append((gene,target))new_lst2=[(key,value,detective_F(value,key)) for (key,value) in fanshi]with open(&quot;./tmp/&#123;&#125;.fanshi.out&quot;.format(name_header),&quot;a&quot;) as f: for (gene,target,site) in new_lst2: f.write(&quot;&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\n&quot;.format(gene,target,site)) step_3.py import pandas as pdimport sysname_header = sys.argv[1]df1 = pd.read_csv(&quot;./tmp/&#123;&#125;.shunshi.out&quot;.format(name_header), sep=&quot;\\t&quot;, header=None)df2 = pd.read_csv(&quot;./tmp/&#123;&#125;.fanshi.out&quot;.format(name_header), sep=&quot;\\t&quot;, header=None)fin = pd.concat([df2, df1], ignore_index=True)fin.columns = [&#x27;gene&#x27;, &#x27;target&#x27;, &#x27;site&#x27;]fin2 = fin[fin[&#x27;site&#x27;] != &quot;None&quot;]fin2.to_csv(&quot;./tmp/&#123;&#125;.second_filter.txt&quot;.format(name_header),sep=&quot;\\t&quot;,index=None) 最后整合的shell为： #!/usr/bin/bash name_header=&quot;hg38_exon.part_001&quot;/dellfsqd2/ST_LBI/USER/panxiaoguang/app/miniconda3/envs/Rlibs2/bin/Rscript first_filter.R $name_headerecho &quot;first filter Done! $(date &quot;+%Y-%m-%d %H:%M:%S&quot;)&quot; echo &quot;first step: cut into two groups! $(date &quot;+%Y-%m-%d %H:%M:%S&quot;)&quot; /dellfsqd2/ST_LBI/USER/panxiaoguang/app/miniconda3/envs/pylibs/bin/python3 step_1.py $name_headerecho &quot;step1 done! $(date &quot;+%Y-%m-%d %H:%M:%S&quot;)&quot;echo &quot;second step: filter stop condon site! $(date &quot;+%Y-%m-%d %H:%M:%S&quot;)&quot; /dellfsqd2/ST_LBI/USER/panxiaoguang/app/miniconda3/envs/pypy/bin/pypy3 step_2.py $name_headerecho &quot;step2 done! $(date &quot;+%Y-%m-%d %H:%M:%S&quot;)&quot;echo &quot;second step: merge data! $(date &quot;+%Y-%m-%d %H:%M:%S&quot;)&quot; /dellfsqd2/ST_LBI/USER/panxiaoguang/app/miniconda3/envs/pylibs/bin/python3 step_3.py $name_headerecho &quot;ALL done! $(date &quot;+%Y-%m-%d %H:%M:%S&quot;)&quot;csvtk -t join -f &quot;gene,target&quot; ./tmp/$&#123;name_header&#125;.second_filter.txt ./tmp/$&#123;name_header&#125;.first_filter.txt --left-join \\ |csvtk -t cut -f gene,target,site,strand,orientation,chromosome,genomestart,genomeend,mis0,mis1,mis2,mis3,mis4,gcContent \\ |csvtk pretty &gt; ./tmp/$&#123;name_header&#125;.finally.txt /dellfsqd2/ST_LBI/USER/panxiaoguang/app/miniconda3/envs/Rlibs2/bin/Rscript getbed.R $name_headerbedtools getfasta -fi hg38.fa -bed ./tmp/$&#123;name_header&#125;.bed -name -tab -s -fo ./tmp/$&#123;name_header&#125;.trap.fa 组装library(readr)library(dplyr)library(pystr)library(purrr)library(openxlsx)merge_data&lt;-function(x)&#123; name_header&lt;-pystr_format(&quot;hg38_exon.part_0&#123;1&#125;&quot;,x) df1&lt;-read_delim(pystr_format(&quot;&#123;1&#125;.finally.txt&quot;,name_header),delim=&quot;\\t&quot;) df2&lt;-read_delim(pystr_format(&quot;&#123;1&#125;.trap.fa&quot;,name_header),delim=&quot;\\t&quot;,col_names=FALSE) names(df2)&lt;-c(&quot;gene&quot;,&quot;trapSeq&quot;) fin&lt;-bind_cols(df1,df2[&#x27;trapSeq&#x27;])&#125;fs_lst&lt;-c(&quot;01&quot;,&quot;02&quot;,&quot;03&quot;,&quot;04&quot;,&quot;05&quot;,&quot;06&quot;,&quot;07&quot;,&quot;08&quot;,&quot;09&quot;,&quot;10&quot;)test&lt;-map_dfr(fs_lst,merge_data)wocao&lt;-test%&gt;% transmute(Gene=gene, Target=target, TrapSeq=trapSeq, GeneStrand=strand, gRNAstrand=orientation, Loc=paste0(&quot;chr&quot;,chromosome,&quot;:&quot;,genomestart,&quot;-&quot;,genomeend), `Mismatch(0-1-2-3-4)`=paste0(mis0,&quot;-&quot;,mis1,&quot;-&quot;,mis2,&quot;-&quot;,mis3,&quot;-&quot;,mis4), GCcontent=gcContent, StopSite=site)%&gt;% arrange(Gene)wb &lt;- createWorkbook(&quot;test&quot;)addWorksheet(wb, &quot;humanstop&quot;)writeData(wb, sheet = &quot;humanstop&quot;, wocao, rowNames = FALSE)saveWorkbook(wb, &quot;HumanStopCBE.xlsx&quot;, overwrite = TRUE)","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"},{"name":"CRISPR","slug":"CRISPR","permalink":"https://xiaohanys.top/tags/CRISPR/"}],"author":"Xiaoguang Pan"},{"title":"基因沉默效率计算方法","slug":"基因沉默效率计算方法","date":"2020-02-28T08:20:39.000Z","updated":"2024-10-18T05:32:14.274Z","comments":true,"path":"gene-stop-eff/","permalink":"https://xiaohanys.top/gene-stop-eff/","excerpt":"前言Crispr基因编辑正越来越广泛的应用在各个方面，包括科研，医疗等等，针对经过筛选的药物靶向基因设计gRNA，使其由原始的基因序列突变为终止密码子，从而无法表达蛋白，进而治疗疾病或者抵抗药物","text":"前言Crispr基因编辑正越来越广泛的应用在各个方面，包括科研，医疗等等，针对经过筛选的药物靶向基因设计gRNA，使其由原始的基因序列突变为终止密码子，从而无法表达蛋白，进而治疗疾病或者抵抗药物 算法 初步过滤： 将12ktrap中，N1-N20内C&gt;T成功的trap提取出来(所有编辑后的reads，不做trap的去重)，定义为dataset#1 将dataset#1分成2部分，一部分是gRNA在基因的正义链（+）上，定义为dataset#1（+），另一部分在反义链（-）上，定义为dataset#1（-）； 正义链的情况（CAG/CGA/CAA &gt; TAG/TGA/TAA）： 找出dataset#1（+）N1-N20中 C&gt;T成功的位置，并往后推2个碱基，判断其是否为CAG或CGA或CAA（编辑前） ，满足其一即抽提出来，完成后定义为dataset#2（+），同时标记出突变位点在gRNA中的位置，譬如N&#x3D;5, N&#x3D;6等； 将dataset#2（+）中的gRNA（不包含我们人工添加的第一个碱基G）mapping回对应基因的CDS中，数出从ATG（翻译起始位置）的A到gRNA第一个碱基的碱基数M，随后算出 (M-1+N-1)&#x2F;3, 判断结果是否为整数，如果是整数，判断该基因被stop成功，输出”TRAP ID + gRNA sequence + strand (+/-) + C&gt;T position + C&gt;T efficiency”; 举例说明4）中算法：如N&#x3D;6, M&#x3D;11, 则说明突变位点的C距离ATG位置是10+5&#x3D;15个碱基，即15&#x2F;3&#x3D;5个氨基酸，到突变位点（如CAG）恰好是3的整数倍，该基因被stop成功； 反义链的情况(CCA &gt; TCA (TGA) / CTA(TAG) / TTA (TAA))： 找出dataset#1（-）N1-N20中 C&gt;T成功的位置，并判断其是否在CCA这样的motif中（编辑前），如果在，输出trap放到dataset#2（-）中，并标记出CCA motif中A所在gRNA中的位置N，譬如N&#x3D;5, N&#x3D;6等； 将dataset#2（-）中的gRNA提取出来（此时gRNA的序列应该是和CDS正义链互补的），数出从ATG（翻译起始位置）的A到gRNA第一个碱基的碱基数M （注意此时gRNA为CCNN20，gRNA第一个碱基在最右端），随后算出(M-N)&#x2F;3是否为整数，如果是整数，判断该基因被stop成功，输出”TRAP ID + gRNA sequence + strand (+/-) + C&gt;T position + C&gt;T efficiency”; 需求是： 获取每条TRAP序列所对应的stop效率，效率的计算方法是针对一条TRAP来说，至少有一个位点能够stop成功，那么这条对应的read为成功一次，一次类推，既不能重复计数，也不能少计数 获取每个位点对应的stop效率，如上原理所述 获取基因的stop率：stop-gene/all-gene 我使用的代码代码一：此代码用于输出所有通过的stop reads并计算每条TRAP的stop效率 # -*- coding=utf-8 -*-import redef complement(seq): return seq.translate(str.maketrans(&#x27;ACGT&#x27;, &#x27;TGCA&#x27;))# Obtain reverse complementary sequencedef revcomp(seq): return complement(seq)[::-1]# Input n site, all CDs sequences and trap37bp sequences, and check whether the site can stop# 1 for success, 0 for failuredef stop_test_for_Z(n, cds_lst, trap): new_cds_lst = [cds for cds in cds_lst if trap in cds] score_lst = [] if len(new_cds_lst) &gt; 0: for haha in new_cds_lst: m = haha.find(trap) + 1 + 10 score = (m - 1 + n - 1) score_lst.append(score) if len([new_score for new_score in score_lst if score % 3 == 0]) &gt; 0: return 1 else: return 0 else: return 0def stop_test_for_F(n, cds_lst, trap): trap = revcomp(trap) new_cds_lst = [cds for cds in cds_lst if trap in cds] score_lst = [] if len(new_cds_lst) &gt; 0: for haha in new_cds_lst: m = haha.find(trap) + 1 + 10 + 23 score = (m - n) score_lst.append(score) if len([new_score for new_score in score_lst if score % 3 == 0]) &gt; 0: return 1 else: return 0 else: return 0# Read all CDS files and return a dictionary. The key is gene and the value is CDS sequencedef read_cds(fs): fasta = &#123;&#125; with open(fs, &#x27;r&#x27;) as f: for line in f: if line.startswith(&quot;&gt;&quot;): seq_lst = [] name = line.strip(&quot;\\n&quot;).replace(&quot;&gt;&quot;, &quot;&quot;) else: seq_lst.append(line.strip(&quot;\\n&quot;)) fasta[name] = &quot;&quot;.join(seq_lst) new_fasta = &#123;key: value for key, value in fasta.items() if &quot;unavailable&quot; not in value&#125; return new_fasta# Read all the trap sequences and return a dictionary. The key is trap label and the value is 37bpdef read_seq(fs): fasta = &#123;&#125; with open(fs, &#x27;r&#x27;) as f: for line in f: if line.startswith(&quot;&gt;&quot;): seq_lst = [] name = line.strip(&quot;\\n&quot;).replace(&quot;&gt;&quot;, &quot;&quot;) else: seq_lst.append(line.strip(&quot;\\n&quot;)) fasta[name] = seq_lst return fasta# Read all reference sequences. The key is label and the value is 37bptrap sequencedef read_ref(fs): ref = &#123;&#125; with open(fs, &#x27;r&#x27;) as f: for line in f: if line.startswith(&quot;&gt;&quot;): name = line.strip(&quot;\\n&quot;).replace(&quot;&gt;&quot;, &quot;&quot;) else: ref[name] = line.strip(&quot;\\n&quot;)[118:155] return ref# Read the corresponding relationship between gene and label and return to the dictionarydef read_duiying(fs): duiying = &#123;&#125; with open(fs, &#x27;r&#x27;) as f: for line in f: lable, gene = line.split(&quot;\\t&quot;) duiying[lable.strip(&quot;\\n&quot;)] = gene.strip(&quot;\\n&quot;) return duiyingdef motif_Z(trap, site): if trap[10:30][site: site + 3] in [&quot;CAG&quot;, &quot;CGA&quot;, &quot;CAA&quot;]: return 1 else: return 0def motif_F(trap, site): if &quot;CCA&quot; in trap[10:30][site-2: site + 3]: return 1 else: return 0lable_map_gene = read_duiying(&quot;../duiying.txt&quot;)all_cds = read_cds(&quot;../mart_export.txt&quot;)all_ref = read_ref(&quot;ref.fa&quot;)all_trapseq = read_seq(&quot;new508.filter.fa&quot;)eff_lst = []for name, traps in all_ref.items(): gene = lable_map_gene[name] cds_lst = [vals for keys, vals in all_cds.items() if gene in keys] pass_lst = [] if name in list(all_trapseq.keys()): alltrap = len(all_trapseq[name]) seq_lst = all_trapseq[name] for m in re.finditer(&quot;C&quot;, traps[10:30]): if motif_Z(traps, m.start()): if stop_test_for_Z((m.start() + 1), cds_lst, traps): while seq_lst: gg=seq_lst.pop(0) if gg[10:30][m.start()] == &quot;T&quot;: pass_lst.append(gg) if len(pass_lst) &gt; 0: with open(&quot;pass_lst_z.fa&quot;, &#x27;a&#x27;) as wocao: wocao.write(&quot;&gt;&#123;&#125;\\n&quot;.format(name)) for nima in pass_lst: wocao.write(&quot;&#123;&#125;\\n&quot;.format(nima)) if alltrap &gt; 0: eff_lst.append([name, len(pass_lst) / alltrap]) else: print(name)for name, traps in all_ref.items(): gene = lable_map_gene[name] cds_lst = [vals for keys, vals in all_cds.items() if gene in keys] pass_lst = [] if name in list(all_trapseq.keys()): alltrap = len(all_trapseq[name]) seq_lst2 = all_trapseq[name] for m in re.finditer(&quot;C&quot;, traps[10:30]): if motif_F(traps, m.start()): if stop_test_for_F(((traps[10:30][m.start()-2: m.start() + 3]).find(&quot;CCA&quot;)+1+2), cds_lst, traps): while seq_lst2: gg=seq_lst2.pop(0) if gg[10:30][m.start()] == &quot;T&quot;: pass_lst.append(gg) if len(pass_lst) &gt; 0: with open(&quot;pass_lst_f.fa&quot;, &#x27;a&#x27;) as wocao: wocao.write(&quot;&gt;&#123;&#125;\\n&quot;.format(name)) for nima in pass_lst: wocao.write(&quot;&#123;&#125;\\n&quot;.format(nima)) if alltrap &gt; 0: eff_lst.append([name, len(pass_lst) / alltrap]) else: print(name)with open(&quot;trap_stop_eff.txt&quot;, &#x27;a&#x27;) as l: l.write(&quot;trap-lable\\ttrap-effective\\n&quot;) for lable, effctive in eff_lst: l.write(&quot;&#123;&#125;\\t&#123;&#125;\\n&quot;.format(lable, effctive)) 代码二：该代码用于计算需求2 # -*- coding=utf-8 -*-import redef complement(seq): return seq.translate(str.maketrans(&#x27;ACGT&#x27;, &#x27;TGCA&#x27;))# 获取反向互补序列def revcomp(seq): return complement(seq)[::-1]# 输入n位点，对应的所有CDS序列以及trap37bp序列，检查该位点能否stop# 成功返回1，失败返回0def stop_test_for_Z(n, cds_lst, trap): new_cds_lst = [cds for cds in cds_lst if trap in cds] score_lst = [] if len(new_cds_lst) &gt; 0: for haha in new_cds_lst: m = haha.find(trap) + 1 + 10 score = (m - 1 + n - 1) score_lst.append(score) if len([new_score for new_score in score_lst if score % 3 == 0]) &gt; 0: return 1 else: return 0 else: return 0def stop_test_for_F(n, cds_lst, trap): trap = revcomp(trap) new_cds_lst = [cds for cds in cds_lst if trap in cds] score_lst = [] if len(new_cds_lst) &gt; 0: for haha in new_cds_lst: m = haha.find(trap) + 1 + 10 + 23 score = (m - n) score_lst.append(score) if len([new_score for new_score in score_lst if score % 3 == 0]) &gt; 0: return 1 else: return 0 else: return 0# 读取所有的cds文件返回一个字典，键为gene，值为cds序列def read_cds(fs): fasta = &#123;&#125; with open(fs, &#x27;r&#x27;) as f: for line in f: if line.startswith(&quot;&gt;&quot;): seq_lst = [] name = line.strip(&quot;\\n&quot;).replace(&quot;&gt;&quot;, &quot;&quot;) else: seq_lst.append(line.strip(&quot;\\n&quot;)) fasta[name] = &quot;&quot;.join(seq_lst) new_fasta = &#123;key: value for key, value in fasta.items() if &quot;unavailable&quot; not in value&#125; return new_fasta# 读取所有的trap序列返回一个字典，键为trap-lable，值为37bp列表def read_seq(fs): fasta = &#123;&#125; with open(fs, &#x27;r&#x27;) as f: for line in f: if line.startswith(&quot;&gt;&quot;): seq_lst = [] name = line.strip(&quot;\\n&quot;).replace(&quot;&gt;&quot;, &quot;&quot;) else: seq_lst.append(line.strip(&quot;\\n&quot;)) fasta[name] = seq_lst return fasta# 读取所有的参考序列，键为lable,值为37bptrap序列def read_ref(fs): ref = &#123;&#125; with open(fs, &#x27;r&#x27;) as f: for line in f: if line.startswith(&quot;&gt;&quot;): name = line.strip(&quot;\\n&quot;).replace(&quot;&gt;&quot;, &quot;&quot;) else: ref[name] = line.strip(&quot;\\n&quot;)[118:155] return ref# 读取基因和lable的对应关系，返回字典def read_duiying(fs): duiying = &#123;&#125; with open(fs, &#x27;r&#x27;) as f: for line in f: lable, gene = line.split(&quot;\\t&quot;) duiying[lable.strip(&quot;\\n&quot;)] = gene.strip(&quot;\\n&quot;) return duiyingdef motif_Z(trap, site): if trap[10:30][site: site + 3] in [&quot;CAG&quot;, &quot;CGA&quot;, &quot;CAA&quot;]: return 1 else: return 0def motif_F(trap, site): if &quot;CCA&quot; in trap[10:30][site-2: site + 3]: return 1 else: return 0lable_map_gene = read_duiying(&quot;../duiying.txt&quot;)all_cds = read_cds(&quot;../mart_export.txt&quot;)all_ref = read_ref(&quot;ref.fa&quot;)all_trapseq = read_seq(&quot;new508.filter.fa&quot;)with open(&quot;pass_lst_f.txt&quot;, &#x27;a&#x27;) as wocao: wocao.write( &quot;TRAP ID\\tGene\\tgRNA sequence\\tstrand\\tC&gt;T position\\tC&gt;T efficiency\\n&quot;)with open(&quot;pass_lst_z.txt&quot;, &#x27;a&#x27;) as wocao: wocao.write( &quot;TRAP ID\\tGene\\tgRNA sequence\\tstrand\\tC&gt;T position\\tC&gt;T efficiency\\n&quot;)for name, traps in all_ref.items(): gene = lable_map_gene[name] cds_lst = [vals for keys, vals in all_cds.items() if gene in keys] pass_lst = [] for m in re.finditer(&quot;C&quot;, traps[10:30]): if motif_Z(traps, m.start()): if stop_test_for_Z((m.start() + 1), cds_lst, traps) and name in list(all_trapseq.keys()): i = 0 for gg in all_trapseq[name]: if gg[10:30][m.start()] == &quot;T&quot;: i = i + 1 eff = i/len(all_trapseq[name]) pass_lst.append( [name, traps[10:33], &quot;+&quot;, (m.start() + 1), eff]) if len(pass_lst) &gt; 0: with open(&quot;pass_lst_z.txt&quot;, &#x27;a&#x27;) as wocao: for nima in pass_lst: [trap_id, trap_seq, strand, ctposition, efftion] = nima wocao.write(&quot;&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\n&quot;.format( trap_id, gene, trap_seq, strand, ctposition, efftion))for name, traps in all_ref.items(): gene = lable_map_gene[name] cds_lst = [vals for keys, vals in all_cds.items() if gene in keys] pass_lst = [] for m in re.finditer(&quot;C&quot;, traps[10:30]): if motif_F(traps, m.start()): if stop_test_for_F(((traps[10:30][m.start()-2: m.start() + 3]).find(&quot;CCA&quot;)+1+2), cds_lst, traps) and name in list(all_trapseq.keys()): j = 0 for gg in all_trapseq[name]: if gg[10:30][m.start()] == &quot;T&quot;: j = j + 1 eff = j/len(all_trapseq[name]) pass_lst.append( [name, traps[10:33], &quot;-&quot;, (m.start() + 1), eff]) if len(pass_lst) &gt; 0: with open(&quot;pass_lst_f.txt&quot;, &#x27;a&#x27;) as wocao: for nima in pass_lst: [trap_id, trap_seq, strand, ctposition, efftion] = nima wocao.write(&quot;&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\t&#123;&#125;\\n&quot;.format( trap_id, gene, trap_seq, strand, ctposition, efftion)) 该博客仅记录工作流程，以作为备忘录，请勿过分借鉴，也不要转载，感谢！","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"},{"name":"CRISPR","slug":"CRISPR","permalink":"https://xiaohanys.top/tags/CRISPR/"},{"name":"gRNA","slug":"gRNA","permalink":"https://xiaohanys.top/tags/gRNA/"}],"author":"Xiaoguang Pan"},{"title":"如何批量计算相关性","slug":"如何批量计算相关性","date":"2020-02-17T07:13:03.000Z","updated":"2024-10-18T05:32:20.439Z","comments":true,"path":"batch-correlate/","permalink":"https://xiaohanys.top/batch-correlate/","excerpt":"1.只计算相关性，不考虑显著性检验：library(corrr)library(dplyr)data(mtcars)mtcars%&gt;%correlate()%&gt;%rearrange()%&gt;%stretch()","text":"1.只计算相关性，不考虑显著性检验：library(corrr)library(dplyr)data(mtcars)mtcars%&gt;%correlate()%&gt;%rearrange()%&gt;%stretch() 2.计算两种结果：library(dplyr)library(purrr)library(broom)library(tidyr)data(mtcars)pairs_names&lt;-t(combn(names(mtcars),2))%&gt;%as_tibble()%&gt;%setNames(c(&quot;x&quot;,&quot;y&quot;))pairs_names%&gt;%mutate(r.test=map2(x,y,~cor.test(mtcars[[.x]],mtcars[[.y]])),r.test=map(r.test,tidy))%&gt;%unnest(r.test) purrr 包查看前面的代码，看到了来自purrr包的map 和map2函数，那么，如何解释这两个函数的使用呢？ 顾名思义，map就和python中的一样，对一个可迭代的序列做向量化的操作，举个python的例子: a=[1,2,3,4,5]list(map(lambda x:x+1,a)) [2, 3, 4, 5, 6] 在R中，map函数做的是相同的操作，只不过它更牛，他可以接受一整个数据框或者tibble. library(purrr)data(&quot;mtcars&quot;)head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carbMazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 mtcars%&gt;%map(mean) $mpg[1] 20.09062$cyl[1] 6.1875$disp[1] 230.7219$hp[1] 146.6875$drat[1] 3.596563$wt[1] 3.21725$qsec[1] 17.84875$vs[1] 0.4375$am[1] 0.40625$gear[1] 3.6875$carb[1] 2.8125 map2函数同样的，因为是2，所以可以同时对两个参数同时迭代： n&lt;-list(4,5,6)m&lt;-list(1,2,3)map2(m,n,&#x27;+&#x27;) [[1]][1] 5[[2]][1] 7[[3]][1] 9 另外，map函数还具有其他的形式，例如 map:返回列表map_df:返回数据框map_dbl: 返回向量 最后，purrr包的函数形式例如~lm(x~y,data=.)，这种格式是为一种简写，实际上为，因此，.为传入的数据。 function(data) lm(data$x~data$y,data=data) 其余的包broom::tidy 这个函数用来处理数据模型，可以帮助我们将模型转化为一系列tidy的数据格式&#96;; tidyr::unnest 这个函数可以将数据框嵌套列表的形式展开便于观察 base::combn 来自基础包的排列组合函数，接受第一个参数为一个列表或者向量，第二个参数为一个数字，为从中取出的个数 %&gt;%管道符： 来自包dplyr包，用于方便数据的流程化处理","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"},{"title":"一文读懂lncRNA分析","slug":"一文读懂lncRNA分析","date":"2020-02-17T04:46:19.000Z","updated":"2024-10-18T05:33:11.723Z","comments":true,"path":"lncrna-analysis/","permalink":"https://xiaohanys.top/lncrna-analysis/","excerpt":"文章已经过时，请去官网查阅相关文档 1:对测序下机数据进行质量检测（QC）","text":"文章已经过时，请去官网查阅相关文档 1:对测序下机数据进行质量检测（QC） 使用软件：fastqc 命令行参数： -o --outdir:输出路径--extract：结果文件解压缩--noextract：结果文件压缩-f --format:输入文件格式.支持bam,sam,fastq文件格式-t --threads:线程数-c --contaminants：制定污染序列。文件格式 name[tab]sequence-a --adapters：指定接头序列。文件格式name[tab]sequence-k --kmers：指定kmers长度（2-10bp,默认7bp）-q --quiet： 安静模式 运行命令： fastqc -f fastq -t 8 -o ./QCreport/ fastq1 fastq2 注意：当样本数量很多时，我们可以采用 multiqc 软件将 fastqc 软件的结果进行合并 multiqc ./*.zip 2.对 qc 后的数据进行过滤使用软件：fastp 命令行参数： usage: fastp -i &lt;in1&gt; -o &lt;out1&gt; [-I &lt;in1&gt; -O &lt;out2&gt;] [options...]options: # I/O options 即输入输出文件设置 -i, --in1 read1 input file name (string) -o, --out1 read1 output file name (string [=]) -I, --in2 read2 input file name (string [=]) -O, --out2 read2 output file name (string [=]) -6, --phred64 indicates the input is using phred64 scoring (it&#x27;ll be converted to phred33, so the output will still be phred33) -z, --compression compression level for gzip output (1 ~ 9). 1 is fastest, 9 is smallest, default is 2. (int [=2]) --reads_to_process specify how many reads/pairs to be processed. Default 0 means process all reads. (int [=0]) # adapter trimming options 过滤序列接头参数设置 -A, --disable_adapter_trimming adapter trimming is enabled by default. If this option is specified, adapter trimming is disabled -a, --adapter_sequence the adapter for read1. For SE data, if not specified, the adapter will be auto-detected. For PE data, this is used if R1/R2 are found not overlapped. (string [=auto]) --adapter_sequence_r2 the adapter for read2 (PE data only). This is used if R1/R2 are found not overlapped. If not specified, it will be the same as &lt;adapter_sequence&gt; (string [=]) # global trimming options 剪除序列起始和末端的低质量碱基数量参数 -f, --trim_front1 trimming how many bases in front for read1, default is 0 (int [=0]) -t, --trim_tail1 trimming how many bases in tail for read1, default is 0 (int [=0]) -F, --trim_front2 trimming how many bases in front for read2. If it&#x27;s not specified, it will follow read1&#x27;s settings (int [=0]) -T, --trim_tail2 trimming how many bases in tail for read2. If it&#x27;s not specified, it will follow read1&#x27;s settings (int [=0]) # polyG tail trimming, useful for NextSeq/NovaSeq data polyG剪裁 -g, --trim_poly_g force polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data --poly_g_min_len the minimum length to detect polyG in the read tail. 10 by default. (int [=10]) -G, --disable_trim_poly_g disable polyG tail trimming, by default trimming is automatically enabled for Illumina NextSeq/NovaSeq data # polyX tail trimming -x, --trim_poly_x enable polyX trimming in 3&#x27; ends. --poly_x_min_len the minimum length to detect polyX in the read tail. 10 by default. (int [=10]) # per read cutting by quality options 划窗裁剪 -5, --cut_by_quality5 enable per read cutting by quality in front (5&#x27;), default is disabled (WARNING: this will interfere deduplication for both PE/SE data) -3, --cut_by_quality3 enable per read cutting by quality in tail (3&#x27;), default is disabled (WARNING: this will interfere deduplication for SE data) -W, --cut_window_size the size of the sliding window for sliding window trimming, default is 4 (int [=4]) -M, --cut_mean_quality the bases in the sliding window with mean quality below cutting_quality will be cut, default is Q20 (int [=20]) # quality filtering options 根据碱基质量来过滤序列 -Q, --disable_quality_filtering quality filtering is enabled by default. If this option is specified, quality filtering is disabled -q, --qualified_quality_phred the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified. (int [=15]) -u, --unqualified_percent_limit how many percents of bases are allowed to be unqualified (0~100). Default 40 means 40% (int [=40]) -n, --n_base_limit if one read&#x27;s number of N base is &gt;n_base_limit, then this read/pair is discarded. Default is 5 (int [=5]) # length filtering options 根据序列长度来过滤序列 -L, --disable_length_filtering length filtering is enabled by default. If this option is specified, length filtering is disabled -l, --length_required reads shorter than length_required will be discarded, default is 15. (int [=15]) # low complexity filtering -y, --low_complexity_filter enable low complexity filter. The complexity is defined as the percentage of base that is different from its next base (base[i] != base[i+1]). -Y, --complexity_threshold the threshold for low complexity filter (0~100). Default is 30, which means 30% complexity is required. (int [=30]) # filter reads with unwanted indexes (to remove possible contamination) --filter_by_index1 specify a file contains a list of barcodes of index1 to be filtered out, one barcode per line (string [=]) --filter_by_index2 specify a file contains a list of barcodes of index2 to be filtered out, one barcode per line (string [=]) --filter_by_index_threshold the allowed difference of index barcode for index filtering, default 0 means completely identical. (int [=0]) # base correction by overlap analysis options 通过overlap来校正碱基 -c, --correction enable base correction in overlapped regions (only for PE data), default is disabled # UMI processing -U, --umi enable unique molecular identifer (UMI) preprocessing --umi_loc specify the location of UMI, can be (index1/index2/read1/read2/per_index/per_read, default is none (string [=]) --umi_len if the UMI is in read1/read2, its length should be provided (int [=0]) --umi_prefix if specified, an underline will be used to connect prefix and UMI (i.e. prefix=UMI, UMI=AATTCG, final=UMI_AATTCG). No prefix by default (string [=]) --umi_skip if the UMI is in read1/read2, fastp can skip several bases following UMI, default is 0 (int [=0]) # overrepresented sequence analysis -p, --overrepresentation_analysis enable overrepresented sequence analysis. -P, --overrepresentation_sampling One in (--overrepresentation_sampling) reads will be computed for overrepresentation analysis (1~10000), smaller is slower, default is 20. (int [=20]) # reporting options -j, --json the json format report file name (string [=fastp.json]) -h, --html the html format report file name (string [=fastp.html]) -R, --report_title should be quoted with &#x27; or &quot;, default is &quot;fastp report&quot; (string [=fastp report]) # threading options 设置线程数 -w, --thread worker thread number, default is 3 (int [=3]) # output splitting options -s, --split split output by limiting total split file number with this option (2~999), a sequential number prefix will be added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (int [=0]) -S, --split_by_lines split output by limiting lines of each file with this option(&gt;=1000), a sequential number prefix will be added to output name ( 0001.out.fq, 0002.out.fq...), disabled by default (long [=0]) -d, --split_prefix_digits the digits for the sequential number padding (1~10), default is 4, so the filename will be padded as 0001.xxx, 0 to disable padding (int [=4]) # help -?, --help print this message 运行命令： fastp -c -i fastq1 -o ./cleandata/ -I fastq2 -O ./cleandata/ 3.对干净数据去除 rRNA 操作（存疑）在 ncbi 下载 rRNA 数据保存为 hg19_rRNA.fasta 使用 bwa 建立索引文件 bwa index hg19_rRNA.fasta 使用 bwa 将测序数据比对到 rRNA 数据库，去除比对上的数据，并恢复为 fastq 文件 bwa mem hg19_rRNA.fasta fastq1 fastq2 | samtools view -bf4 |samtools -c 9 -1 paired1.fq.gz -2 paired2.fq.gz - 4.将去除 rRNA 的数据比对到 hg19|hg38 上使用软件：subread 建立索引： subread-buildindex -o hg19 ./database/hg19.fasta 比对： subjunc -T 5 -i hg19 -r fastq1 -R fastq2 -o out.bam 统计 readcounts featureCounts -p -T 6 -t exon -g transcript_id -a all_lncRNA_know.gtf -o sample.lncRNA.readcount out.bam 这里需要注意：因为一个基因会包含多个转录本，一个转录本又包含多个外显子和内含子，所以为了精确比对结果，我们需要基于外显子进行比对-t exon，软件会自动根据注释文件进行合并同一个转录本的计数结果，并计算出转录本的长度汇总到结果中. 另外，需要注意的是，正因为可变剪切的存在，我们不能简单的认为转录本的计数结果相加即为对应基因的计数结果，这是不准确的，所以，要想知道基因的计数结果，我们还需要修改参数，另行计算-g gene_id 将计数的结果提取出来： awk -F &#x27;\\t&#x27; &#x27;&#123;print $1,$7&#125;&#x27; OFS=&#x27;\\t&#x27; sample.lncRNA.readcount &gt; ./tiqu/sample.counts 注：对于结果中的 length，有人以为就是有效长度（effctive length),但事实是，这个长度仅仅只是一个转录本上所有外显子长度的加和而已，因此，并不能作为有效长度。网上有估算有效长度的公式： efflength = transcripts_length - LFD +1 LFD 是测序时的平均片段长度，这个可以估算，但不准确，最好的方法是询问测序人员这个数值。 5.整合所有样本信息到表达矩阵并计算 TPM计算完 readcount，转录组测序的打基础过程已经完毕，后续的分析都是基于此的，但是，为了下游分析的方便，我们有必要将所有的数据整理到一个表达矩阵中。 使用python或者R或者shell，我建议使用 python,因为简单快速易控： import pandas as pdimport glob#使用pandas速度要更快，先预读一个样本作为dataframe的起始框架df=pd.read_csv(&quot;sample.lncRNA.trans.counts&quot;,sep=&quot;\\t&quot;,skiprows=1)#featurecount的计算结果提取后通常有这样的表头df=df[[&#x27;Geneid&#x27;,&#x27;LncRNA/bidui/normal/sample.subjunc.bam&#x27;]]df.columns=[&#x27;id&#x27;,&#x27;counts&#x27;]#遍历所有样本，使用pd.join整合到一个大表for name in glob.glob(&quot;*.counts&quot;): sample=name.replace(&quot;.lncRNA.trans.counts&quot;,&quot;&quot;) df2=pd.read_csv(name,sep=&quot;\\t&quot;,skiprows=1) df2.columns=[&#x27;id&#x27;,&#x27;counts&#x27;,&#x27;ot&#x27;] df2=df2[[&#x27;counts&#x27;]] df2.columns=[sample] df=pd.concat([df,df2],axis=1)#删除预读样本信息，并写入文件df.drop(&#x27;counts&#x27;,axis=1).set_index(&quot;id&quot;).to_csv(&quot;normal_lncRNA_trans_allcounts.csv&quot;,sep=&quot;\\t&quot;) 目前主流的表达量标准化方法多种多样，有 RPKM，FPKM，TPM，CPM 等，下游差异分析软件 deseq2 也有自己的标准化方法，但是，为了保证生物学意义，同时又保证数据的可比性，我还是建议选择 TPM。 reads Count定义: 高通量测序中比对到 exon 上的 reads 数。可使用 featureCount 等软件进行计算。 优点：可有效说明该区域是否真的有表达及真实的表达丰度。能够近似呈现真实的表达情况。有利于实验验证。 缺点：由于 exon 长度不同，难以进行不同 exon 丰度比较；由于测序总数不同，难以对不同测序样本间进行比较。 RPKM&#x2F;FPKM定义：RPKM: Reads Per Kilobase of exon model per Million mapped reads (每千个碱基的转录每百万映射读取的 reads)；FPKM: Fragments Per Kilobase of exon model per Million mapped fragments(每千个碱基的转录每百万映射读取的 fragments)公式： $$RPKM &#x3D;\\frac{ExonMappedReads * 10^9 }{TotalMappedReads * ExonLength}$$ 上述公式可从下面公式推导而出：$$RPKM&#x3D;\\frac{ExonMappedReads&#x2F;ExonLength*10^9}{TotalMappedreads&#x2F;GenomeLength}$$ 解释：ExonMappedReads 即为比对到该 exon 上的 reads count； TotalMappedReads 即为比对到基因组上所有 reads count 的总和；ExonLength 为该 Exon 的长度；GenomeLength 即为基因组全长，因为是相同基因组，所以该数值可消除。 优点：tophat-cufflinks 流程固定，应用范围广。理论上，可弥补 reads Count 的缺点，消除样本间和基因间差异。 讨论：有人说 RPKM&#x2F;FPKM 标准化特别不合理，看着是个大牛 YellowTree。公式 2 中，TotalMappedReads&#x2F;GenomeLength 为测序深度，ExonMappedReads &#x2F; ExonLength 可以简单的认为是该 Exon 上的“测序深度”。两者相除，就得出该 Exon 依据测序深度而进行的标准化，那么因 Exon 长短、测序深度造成的样本间造成的偏差，都可以消除。因一般是相同物种，基因组一般相同，所以公式 2 换算并消去 GenomeLength，就成为公式 1 的形式了。不知道哪里错了，斗胆提出质疑：RPKM&#x2F;FPKM 怎么就不能消除两种类型的 bias？不过有论文陈述说 RPKM 的结果难以消除组间测序造成的差异，可能未采用比对到基因组上所有的 reads 数，而是采用了比对到所有 Exon 的 reads 数作为 TotalMappedReads 吧。不是很确定。 FPKM：与 RPKM 计算过程类似。只有一点差异：RPKM 计算的是 reads，FPKM 计算的是 fragments。single-end&#x2F;paired-end 测序数据均可计算 reads count，fragments count 只能通过 paired-end 测序数据计算。paired-end 测序数据时，两端的 reads 比对到相同区域，且方向相反，即计数 1 个 fragments；如果只有单端 reads 比对到该区域，则一个 reads 即计数 1 个 fragments。所以 fragments count 接近且小于 2 * reads count。 RPM定义：RPM&#x2F;CPM: Reads&#x2F;Counts of exon model per Million mapped reads (每百万映射读取的 reads) 公式： $$RPM &#x3D; \\frac{ExonMappedReads * 10^6}{TotalMappedReads}$$ 优点：利于进行样本间比较。根据比对到基因组上的总 reads count，进行标准化。即：不论比对到基因组上的总 reads count 是多少，都将总 reads count 标准化为 10^6。 缺点：未消除 exon 长度造成的表达差异，难以进行样本内 exon 差异表达的比较。 TPM定义：TPM: Transcripts Per Kilobase of exon model per Million mapped reads (每千个碱基的转录每百万映射读取的 Transcripts) 公式： $$TPM&#x3D;\\frac{N_i&#x2F;L_i*10^6}{sum(N_1&#x2F;L_1+N_2&#x2F;L_2+··+N_n&#x2F;L_n)}$$ 解释：Ni 为比对到第 i 个 exon 的 reads 数； Li 为第 i 个 exon 的长度；sum(N1&#x2F;L1+N2&#x2F;L2 + … + Nn&#x2F;Ln)为所有 (n 个)exon 按长度进行标准化之后数值的和。 计算过程：首先对每个 exon 计算 Pi&#x3D;Ni&#x2F;Li，即按长度对 reads count 进行标准化；随后计算过程类似 RPM (将 Pi 作为正常的 ExonMappedReads，然后以 RPM 的公式计算 TPM)。 优点：首先消除 exon 长度造成的差异，随后消除样本间测序总 reads count 不同造成的差异。 缺点：因为不是采用比对到基因组上的总 reads count，所以特殊情况下不够准确。例如：某突变体对 exon 造成整体影响时，难以找出差异。 相互关系评价：以上几种计算 exon 表达丰度的方法，差异不是非常大。如果结果是显著的，那么采用上面任一计算方法大多均可找出显著结果。但是当表达风度差异不是那么显著时，不易区分不同类别，需要根据实际需要选择对应的标准化方法。 注意：以上 TotalMappedReads 推荐首选比对到基因组上的总 reads 数，而不是比对到 exon 或者 gene 上总 reads 数。这同样需要根据实际情况而确定。 如何根据 counts 计算 TPM？这里摘取一段网络代码： countToTpm &lt;- function(counts, effLen)&#123; rate &lt;- log(counts) - log(effLen) denom &lt;- log(sum(exp(rate))) exp(rate - denom + log(1e6))&#125;countToFpkm &lt;- function(counts, effLen)&#123; N &lt;- sum(counts) exp( log(counts) + log(1e9) - log(effLen) - log(N) )&#125;fpkmToTpm &lt;- function(fpkm)&#123; exp(log(fpkm) - log(sum(fpkm)) + log(1e6))&#125;countToEffCounts &lt;- function(counts, len, effLen)&#123; counts * (len / effLen)&#125;################################################################################# An example################################################################################cnts &lt;- c(4250, 3300, 200, 1750, 50, 0)lens &lt;- c(900, 1020, 2000, 770, 3000, 1777)countDf &lt;- data.frame(count = cnts, length = lens)# assume a mean(FLD) = 203.7countDf$effLength &lt;- countDf$length - 203.7 + 1countDf$tpm &lt;- with(countDf, countToTpm(count, effLength))countDf$fpkm &lt;- with(countDf, countToFpkm(count, effLength))with(countDf, all.equal(tpm, fpkmToTpm(fpkm)))countDf$effCounts &lt;- with(countDf, countToEffCounts(count, length, effLength)) 显而易见，要想通过 counts 计算 TPM，必须知道 count,efflength,但最难的也是这个有效长度 what is effective length of the feature?I traced back to this paper by Lior pachter group Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation. It is quite mathematical, but the general idea is: If we take the fragment length to be fixed, then the effective length is how many fragments can occur in the transcript. This turns out to be length - frag_len +1. The number of fragments coming from a transcript will be proportional to this number, regardless of whether you sequenced one or both ends of the fragment. In turn, when it comes to probabilistically assigning reads to transcripts the effective length plays a similar role again. Thus for short transcripts, there can be quite a difference between two fragment lengths. To go back to your example if you have transcript of length 310, your effective length is 10 (if fragment length is 300) or 160 (if fragment length is 150) in either case, which explains the discrepancy you see. From @Rob The effective length is computed by using the fragment length distribution to determine the effective number of positions that can be sampled on each transcript. You can think of this as convolving the fragment length distribution with the characteristic function (the function that simply takes a value of 1) over the transcript. For example if we observe fragments of length 50 — 1000, a position more than 1000 bases from the end of the transcript will contribute a value of 1 to the effective length, while a position 150 bases will contribute a value of F(150), where F is the cumulative distribution function of the fragment length distribution. For single end data, where we can’t learn an empirical FLD, we use a gaussian whose mean and standard deviation can be set with –fldMean and –fldSD respectively. From Harold Pimentel’s post above. He is in Lior Pachter’s group. Effective length refers to the number of possible start sites a feature could have generated a fragment of that particular length. In practice, the effective length is usually computed as: where uFDL is the mean of the fragment length distribution which was learned from the aligned read. If the abundance estimation method you’re using incorporates sequence bias modeling (such as eXpress or Cufflinks), the bias is often incorporated into the effective length by making the feature shorter or longer depending on the effect of the bias. 所以说，自己计算 efflength 相当麻烦而且可能出错，有没有简单的方法呢？ 有： 使用 salmon 软件会进行伪比对计算 TPM 和 count 同时会得到对应转录本的有效长度，将其计算结果中有效长度提取出来即可。 首先，提取对应有效长度给 fc 计算结果： import pandas as pdimport os,globfc_dir=&quot;lncRNA/normal/tiqu_trans&quot;sm_dir=&quot;LncRNA/salmon/normal/lncRNA&quot;fc=glob.glob(os.path.join(fc_dir,&quot;*.lncRNA.trans.counts&quot;))for fs in fc: sample_name=fs.replace(&quot;lncRNA/normal/tiqu_trans/&quot;,&quot;&quot;).replace(&quot;.lncRNA.trans.counts&quot;,&quot;&quot;) df=pd.read_csv(fs,sep=&quot;\\t&quot;,header=0,skiprows=1).iloc[:,[0,1]].set_index(&quot;Geneid&quot;) df.columns=[&#x27;count&#x27;] df2=pd.read_csv(os.path.join(sm_dir,&quot;&#123;&#125;_quant&quot;.format(sample_name),&quot;quant.sf&quot;),sep=&quot;\\t&quot;,header=0).set_index(&quot;Name&quot;) df1=df.join(df2[[&#x27;EffectiveLength&#x27;]]) df1.to_csv(os.path.join(fc_dir,&quot;&#123;&#125;.lncRNA.trans_eff.counts&quot;.format(sample_name)),sep=&quot;\\t&quot;) 然后计算 TPM： options(scipen = 200)options(digits = 3)for (name in Sys.glob(&quot;*.lncRNA.trans_eff.counts&quot;))&#123; countdata&lt;-read.table(name,sep=&quot;\\t&quot;,header = TRUE,row.names = &quot;Geneid&quot;) countToTpm &lt;- function(counts, effLen)&#123; rate &lt;- log(counts) - log(effLen) denom &lt;- log(sum(exp(rate))) exp(rate - denom + log(1e6)) &#125; countdata&lt;-na.omit(countdata) countdata$TPM&lt;-with(countdata,countToTpm(countdata$count,countdata$EffectiveLength)) id&lt;-unlist(strsplit(name,&quot;[.]&quot;))[1] write.table(countdata,paste(id,&quot;.lncRNA.trans.TPM&quot;,sep = &quot;&quot;),quote = FALSE,sep=&quot;\\t&quot;)&#125; 然后，将所有的 TPM 再次整合到表达矩阵中： import pandas as pdimport globdf=pd.read_csv(&quot;sample.lncRNA.trans.TPM&quot;,sep=&quot;\\t&quot;)df=df[[&#x27;TPM&#x27;]]for name in glob.glob(&quot;*.TPM&quot;): sample=name.replace(&quot;.lncRNA.trans.TPM&quot;,&quot;&quot;) df2=pd.read_csv(name,sep=&quot;\\t&quot;) df2=df2[[&#x27;TPM&#x27;]] df2.columns=[sample] df=df.join(df2)df.drop([&#x27;TPM&#x27;],axis=1).to_csv(&quot;normal_lncRNA_trans_TPM.csv&quot;,sep=&quot;\\t&quot;) 现在，又有一个问题了。……基因的表达量怎么算呢，基因的有效长度没法算啊，因为 salmon 只能计算转录本的表达量啊。这里，我们可以通过转录本推算基因有效长度： import argparseimport pandas as pdimport numpy as npimport sysdef main(args): gtable = pd.read_csv(args.ginput,skiprows=1,sep=&quot;\\t&quot;).set_index(&#x27;Geneid&#x27;) gtable = gtable.iloc[:,0:1] gtable.columns = [&#x27;Count&#x27;] ttable = pd.read_table(args.tinput).set_index(&#x27;Name&#x27;) tgmap = pd.read_table(args.tgmap, names=[&#x27;t&#x27;, &#x27;g&#x27;]).set_index(&#x27;t&#x27;) gene_lengths = &#123;&#125; j = 0 # Map over all gene groups (a gene and its associated transcripts) for g, txps in tgmap.groupby(&#x27;g&#x27;).groups.iteritems(): if j % 500 == 1: print(&quot;Processed &#123;&#125; genes&quot;.format(j)) j += 1 # The set of transcripts present in our salmon index tset = [] for t in txps: if t in ttable.index: tset.append(t) # If at least one of the transcripts was present if len(tset) &gt; 0: # The denominator is the sum of all TPMs totlen = ttable.loc[tset,&#x27;TPM&#x27;].sum() # Turn the relative TPMs into a proper partition of unity if totlen &gt; 0: tpm_fracs = ttable.loc[tset, &#x27;TPM&#x27;].values / ttable.loc[tset,&#x27;TPM&#x27;].sum() else: tpm_fracs = np.ones(len(tset)) / float(len(tset)) # Compute the gene&#x27;s effective length as the abundance-weight # sum of the transcript lengths elen = 0.0 for i,t in enumerate(tset): elen += tpm_fracs[i] * ttable.loc[t, &#x27;EffectiveLength&#x27;] gene_lengths[g] = elen # Give the table an effective length field gtable[&#x27;EffectiveLength&#x27;] = gtable.apply(lambda r : gene_lengths[r.name] if r.name in gene_lengths else 1.0, axis=1) # Write it to the output file gtable.to_csv(args.output, sep=&#x27;\\t&#x27;, index_label=&#x27;Name&#x27;)if __name__ == &quot;__main__&quot;: parser = argparse.ArgumentParser(description=&quot;Compute gene-level effective lengths&quot;) parser.add_argument(&#x27;--ginput&#x27;, type=str, help=&#x27;gene level input table&#x27;) parser.add_argument(&#x27;--tinput&#x27;, type=str, help=&#x27;transcript level input table&#x27;) parser.add_argument(&#x27;--tgmap&#x27;, type=str, help=&#x27;transcript -&gt; gene mapping&#x27;) parser.add_argument(&#x27;--output&#x27;, type=str, help=&#x27;output table with extra column&#x27;) args = parser.parse_args() main(args) python inferlength.py --ginput sample.lncRNA.counts --tinput sample.quant.sf --tgmap lnc_tx2gene.txt --output sample.eff.lnc.counts 然后，老办法计算 TPM,然后整合。 6.差异分析差异分析有两种情况，一种是组间差异，一种是样本间差异，组间差异的意思就是，有两组样本做比较，每组样本默认其都是重复性实验，即两组进行比较；而样本间差异意思就是虽然也是两个组进行比较，但是每个组内的样本是独立的，之间也是有差异的。 对于两种情况，分别推荐组间差异分析使用deseq2;样本间差异分析使用degseq 通常情况下就是，使用 deseq2 捕获不到的差异，会被 degseq 得到，但是否符合心中预期，还需要进一步衡量，一般要做热图的话，degseq 得到的差异基因很难形成红白对照，聚类的话，样本和组很难聚在一起。 Deseq2:library(DESeq2)path=getwd()normal&lt;-read.table(paste(path,&quot;/&quot;,&quot;normal_lncRNA_smcounts.csv&quot;,sep = &quot;&quot;),header = TRUE,row.names = &quot;Name&quot;,check.names=FALSE)sick&lt;-read.table(paste(path,&quot;/&quot;,&quot;sick_lncRNA_smcounts.csv&quot;,sep = &quot;&quot;),header = TRUE,row.names = &quot;Name&quot;,check.names=FALSE)mydata&lt;-cbind(normal,sick)sample&lt;-colnames(mydata)group_lst=c(rep(&quot;normal&quot;,length(colnames(normal))),rep(&quot;sick&quot;,length(colnames(sick))))colData &lt;- data.frame(row.names=sample,group_list=group_lst)dds &lt;- DESeqDataSetFromMatrix(countData = round(mydata),colData = colData,design = ~ group_list)register(MulticoreParam(4))dds2&lt;-DESeq(dds,parallel=TRUE)saveRDS(dds2,paste(path,&quot;/&quot;,&quot;lncRNA_normlized_by_deseq2_sm.rds&quot;,sep = &quot;&quot;))#suppressMessages(dds2)res &lt;- results(dds2, contrast=c(&quot;group_list&quot;,&quot;normal&quot;,&quot;sick&quot;))resOrdered &lt;- res[order(res$padj),]resOrdered=as.data.frame(resOrdered)diff_gene_deseq &lt;-subset(res, padj &lt; 0.05 &amp; abs(log2FoldChange) &gt; 1)write.table(diff_gene_deseq,file = paste(path,&quot;/&quot;,&quot;lncRNA_top_diff2_sm.txt&quot;,sep = &quot;&quot;),quote = FALSE,sep=&quot;\\t&quot;)plotdata&lt;-counts(dds2, normalized = TRUE)write.table(plotdata,file = paste(path,&quot;/&quot;,&quot;lncRNA_normlized_by_deseq2_sm.txt&quot;,sep = &quot;&quot;),quote = FALSE,sep=&quot;\\t&quot;) Degseq:library(qvalue)library(samr)library(impute)library(matrixStats)library(&quot;DEGseq&quot;)path=getwd()normal&lt;-read.table(paste(path,&quot;/&quot;,&quot;normal_lncRNA_smcounts.csv&quot;,sep = &quot;&quot;),header = TRUE,row.names = &quot;Name&quot;,check.names=FALSE)sick&lt;-read.table(paste(path,&quot;/&quot;,&quot;sick_lncRNA_smcounts.csv&quot;,sep = &quot;&quot;),header = TRUE,row.names = &quot;Name&quot;,check.names=FALSE)mydata&lt;-cbind(normal,sick)write.table(mydata,paste(path,&quot;/&quot;,&quot;lncRNA_all_smcounts.txt&quot;,sep = &quot;&quot;),quote=FALSE,sep=&quot;\\t&quot;)matrix1 &lt;- readGeneExp(paste(path,&quot;/&quot;,&quot;lncRNA_all_smcounts.txt&quot;,sep = &quot;&quot;), geneCol=1, valCol=c(2:83))matrix2 &lt;- readGeneExp(paste(path,&quot;/&quot;,&quot;lncRNA_all_smcounts.txt&quot;,sep = &quot;&quot;), geneCol=1, valCol=c(84:207))DEGexp(geneExpMatrix1=matrix1, geneCol1=1, expCol1=c(2:83), groupLabel1=&quot;nomal&quot;, geneExpMatrix2=matrix2, geneCol2=1, expCol2=c(2:125), groupLabel2=&quot;sick&quot;, pValue=1e-3, zScore=4, qValue=1e-3, foldChange=2, thresholdKind=5, method=&quot;MARS&quot;, outputDir=path) 合并 case，control 的 TPM 到一个矩阵：path=getwd()normal&lt;-read.table(paste(path,&quot;/&quot;,&quot;normal_mRNA_smTPM.csv&quot;,sep = &quot;&quot;),row.names=&quot;Name&quot;,header = TRUE,check.names=FALSE)sick&lt;-read.table(paste(path,&quot;/&quot;,&quot;sick_mRNA_smTPM.csv&quot;,sep = &quot;&quot;),row.names=&quot;Name&quot;,header = TRUE,check.names=FALSE)mydata&lt;-cbind(normal,sick)write.table(mydata,&quot;mRNA.salmon.TPM.txt&quot;,sep=&quot;\\t&quot;,quote=FALSE) 将表达量为 0 的过滤掉：path=getwd()mydata&lt;-read.table(paste(path,&quot;mRNA.salmon.TPM.txt&quot;,sep=&quot;/&quot;),header=TRUE)#这里也可以在read.table里面添加check.names=FALSEcolnames(mydata)&lt;-gsub(&quot;X&quot;,&quot;&quot;,colnames(mydata))top_m&lt;-read.table(paste(path,&quot;top_mRNA_degseq_fliter.csv&quot;,sep=&quot;/&quot;),header=TRUE,sep=&quot;\\t&quot;)top_m_TPM&lt;-mydata[top_m$GeneNames,]top_m_filter&lt;-top_m_TPM[apply(top_m_TPM,1,mean)&gt;0,]write.table(top_m_filter,&quot;top_m_filter.txt&quot;,sep=&quot;\\t&quot;,quote=FALSE) 7.对获取到的差异转录本做 GO PATHWAY 富集分析不推荐使用 Y 叔的clusterprofile,这个软件虽然易用，但是注释信息非常有限，功能也不完善，做富集分析和注释，还是需要使用在线的数据库注释，这里推荐几个非常好的网站： DAVID metascape webgestalt BioDbNet DAVID这个网站被诟病许久，大家都在吐槽它更新缓慢的数据库，但是用起来还是真香，滑稽。 metascape是我用过的迄今为止最好用的网站，它可以接受几乎所有的 id，做的分析有一般分析和自定义分析，自定义分析里面可以帮助挑选出我们想要的通路，得到其对应的基因，这个真的非常好用，而且它有一个 excel 插件，可以直接在 Excel 里面运行， webgestalt这个可能我还没发觉它真正的功能，我用着不好用，富集不全，但是图做的好，这个没得黑。 BioDbNet这个网站简直全能，可以做 id 转换，还可以做全注释，强无敌！！！ 对 LncRNA 的注释，可以使用 R 包LncPath，这个包用法简单，但最好需要配合WGCNA使用 8.对得到的差异基因做可视化目前我所做的可视化无非两种，一种是聚类热图，另一种是箱线图或者小提琴图，一种应用于大量基因，另一种是少量基因的情况。 热图很简单，直接用表达矩阵和分组信息做就可以了，默认参数，数据最好 log2(n+1) 箱线图同理，ggplot2 就可以做了 另外一种图是网络交互图，使用 igraph 9.寻找 lncRNA 的靶基因针对 lncRNA 研究，其中比较关键的点是确定 lncRNA 的靶基因，如果能够有方法预测到 lncRNA 的靶基因，会减轻我们的工作量。而如何去测 lncRNA 的靶基因，可以从 lncRNA 不同的作用模式入手。 lncRNAs 作用范围广泛，机制非常复杂。根据 lncRNA 不同的作用模式比如顺式（cis）和反式（trans）之分，比如 Signal，Decoy，Duide，Scaffold，也可以根据 lncRNA 与不同的分子分为 DNA、RNA 和蛋白，总体上包括了转录和转录后水平。 根据顺式（cis）和反式（trans）的作用模式来预测RNA 聚合酶转录得到一个与调控蛋白相关的 LncRNA 从而影响附近区域的编码基因。 cis 作用靶基因预测，认为 LncRNA 的功能与其坐标临近的蛋白编码基因相关，位于编码蛋白上下游的 LncRNA 可能与启动子或者共表达基因的其他顺式作用元件有交集，从而在转录或者转录后水平对基因的表达进行调控。判断一个 LncRNA 具有 Cis 调控作用通常要同时满足以下几个条件： 附近的基因表达情况与其保持一致； 该基因失活后会影响周围基因的表达； 会影响附近同一位点的基因表达。 对于满足以上条件的 LncRNA，首先找出位于其上游或者下游附近（10K）的编码蛋白基因，通过对编码蛋白的功能富集分析，从而预测 LncRNA 的主要功能，为后续 Cis 作用分析打下基础。 LncRNA 与 DNA 结合蛋白相关联，并调控相关靶基因的表达。 Trans 作用靶基因预测基本原理认为 LncRNA 的功能跟编码基因的位置关系没有关系，而与其共表达的蛋白编码基因相关。也就是说，当 LncRNA 与一些距离较远的基因在表达量上存在正相关或负相关的情况时，就可以通过样本间 lncRNA 与蛋白编码基因的表达量相关性分析或 WGCNA 共表达分析来预测其靶基因。 需要注意的是，lncRNA 的 Trans 作用靶基因预测只适合于样本量大的情况，如果样本量太少（如 6 个以下），分析将不可靠。 所以，我的分析方法是，首先分析相关性，使用 p 值（FDR）和相关性参数确定相关性较强的基因作为备选 然后，分析 mRNA 上游 10K 和下游 20K 的 lncRNA 作为 cis；不在范围内的使用 RNAplex 计算结合能，结合能小于-30 作为 trans 计算相关性： library(&quot;fdrtool&quot;)base&lt;-getwd()mydata&lt;-read.table(paste(base,&quot;top_m_filter.txt&quot;,sep=&quot;/&quot;),header=TRUE,check.names=FALSE)mydata&lt;-as.data.frame(t(mydata))chayi&lt;-read.table(paste(base,&quot;top_lnc_filter.txt&quot;,sep=&quot;/&quot;),header=TRUE,check.names=FALSE)chayi&lt;-as.data.frame(t(chayi))core&lt;-function(x,y)&#123; cor.test(x,y)$estimate&#125;pval&lt;-function(x,y)&#123; cor.test(x,y)$p.value&#125;fdr&lt;-function(x)&#123; fdr&lt;-fdrtool(x,statistic=&quot;pvalue&quot;) fdr$qval&#125;mycor&lt;-data.frame(test=rep(0,length(colnames(mydata))))for (i in 1:length(colnames(chayi)))&#123; haha&lt;-0 for (j in 1:length(colnames(mydata)))&#123; xiangguan&lt;-core(chayi[,i],mydata[,j]) haha&lt;-c(haha,xiangguan) &#125; mycor&lt;-cbind(mycor,as.data.frame(haha)[-1,])&#125;mycor&lt;-mycor[,-1]colnames(mycor)&lt;-colnames(chayi)rownames(mycor)&lt;-colnames(mydata)write.table(mycor,paste(base,&quot;correlation2.txt&quot;,sep=&quot;/&quot;),quote=FALSE)myp&lt;-data.frame(test=rep(0,length(colnames(mydata))))for (i in 1:length(colnames(chayi)))&#123; haha&lt;-0 for (j in 1:length(colnames(mydata)))&#123; xiangguan&lt;-pval(chayi[,i],mydata[,j]) haha&lt;-c(haha,xiangguan) &#125; myp&lt;-cbind(myp,as.data.frame(haha)[-1,])&#125;myp&lt;-myp[,-1]colnames(myp)&lt;-colnames(chayi)rownames(myp)&lt;-colnames(mydata)mfdr&lt;-apply(myp,2,fdr)write.table(mfdr,paste(base,&quot;p_values(FDR)2.txt&quot;,sep=&quot;/&quot;),quote=FALSE)##guolvhaha&lt;-data.frame(&quot;correlation&quot;=0,&quot;mRNA&quot;=0,&quot;lncRNA&quot;=0)xiangguan&lt;-read.table(paste(base,&quot;correlation2.txt&quot;,sep=&quot;/&quot;),header = TRUE)for (i in colnames(xiangguan))&#123; if (length(row.names(subset(xiangguan,abs(xiangguan[,i])&gt;0.6,select = i)))==0)&#123; next &#125;else&#123; xiang&lt;-subset(xiangguan,abs(xiangguan[,i])&gt;0.6,select = i) colnames(xiang)&lt;-&quot;correlation&quot; xiang$mRNA&lt;-rownames(xiang) xiang$lncRNA&lt;-i rownames(xiang)&lt;-NULL haha&lt;-rbind(haha,xiang) &#125;&#125;heihei&lt;-data.frame(&#x27;p_values(FDR)&#x27;=0,&quot;mRNA&quot;=0,&quot;lncRNA&quot;=0)colnames(heihei)[1]&lt;-&quot;p_values(FDR)&quot;xiangguan&lt;-read.table(paste(base,&quot;p_values(FDR)2.txt&quot;,sep=&quot;/&quot;),header = TRUE)for (i in colnames(xiangguan))&#123; if (length(row.names(subset(xiangguan,xiangguan[,i]&lt;0.05,select = i)))==0)&#123; next &#125;else&#123; xiang&lt;-subset(xiangguan,xiangguan[,i]&lt;0.05,select = i) colnames(xiang)&lt;-&quot;p_values(FDR)&quot; xiang$mRNA&lt;-rownames(xiang) xiang$lncRNA&lt;-i rownames(xiang)&lt;-NULL heihei&lt;-rbind(heihei,xiang) &#125;&#125;haha&lt;-haha[-1,]heihei&lt;-heihei[-1,]fin&lt;-merge(haha,heihei,by.x = c(&quot;lncRNA&quot;,&quot;mRNA&quot;),by.y = c(&quot;lncRNA&quot;,&quot;mRNA&quot;))write.table(fin,paste(base,&quot;xiangguanxing2.txt&quot;,sep=&quot;/&quot;),quote = FALSE,row.names = FALSE) note: 这里计算的方法是使用 for 循环迭代结算，效率较低，不适合与大量数据，而且这里两个数据表一定要有相同的样本顺序，不然计算的相关性和 P 值会有显著的不同。最后，对于 FDR 矫正，这里只是针对一个 lncRNA 和多个 mRNA 的 P 值做了矫正。 library(Hmisc)library(reshape2)library(fdrtool)lncRNA&lt;-read.table(&quot;lncRNA_difference.txt&quot;,header = TRUE,check.names = FALSE,stringsAsFactors = FALSE,sep=&quot;\\t&quot;)mRNA&lt;-read.table(&quot;mRNA_difference.txt&quot;,header = TRUE,check.names = FALSE,stringsAsFactors = FALSE,sep=&quot;\\t&quot;)n_lnc&lt;-nrow(lncRNA)n_mRNA&lt;-nrow(mRNA)alldata&lt;-rbind(lncRNA,mRNA)mydata&lt;-as.matrix(t(alldata))p&lt;-rcorr(mydata)cor&lt;-p[[&quot;r&quot;]][1:n_lnc,(n_lnc+1):nrow(alldata)]pvalue&lt;-p[[&quot;P&quot;]][1:n_lnc,(n_lnc+1):nrow(alldata)]cor&lt;-cbind(rownames(cor),cor)colnames(cor)[1]&lt;-&quot;LncRNA&quot;pvalue&lt;-cbind(rownames(pvalue),pvalue)colnames(pvalue)[1]&lt;-&quot;LncRNA&quot;new_cor&lt;-melt(as.data.frame(cor),id.vars=&quot;LncRNA&quot;,variable.name = &quot;mRNA&quot;,value.name = &quot;correlation&quot;)new_p&lt;-melt(as.data.frame(pvalue),id.vars=&quot;LncRNA&quot;,variable.name = &quot;mRNA&quot;,value.name = &quot;p.value&quot;)jiaozheng&lt;-function(x)&#123;fdrtool(x,statistic = &quot;pvalue&quot;,plot = FALSE,verbose = FALSE)$qval&#125;all_cor&lt;-merge(new_cor, new_p, by.x=c(&quot;LncRNA&quot;,&quot;mRNA&quot;), by.y=c(&quot;LncRNA&quot;,&quot;mRNA&quot;))all_cor&lt;-na.omit(all_cor)all_cor$p.value&lt;-as.numeric(all_cor$p.value)all_cor$correlation&lt;-as.numeric(all_cor$correlation)all_cor$p.asjust&lt;-jiaozheng(all_cor$p.value)all_fli&lt;-subset(all_cor,abs(all_cor$correlation)&gt;0.6&amp;all_cor$p.asjust&lt;0.05)write.table(all_fli,&quot;xiangguanxing.txt&quot;,quote=FALSE,row.names=FALSE) 这个脚本可以解决迭代问题，而且避免了样本不一致的情况。计算 cis 首先，获取 bed 文件 anno_m&lt;-read.table(&quot;LncRNA/readscounts/gene_hit/hg19_all_transcript.bed&quot;,sep = &quot;\\t&quot;,header = FALSE)anno_lnc&lt;-read.table(&quot;LncRNA/readscounts/gene_hit/NONCODE.bed&quot;,sep = &quot;\\t&quot;,header = FALSE)chayi&lt;-read.table(&quot;xiangguanxing2.txt&quot;,header = TRUE)lncRNA&lt;-subset(anno_lnc,V4%in%chayi$lncRNA)mRNA&lt;-subset(anno_m,V4%in%chayi$mRNA)write.table(lncRNA,&quot;lncRNA.bed&quot;,sep = &quot;\\t&quot;,row.names = FALSE,col.names = FALSE,quote = FALSE)write.table(mRNA,&quot;mRNA.bed&quot;,sep = &quot;\\t&quot;,row.names = FALSE,col.names = FALSE,quote = FALSE) 所有基因的 bed 文件 noncode.bed 和 hg19.bed 可以在网上下载，也可以自己做，自己做的话使用 python 的 gtf_parse 包对 gtf 文件进行解析，然后做一个就行了 然后，使用 bedtools 计算位置信息： windowBed -a mRNA.bed -b LncRNA.bed -l 10240 -r 20480 -sm &gt; cis.txt 计算 trans 结合能，需要首先获取转录组序列，然后运行 RNAplex import pandas as pdimport subprocess,os,refrom Bio import SeqIOhg19=&quot;LncRNA/testdir/all_mRNA_know.fa&quot;noncode=&quot;LncRNA/testdir/all_lncRNA_know.fa&quot;base=&quot;LncRNA/readscounts/salmon/gene_hit&quot;hg19_fasta=&#123;seq_record.id:str(seq_record.seq) for seq_record in SeqIO.parse(hg19,&#x27;fasta&#x27;)&#125;lnc_fasta=&#123;seq_record.id:str(seq_record.seq) for seq_record in SeqIO.parse(noncode,&#x27;fasta&#x27;)&#125;def get_energy(lncRNA,mRNA): fa=&quot;lnc_tmp.fasta&quot; fa2=&quot;mRNA_tmp.fasta&quot; with open(fa2,&#x27;w&#x27;) as f: f.write(&quot;&gt;&#123;&#125;\\n&#123;&#125;&quot;.format(mRNA,hg19_fasta[mRNA])) with open(fa,&#x27;w&#x27;) as l: l.write(&quot;&gt;&#123;&#125;\\n&#123;&#125;&quot;.format(lncRNA,lnc_fasta[lncRNA])) get_energy=[&#x27;app/RNAplex-0.2/RNAplex/bin/RNAplex&#x27;,&#x27;-t&#x27;,fa2,&#x27;-q&#x27;,fa] cm3=subprocess.Popen(get_energy,stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE) out,err=cm3.communicate() ans=0 for line in out.decode(&quot;utf-8&quot;).splitlines(): if not line.startswith(&quot;&gt;&quot;): ans=re.findall(&quot;\\(-?\\w+.?\\w+\\)&quot;,line)[0].replace(&quot;(&quot;,&quot;&quot;).replace(&quot;)&quot;,&quot;&quot;) #print(ans) os.remove(fa2) os.remove(fa) return answith open(&quot;xiangguanxing2.txt&quot;) as f: for line2 in f: if line2.startswith(&quot;NONH&quot;): lncRNA,mRNA,*ot=line2.split(&quot; &quot;) print(&quot;&#123;&#125;--&#123;&#125;:&quot;.format(lncRNA,mRNA),get_energy(lncRNA,mRNA)) 注意：这里想要获取转录本序列，千万不要使用 bedtools getfast，因为这样获取的是整个转录本的基因组序列，包含内含子，所以，我们使用 cufflinks 软件中 gffread 命令来获取转录本序列 gffread -w out.fa -g hg19.fa in.gtf 根据 lncRNA 结合的核酸序列来预测实际上这种方式是最容易想到的，因为 lncRNA 是核酸，而核酸与核酸之间是有碱基互补配对的，不管是 lncRNA 与 RNA（比如 microRNA，mRNA）之间还是 lncRNA 与 DNA 之间，我们可以根据互补配对这一特性进行预测。这里我们展开举三个例子： A、lncRNA-microRNA-mRNA microRNA 对 mRNA 靶基因预测的方式可以反过来用于 lncRNA 靶基因预测，这里适用的模式就是 lncRNA 作为 microRNA sponge 吸附 microRNA，或者进一步通过 ceRNA 的作用机制调控靶基因，原理都是 microRNA 与 lncRNA 和 mRNA 的序列结合，这种方式也是现在预测靶基因用的最多的，比如网站：starbase（http://starbase.sysu.edu.cn/）。 B、lncRNA-mRNA 在上面这种模式里，lncRNA 对靶基因 mRNA 的调控是通过 microRNA 来介导的，当然 lncRNA 与 mRNA 之间的直接互补也能帮我们预测靶基因，这种模式我们以前说过，比较常见的是反义 lncRNA（Antisense lncRNA）通过与 mRNA 结合形成 RNA 二聚体https://www.jianshu.com/p/92451bd7c030，保护 mRNA 使之更难被 RNA 酶降解。因此这种模式来预测靶基因的原理是预测 lncRNA 序列与 mRNA 序列结合的自由能，自由能越小，越容易结合。这种模式的在线工具有 lncRNATargets（http://www.herbbol.org:8001/lrt/） C、lncRNA-DNA 在以前文章中我们也介绍过 lncRNA 通过结合单链 DNA 发挥调控作用的： 因此，基于同样的原理，理论上我们也可以进行预测，这种预测工具 lncRNATargets 同样可实现。 根据蛋白的结合特性来预测:lncRNA 核酸与核酸结合依据的是碱基配对原理，同样某些蛋白与核酸的结合也有一定规律，比如 RNA 结合蛋白，因此，我们可以依据这些规律预测 lncRNA 的结合蛋白，比如工具包括 RBPDB（http://rbpdb.ccbr.utoronto.ca/index.php）、RNAcommender（http://rnacommender.disi.unitn.it/）等。","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"},{"name":"转录组","slug":"转录组","permalink":"https://xiaohanys.top/tags/%E8%BD%AC%E5%BD%95%E7%BB%84/"}],"author":"Xiaoguang Pan"},{"title":"教程：整合刺激性和对照性PBMC数据集，以学习细胞类型特异性反应","slug":"整合刺激性和对照性PBMC数据集","date":"2019-10-20T05:02:08.000Z","updated":"2024-10-18T05:33:27.315Z","comments":true,"path":"scrna-diff/","permalink":"https://xiaohanys.top/scrna-diff/","excerpt":"文章已经过时，请去官网查阅相关文档 本教程介绍了Kang等人（2017)的两组PBMC的对齐方式。在该实验中，将PBMC分为刺激组和对照组，并用干扰素β治疗刺激组。对干扰素的反应引起细胞类型特异性基因表达的变化，这使得对所有数据进行联合分析变得困难，并且细胞按刺激条件和细胞类型聚类。在这里，我们证明了我们的整合策略，如Stuart和Butler等人（2018年）所述，用于执行整合分析以促进常见细胞类型的鉴定并进行比较分析。尽管此示例演示了两个数据集（条件）的集成，但这些方法已扩展到多个数据集。这个工作流程提供了整合四个胰岛数据集的示例。","text":"文章已经过时，请去官网查阅相关文档 本教程介绍了Kang等人（2017)的两组PBMC的对齐方式。在该实验中，将PBMC分为刺激组和对照组，并用干扰素β治疗刺激组。对干扰素的反应引起细胞类型特异性基因表达的变化，这使得对所有数据进行联合分析变得困难，并且细胞按刺激条件和细胞类型聚类。在这里，我们证明了我们的整合策略，如Stuart和Butler等人（2018年）所述，用于执行整合分析以促进常见细胞类型的鉴定并进行比较分析。尽管此示例演示了两个数据集（条件）的集成，但这些方法已扩展到多个数据集。这个工作流程提供了整合四个胰岛数据集的示例。 整合目标以下教程旨在概述使用Seurat集成过程可能进行的复杂细胞类型的比较分析。在这里，我们解决了三个主要目标： 识别两个数据集中都存在的单元格类型 获得在对照和刺激细胞中均保守的细胞类型标记 比较数据集以找到对刺激的细胞类型特异性反应 设置Seurat对象为方便起见，我们通过SeuratData软件包分发此数据集。 library(Seurat)library(SeuratData)library(cowplot)InstallData(&quot;ifnb&quot;)data(&quot;ifnb&quot;)ifnb.list &lt;- SplitObject(ifnb, split.by = &quot;stim&quot;)ifnb.list &lt;- lapply(X = ifnb.list, FUN = function(x) &#123; x &lt;- NormalizeData(x) x &lt;- FindVariableFeatures(x, selection.method = &quot;vst&quot;, nfeatures = 2000)&#125;) 执行整合然后，我们使用FindIntegrationAnchors函数来识别锚点，该函数将Seurat对象的列表作为输入，并使用这些锚点将两个数据集与集成在一起。 immune.anchors &lt;- FindIntegrationAnchors(object.list = ifnb.list, dims = 1:20)immune.combined &lt;- IntegrateData(anchorset = immune.anchors, dims = 1:20) 进行综合分析现在，我们可以在所有单元上运行单个集成分析！ DefaultAssay(immune.combined) &lt;- &quot;integrated&quot;# Run the standard workflow for visualization and clusteringimmune.combined &lt;- ScaleData(immune.combined, verbose = FALSE)immune.combined &lt;- RunPCA(immune.combined, npcs = 30, verbose = FALSE)# t-SNE and Clusteringimmune.combined &lt;- RunUMAP(immune.combined, reduction = &quot;pca&quot;, dims = 1:20)immune.combined &lt;- FindNeighbors(immune.combined, reduction = &quot;pca&quot;, dims = 1:20)immune.combined &lt;- FindClusters(immune.combined, resolution = 0.5)# Visualizationp1 &lt;- DimPlot(immune.combined, reduction = &quot;umap&quot;, group.by = &quot;stim&quot;)p2 &lt;- DimPlot(immune.combined, reduction = &quot;umap&quot;, label = TRUE)plot_grid(p1, p2) 为了并排可视化这两个条件，我们可以使用split.by参数来显示每个以聚类着色的条件。 DimPlot(immune.combined, reduction = &quot;umap&quot;, split.by = &quot;stim&quot;) 识别保守的细胞类型标记为了确定跨条件保守的规范细胞类型标记基因，我们提供了该FindConservedMarkers功能。此功能对每个数据集&#x2F;组执行差异基因表达测试，并使用MetaDER软件包中的荟萃分析方法组合p值。例如，无论簇7中的刺激条件如何，我们都可以计算出保守标记的基因（NK细胞）。 DefaultAssay(immune.combined) &lt;- &quot;RNA&quot;nk.markers &lt;- FindConservedMarkers(immune.combined, ident.1 = 7, grouping.var = &quot;stim&quot;, verbose = FALSE)head(nk.markers)## CTRL_p_val CTRL_avg_logFC CTRL_pct.1 CTRL_pct.2 CTRL_p_val_adj## SNHG12 1.059703e-193 1.3678805 0.335 0.019 1.489200e-189## HSPH1 2.552539e-139 2.0586178 0.553 0.100 3.587083e-135## NR4A2 8.671555e-136 1.5545769 0.296 0.023 1.218614e-131## SRSF2 1.556024e-113 1.6410606 0.704 0.220 2.186680e-109## BATF 1.573042e-09 0.5991204 0.116 0.042 2.210596e-05## CD69 1.188324e-122 1.8357378 0.525 0.096 1.669952e-118## STIM_p_val STIM_avg_logFC STIM_pct.1 STIM_pct.2 STIM_p_val_adj## SNHG12 8.090842e-159 1.054494 0.256 0.015 1.137006e-154## HSPH1 4.097380e-89 1.580183 0.471 0.114 5.758049e-85## NR4A2 3.130700e-78 1.009556 0.172 0.015 4.399572e-74## SRSF2 1.829674e-128 1.625081 0.675 0.182 2.571241e-124## BATF 9.234006e-126 1.354443 0.305 0.031 1.297655e-121## CD69 3.733167e-78 1.677068 0.688 0.291 5.246220e-74## max_pval minimump_p_val## SNHG12 8.090842e-159 2.119406e-193## HSPH1 4.097380e-89 5.105078e-139## NR4A2 3.130700e-78 1.734311e-135## SRSF2 1.556024e-113 3.659348e-128## BATF 1.573042e-09 1.846801e-125## CD69 3.733167e-78 2.376649e-122 我们可以为每个簇探索这些标记基因，并使用它们将我们的簇注释为特定的细胞类型。 FeaturePlot(immune.combined, features = c(&quot;CD3D&quot;, &quot;SELL&quot;, &quot;CREM&quot;, &quot;CD8A&quot;, &quot;GNLY&quot;, &quot;CD79A&quot;, &quot;FCGR3A&quot;, &quot;CCL2&quot;, &quot;PPBP&quot;), min.cutoff = &quot;q9&quot;) immune.combined &lt;- RenameIdents(immune.combined, `0` = &quot;CD14 Mono&quot;, `1` = &quot;CD4 Naive T&quot;, `2` = &quot;CD4 Memory T&quot;, `3` = &quot;CD16 Mono&quot;, `4` = &quot;B&quot;, `5` = &quot;CD8 T&quot;, `6` = &quot;T activated&quot;, `7` = &quot;NK&quot;, `8` = &quot;DC&quot;, `9` = &quot;B Activated&quot;, `10` = &quot;Mk&quot;, `11` = &quot;pDC&quot;, `12` = &quot;Eryth&quot;, `13` = &quot;Mono/Mk Doublets&quot;)DimPlot(immune.combined, label = TRUE) DotPlot带有split.by参数的函数可用于查看各种条件下保守的细胞类型标记，显示表达水平和表达任何给定基因的簇中细胞的百分比。在这里，我们为13个簇中的每个簇绘制2-3个强标记基因。 Idents(immune.combined) &lt;- factor(Idents(immune.combined), levels = c(&quot;Mono/Mk Doublets&quot;, &quot;pDC&quot;, &quot;Eryth&quot;, &quot;Mk&quot;, &quot;DC&quot;, &quot;CD14 Mono&quot;, &quot;CD16 Mono&quot;, &quot;B Activated&quot;, &quot;B&quot;, &quot;CD8 T&quot;, &quot;NK&quot;, &quot;T activated&quot;, &quot;CD4 Naive T&quot;, &quot;CD4 Memory T&quot;))markers.to.plot &lt;- c(&quot;CD3D&quot;, &quot;CREM&quot;, &quot;HSPH1&quot;, &quot;SELL&quot;, &quot;GIMAP5&quot;, &quot;CACYBP&quot;, &quot;GNLY&quot;, &quot;NKG7&quot;, &quot;CCL5&quot;, &quot;CD8A&quot;, &quot;MS4A1&quot;, &quot;CD79A&quot;, &quot;MIR155HG&quot;, &quot;NME1&quot;, &quot;FCGR3A&quot;, &quot;VMO1&quot;, &quot;CCL2&quot;, &quot;S100A9&quot;, &quot;HLA-DQA1&quot;, &quot;GPR183&quot;, &quot;PPBP&quot;, &quot;GNG11&quot;, &quot;HBA2&quot;, &quot;HBB&quot;, &quot;TSPAN13&quot;, &quot;IL3RA&quot;, &quot;IGJ&quot;)DotPlot(immune.combined, features = rev(markers.to.plot), cols = c(&quot;blue&quot;, &quot;red&quot;), dot.scale = 8, split.by = &quot;stim&quot;) + RotatedAxis() 跨条件鉴定差异表达基因现在我们已经排列了刺激细胞和对照细胞，我们可以开始进行比较分析，并观察刺激引起的差异。广泛观察这些变化的一种方法是绘制受刺激细胞和对照细胞的平均表达，并在散点图上寻找视觉异常值的基因。在这里，我们采用受刺激的和对照的原始T细胞和CD14单核细胞群体的平均表达，并生成散点图，突出显示对干扰素刺激表现出戏剧性反应的基因。 library(ggplot2)library(cowplot)theme_set(theme_cowplot())t.cells &lt;- subset(immune.combined, idents = &quot;CD4 Naive T&quot;)Idents(t.cells) &lt;- &quot;stim&quot;avg.t.cells &lt;- log1p(AverageExpression(t.cells, verbose = FALSE)$RNA)avg.t.cells$gene &lt;- rownames(avg.t.cells)cd14.mono &lt;- subset(immune.combined, idents = &quot;CD14 Mono&quot;)Idents(cd14.mono) &lt;- &quot;stim&quot;avg.cd14.mono &lt;- log1p(AverageExpression(cd14.mono, verbose = FALSE)$RNA)avg.cd14.mono$gene &lt;- rownames(avg.cd14.mono)genes.to.label = c(&quot;ISG15&quot;, &quot;LY6E&quot;, &quot;IFI6&quot;, &quot;ISG20&quot;, &quot;MX1&quot;, &quot;IFIT2&quot;, &quot;IFIT1&quot;, &quot;CXCL10&quot;, &quot;CCL8&quot;)p1 &lt;- ggplot(avg.t.cells, aes(CTRL, STIM)) + geom_point() + ggtitle(&quot;CD4 Naive T Cells&quot;)p1 &lt;- LabelPoints(plot = p1, points = genes.to.label, repel = TRUE)p2 &lt;- ggplot(avg.cd14.mono, aes(CTRL, STIM)) + geom_point() + ggtitle(&quot;CD14 Monocytes&quot;)p2 &lt;- LabelPoints(plot = p2, points = genes.to.label, repel = TRUE)plot_grid(p1, p2) 如您所见，许多相同的基因在这两种细胞类型中均被上调，可能代表保守的干扰素应答途径。 因为我们有信心在各种情况下都能识别出常见的细胞类型，所以我们可以询问相同类型的细胞在不同条件下会改变哪些基因。首先，我们在meta.data插槽中创建一列，以保存细胞类型和刺激信息，并将当前标识切换到该列。然后，我们使用它FindMarkers来找到受刺激的B细胞和对照B细胞之间不同的基因。请注意，此处显示的许多顶级基因与我们之前绘制的核心干扰素应答基因相同。此外，我们看到的诸如CXCL10的基因对单核细胞和B细胞干扰素的反应也具有特异性，在该列表中也显示出很高的意义。 immune.combined$celltype.stim &lt;- paste(Idents(immune.combined), immune.combined$stim, sep = &quot;_&quot;)immune.combined$celltype &lt;- Idents(immune.combined)Idents(immune.combined) &lt;- &quot;celltype.stim&quot;b.interferon.response &lt;- FindMarkers(immune.combined, ident.1 = &quot;B_STIM&quot;, ident.2 = &quot;B_CTRL&quot;, verbose = FALSE)head(b.interferon.response, n = 15)## p_val avg_logFC pct.1 pct.2 p_val_adj## ISG15 9.008631e-168 3.2061225 0.998 0.235 1.265983e-163## IFIT3 1.566158e-161 3.1240285 0.961 0.049 2.200921e-157## ISG20 1.206176e-158 2.0549983 1.000 0.662 1.695038e-154## IFI6 1.544804e-158 2.9139826 0.959 0.077 2.170914e-154## IFIT1 1.531805e-144 2.8529052 0.899 0.031 2.152645e-140## MX1 3.490304e-129 2.2712525 0.902 0.113 4.904924e-125## LY6E 3.320706e-127 2.1767293 0.897 0.146 4.666588e-123## TNFSF10 2.108526e-114 2.5996223 0.773 0.021 2.963111e-110## IFIT2 7.373988e-113 2.5114519 0.781 0.035 1.036267e-108## B2M 1.570724e-101 0.4194467 1.000 1.000 2.207338e-97## PLSCR1 8.204551e-101 1.9635626 0.792 0.115 1.152986e-96## IRF7 1.517587e-100 1.8203577 0.837 0.181 2.132665e-96## CXCL10 7.783861e-92 3.6869595 0.660 0.012 1.093866e-87## UBE2L6 1.659777e-88 1.4951634 0.855 0.296 2.332484e-84## EPSTI1 1.324448e-82 1.7540298 0.717 0.103 1.861247e-78 可视化基因表达中这些变化的另一种有用方法是split.by选择FeaturePlot或VlnPlot功能。这将显示给定基因列表的FeaturePlots，并按分组变量（此处为刺激条件）进行划分。诸如CD3D和GNLY之类的基因是典型的细胞类型标记（对于T细胞和NK &#x2F; CD8 T细胞），实际上不受干扰素刺激的影响，并且在对照组和受刺激组中显示出相似的基因表达模式。另一方面，IFI6和ISG15是核心干扰素应答基因，因此在所有细胞类型中均被上调。最后，CD14和CXCL10是显示细胞类型特异性干扰素应答的基因。CD14单核细胞受刺激后，CD14表达下降，这可能导致在有监督的分析框架中进行错误分类，从而强调了整合分析的价值。 FeaturePlot(immune.combined, features = c(&quot;CD3D&quot;, &quot;GNLY&quot;, &quot;IFI6&quot;), split.by = &quot;stim&quot;, max.cutoff = 3, cols = c(&quot;grey&quot;, &quot;red&quot;)) plots &lt;- VlnPlot(immune.combined, features = c(&quot;LYZ&quot;, &quot;ISG15&quot;, &quot;CXCL10&quot;), split.by = &quot;stim&quot;, group.by = &quot;celltype&quot;, pt.size = 0, combine = FALSE)CombinePlots(plots = plots, ncol = 1)","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"单细胞测序","slug":"单细胞测序","permalink":"https://xiaohanys.top/tags/%E5%8D%95%E7%BB%86%E8%83%9E%E6%B5%8B%E5%BA%8F/"},{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"},{"title":"Seurat3.1的灵活操作指南","slug":"Seurat3-1的灵活操作指南","date":"2019-10-20T04:47:57.000Z","updated":"2024-10-18T05:34:12.793Z","comments":true,"path":"seurat3-operate/","permalink":"https://xiaohanys.top/seurat3-operate/","excerpt":"文章已经过时，请去官网查阅相关文档 下面演示了一些与Seurat对象进行交互的有用功能。出于演示目的，我们将使用在第一个指导教程中创建的2700 PBMC对象。您可以在此处下载预先计算的对象。为了模拟有两个重复的情况，将一半命名为“rep1”,另一半命名为”rep2”","text":"文章已经过时，请去官网查阅相关文档 下面演示了一些与Seurat对象进行交互的有用功能。出于演示目的，我们将使用在第一个指导教程中创建的2700 PBMC对象。您可以在此处下载预先计算的对象。为了模拟有两个重复的情况，将一半命名为“rep1”,另一半命名为”rep2” 官网3.1版本已经无法找到该指南的链接，其实还是有的，网址：https://satijalab.org/seurat/v3.1/interaction_vignette.html 载入数据下面演示了一些与Seurat对象进行交互的有用功能。出于演示目的，我们将使用在第一个指导教程中创建的2700 PBMC对象。您可以在此处下载预先计算的对象。为了模拟有两个重复的情况，将一半命名为“rep1”,另一半命名为”rep2” library(Seurat)pbmc &lt;- readRDS(file = &quot;../data/pbmc3k_final.rds&quot;)### 随机设置两个重复set.seed(42)pbmc$replicate &lt;- sample(c(&quot;rep1&quot;, &quot;rep2&quot;), size = ncol(pbmc), replace = TRUE) 从细胞聚类和样本重复中切换Idents# 默认画的是object@ident)DimPlot(pbmc, reduction = &quot;umap&quot;) # 把细胞分类因子先储存到对象中pbmc$CellType &lt;- Idents(pbmc)# 切换IdentsIdents(pbmc) &lt;- &quot;replicate&quot;DimPlot(pbmc, reduction = &quot;umap&quot;) # alternately : DimPlot(pbmc, reduction = &#x27;umap&#x27;, group.by = &#x27;replicate&#x27;) you can pass the# shape.by to label points by both replicate and cell type# Switch back to cell type labelsIdents(pbmc) &lt;- &quot;CellType&quot; 分别或同时统计不同聚类或者不同样本来源的细胞数目# 每个聚类包含多少细胞？table(Idents(pbmc)) ## ## Naive CD4 T Memory CD4 T CD14+ Mono B CD8 T ## 697 483 480 344 271 ## FCGR3A+ Mono NK DC Platelet ## 162 155 32 14 # 每组重复包含多少细胞？table(pbmc$replicate) ## ## rep1 rep2 ## 1348 1290 # 每个聚类细胞数占比prop.table(table(Idents(pbmc))) ## ## Naive CD4 T Memory CD4 T CD14+ Mono B CD8 T ## 0.264215315 0.183093252 0.181956027 0.130401820 0.102729340 ## FCGR3A+ Mono NK DC Platelet ## 0.061410159 0.058756634 0.012130402 0.005307051 # 样本分组和细胞聚类一起统计table(Idents(pbmc), pbmc$replicate) ## ## rep1 rep2## Naive CD4 T 354 343## Memory CD4 T 249 234## CD14+ Mono 232 248## B 173 171## CD8 T 154 117## FCGR3A+ Mono 81 81## NK 81 74## DC 18 14## Platelet 6 8 prop.table(table(Idents(pbmc), pbmc$replicate), margin = 2) ## ## rep1 rep2## Naive CD4 T 0.262611276 0.265891473## Memory CD4 T 0.184718101 0.181395349## CD14+ Mono 0.172106825 0.192248062## B 0.128338279 0.132558140## CD8 T 0.114243323 0.090697674## FCGR3A+ Mono 0.060089021 0.062790698## NK 0.060089021 0.057364341## DC 0.013353116 0.010852713## Platelet 0.004451039 0.006201550 提取特定的Seurat子集做亚型分析# What are the cell names of all NK cells?WhichCells(pbmc, idents = &quot;NK&quot;) ## [1] &quot;AAACCGTGTATGCG&quot; &quot;AAATTCGATTCTCA&quot; &quot;AACCTTACGCGAGA&quot; &quot;AACGCCCTCGTACA&quot;## [5] &quot;AACGTCGAGTATCG&quot; &quot;AAGATTACCTCAAG&quot; &quot;AAGCAAGAGCTTAG&quot; &quot;AAGCAAGAGGTGTT&quot;## [9] &quot;AAGTAGGATACAGC&quot; &quot;AATACTGAATTGGC&quot; &quot;AATCCTTGGTGAGG&quot; &quot;AATCTCTGCTTTAC&quot;## [13] &quot;ACAAATTGTTGCGA&quot; &quot;ACAACCGAGGGATG&quot; &quot;ACAATTGATGACTG&quot; &quot;ACACCCTGGTGTTG&quot;## [17] &quot;ACAGGTACTGGTGT&quot; &quot;ACCTGGCTAAGTAG&quot; &quot;ACGAACACCTTGTT&quot; &quot;ACGATCGAGGACTT&quot;## [21] &quot;ACGCAATGGTTCAG&quot; &quot;ACGCTGCTGTTCTT&quot; &quot;ACGGAACTCAGATC&quot; &quot;ACGTGATGTGACAC&quot;## [25] &quot;ACGTTGGAGCCAAT&quot; &quot;ACTGCCACTCCGTC&quot; &quot;ACTGGCCTTCAGTG&quot; &quot;ACTTCAACGTAGGG&quot;## [29] &quot;AGAACAGAAATGCC&quot; &quot;AGATATACCCGTAA&quot; &quot;AGATTCCTGTTCAG&quot; &quot;AGCCTCTGCCAATG&quot;## [33] &quot;AGCGATTGAGATCC&quot; &quot;AGGATGCTTTAGGC&quot; &quot;AGGGACGAGTCAAC&quot; &quot;AGTAATACATCACG&quot;## [37] &quot;AGTCACGATGAGCT&quot; &quot;AGTTTGCTACTGGT&quot; &quot;ATACCACTGCCAAT&quot; &quot;ATACTCTGGTATGC&quot;## [41] &quot;ATCCCGTGCAGTCA&quot; &quot;ATCTTTCTTGTCCC&quot; &quot;ATGAAGGACTTGCC&quot; &quot;ATGATAACTTCACT&quot;## [45] &quot;ATGATATGGTGCTA&quot; &quot;ATGGACACGCATCA&quot; &quot;ATGGGTACATCGGT&quot; &quot;ATTAACGATGAGAA&quot;## [49] &quot;ATTCCAACTTAGGC&quot; &quot;CAAGGTTGTCTGGA&quot; &quot;CAATCTACTGACTG&quot; &quot;CACCACTGGCGAAG&quot;## [53] &quot;CACGGGTGGAGGAC&quot; &quot;CAGATGACATTCTC&quot; &quot;CAGCAATGGAGGGT&quot; &quot;CAGCGGACCTTTAC&quot;## [57] &quot;CAGCTCTGTGTGGT&quot; &quot;CAGTTTACACACGT&quot; &quot;CATCAGGACTTCCG&quot; &quot;CATCAGGATAGCCA&quot;## [61] &quot;CATGAGACGTTGAC&quot; &quot;CATTACACCAACTG&quot; &quot;CATTTCGAGATACC&quot; &quot;CCTCGAACACTTTC&quot;## [65] &quot;CGACCACTAAAGTG&quot; &quot;CGACCACTGCCAAT&quot; &quot;CGAGGCTGACGCTA&quot; &quot;CGCCGAGAGCTTAG&quot;## [69] &quot;CGGCGAACGACAAA&quot; &quot;CGGCGAACTACTTC&quot; &quot;CGGGCATGTCTCTA&quot; &quot;CGTACCTGGCATCA&quot;## [73] &quot;CGTGTAGACGATAC&quot; &quot;CGTGTAGAGTTACG&quot; &quot;CGTGTAGATTCGGA&quot; &quot;CTAAACCTCTGACA&quot;## [77] &quot;CTAACGGAACCGAT&quot; &quot;CTACGCACTGGTCA&quot; &quot;CTACTCCTATGTCG&quot; &quot;CTAGTTACGAAACA&quot;## [81] &quot;CTATACTGCTACGA&quot; &quot;CTATACTGTCTCAT&quot; &quot;CTCGACTGGTTGAC&quot; &quot;CTGAGAACGTAAAG&quot;## [85] &quot;CTTTAGTGACGGGA&quot; &quot;GAACCAACTTCCGC&quot; &quot;GAAGTGCTAAACGA&quot; &quot;GAATGCACCTTCGC&quot;## [89] &quot;GAATTAACGTCGTA&quot; &quot;GACGGCACACGGGA&quot; &quot;GAGCGCTGAAGATG&quot; &quot;GAGGTACTGACACT&quot;## [93] &quot;GAGGTGGATCCTCG&quot; &quot;GATAGAGAAGGGTG&quot; &quot;GATCCCTGACCTTT&quot; &quot;GCACACCTGTGCTA&quot;## [97] &quot;GCACCACTTCCTTA&quot; &quot;GCACTAGAGTCGTA&quot; &quot;GCAGGGCTATCGAC&quot; &quot;GCCGGAACGTTCTT&quot;## [101] &quot;GCCTACACAGTTCG&quot; &quot;GCGCATCTTGCTCC&quot; &quot;GCGCGATGGTGCAT&quot; &quot;GGAAGGTGGCGAGA&quot;## [105] &quot;GGACGCTGTCCTCG&quot; &quot;GGAGGCCTCGTTGA&quot; &quot;GGCAAGGAAAAAGC&quot; &quot;GGCATATGCTTATC&quot;## [109] &quot;GGCCGAACTCTAGG&quot; &quot;GGCTAAACACCTGA&quot; &quot;GGGTTAACGTGCAT&quot; &quot;GGTGGAGAAACGGG&quot;## [113] &quot;GTAGTGTGAGCGGA&quot; &quot;GTCGACCTGAATGA&quot; &quot;GTGATTCTGGTTCA&quot; &quot;GTGTATCTAGTAGA&quot;## [117] &quot;GTTAAAACCGAGAG&quot; &quot;GTTCAACTGGGACA&quot; &quot;GTTGACGATATCGG&quot; &quot;TAACTCACTCTACT&quot;## [121] &quot;TAAGAGGACTTGTT&quot; &quot;TAATGCCTCGTCTC&quot; &quot;TACGGCCTGGGACA&quot; &quot;TACTACTGATGTCG&quot;## [125] &quot;TACTCTGAATCGAC&quot; &quot;TACTGTTGAGGCGA&quot; &quot;TAGCATCTCAGCTA&quot; &quot;TAGCCCACAGCTAC&quot;## [129] &quot;TAGGGACTGAACTC&quot; &quot;TAGTGGTGAAGTGA&quot; &quot;TAGTTAGAACCACA&quot; &quot;TATGAATGGAGGAC&quot;## [133] &quot;TATGGGTGCATCAG&quot; &quot;TATTTCCTGGAGGT&quot; &quot;TCAACACTGTTTGG&quot; &quot;TCAGACGACGTTAG&quot;## [137] &quot;TCCCGAACACAGTC&quot; &quot;TCCTAAACCGCATA&quot; &quot;TCGATTTGCAGCTA&quot; &quot;TCTAACACCAGTTG&quot;## [141] &quot;TGATAAACTCCGTC&quot; &quot;TGCACAGACGACAT&quot; &quot;TGCCACTGCGATAC&quot; &quot;TGCTGAGAGAGCAG&quot;## [145] &quot;TGGAACACAAACAG&quot; &quot;TGGTAGACCCTCAC&quot; &quot;TGTAATGACACAAC&quot; &quot;TGTAATGAGGTAAA&quot;## [149] &quot;TTACTCGATCTACT&quot; &quot;TTAGTCTGCCAACA&quot; &quot;TTCCAAACTCCCAC&quot; &quot;TTCCCACTTGAGGG&quot;## [153] &quot;TTCTAGTGGAGAGC&quot; &quot;TTCTGATGGAGACG&quot; &quot;TTGTCATGGACGGA&quot; # 提取NK细胞的表达矩阵nk.raw.data &lt;- as.matrix(GetAssayData(pbmc, slot = &quot;counts&quot;)[, WhichCells(pbmc, ident = &quot;NK&quot;)])# 获取基因表达量大于1的对象subset(pbmc, subset = MS4A1 &gt; 1) ## An object of class Seurat ## 13714 features across 414 samples within 1 assay ## Active assay: RNA (13714 features)## 2 dimensional reductions calculated: pca, umap subset(pbmc, subset = replicate == &quot;rep2&quot;) ## An object of class Seurat ## 13714 features across 1290 samples within 1 assay ## Active assay: RNA (13714 features)## 2 dimensional reductions calculated: pca, umap # 选择两个细胞类型subset(pbmc, idents = c(&quot;NK&quot;, &quot;B&quot;)) ## An object of class Seurat ## 13714 features across 499 samples within 1 assay ## Active assay: RNA (13714 features)## 2 dimensional reductions calculated: pca, umap # 排除掉某些细胞类型subset(pbmc, idents = c(&quot;NK&quot;, &quot;B&quot;), invert = TRUE) ## An object of class Seurat ## 13714 features across 2139 samples within 1 assay ## Active assay: RNA (13714 features)## 2 dimensional reductions calculated: pca, umap # note that if you wish to perform additional rounds of clustering after subsetting we recommend# re-running FindVariableFeatures() and ScaleData() 计算基因的平均表达量# 计算平均表达量cluster.averages &lt;- AverageExpression(pbmc)head(cluster.averages[[&quot;RNA&quot;]][, 1:5]) Native CD4 T Memory CD4 T CD14+Mono B CD8 T AL627309.1 0.0061287 0.0059273 0.0485434 0.0000000 0.0205459 AP006222.2 0.0000000 0.0082061 0.0108847 0.0000000 0.0119149 RP11-206L10.2 0.0074531 0.0000000 0.0000000 0.0206503 0.0000000 RP11-206L10.9 0.0000000 0.0000000 0.0105012 0.0000000 0.0000000 LINC00115 0.0191189 0.0246905 0.0375374 0.0388854 0.0194828 NOC2L 0.4974632 0.3598115 0.2725375 0.5865349 0.5570490 # 返回Seurat对象用于下游分析orig.levels &lt;- levels(pbmc)Idents(pbmc) &lt;- gsub(pattern = &quot; &quot;, replacement = &quot;_&quot;, x = Idents(pbmc))orig.levels &lt;- gsub(pattern = &quot; &quot;, replacement = &quot;_&quot;, x = orig.levels)levels(pbmc) &lt;- orig.levelscluster.averages &lt;- AverageExpression(pbmc, return.seurat = TRUE)cluster.averages ## An object of class Seurat ## 13714 features across 9 samples within 1 assay ## Active assay: RNA (13714 features) # How can I plot the average expression of NK cells vs. CD8 T cells? Pass do.hover = T for an# interactive plot to identify gene outliersCellScatter(cluster.averages, cell1 = &quot;NK&quot;, cell2 = &quot;CD8_T&quot;) # How can I calculate expression averages separately for each replicate?cluster.averages &lt;- AverageExpression(pbmc, return.seurat = TRUE, add.ident = &quot;replicate&quot;)CellScatter(cluster.averages, cell1 = &quot;CD8_T_rep1&quot;, cell2 = &quot;CD8_T_rep2&quot;) # You can also plot heatmaps of these &#x27;in silico&#x27; bulk datasets to visualize agreement between# replicatesDoHeatmap(cluster.averages, features = unlist(TopFeatures(pbmc[[&quot;pca&quot;]], balanced = TRUE)), size = 3, draw.lines = FALSE)","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"单细胞测序","slug":"单细胞测序","permalink":"https://xiaohanys.top/tags/%E5%8D%95%E7%BB%86%E8%83%9E%E6%B5%8B%E5%BA%8F/"},{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"},{"title":"使用Monocle3对多样本单细胞数据进行伪时间分析","slug":"使用Monocle3对多样本单细胞数据进行伪时间分析","date":"2019-09-20T04:50:01.000Z","updated":"2024-10-18T05:32:54.311Z","comments":true,"path":"scrna-monocle/","permalink":"https://xiaohanys.top/scrna-monocle/","excerpt":"文章已经过时，请去官网查阅相关文档 简介在发育过程中，细胞对刺激作出反应，并在整个生命过程中，从一种功能“状态”过渡到另一种功能“状态”。不同状态的细胞表达不同的基因，产生蛋白质和代谢物的动态重复序列，从而完成它们的工作。当细胞在状态之间移动时，它们经历一个转录重组的过程，一些基因被沉默，另一些基因被激活。这些瞬时状态通常很难描述，因为在更稳定的端点状态之间纯化细胞可能是困难的或不可能的。单细胞RNA-Seq可以使您在不需要纯化的情况下看到这些状态。然而，要做到这一点，我们必须确定每个cell在可能的状态范围内的位置。","text":"文章已经过时，请去官网查阅相关文档 简介在发育过程中，细胞对刺激作出反应，并在整个生命过程中，从一种功能“状态”过渡到另一种功能“状态”。不同状态的细胞表达不同的基因，产生蛋白质和代谢物的动态重复序列，从而完成它们的工作。当细胞在状态之间移动时，它们经历一个转录重组的过程，一些基因被沉默，另一些基因被激活。这些瞬时状态通常很难描述，因为在更稳定的端点状态之间纯化细胞可能是困难的或不可能的。单细胞RNA-Seq可以使您在不需要纯化的情况下看到这些状态。然而，要做到这一点，我们必须确定每个cell在可能的状态范围内的位置。 Monocle介绍了利用RNA-Seq进行单细胞轨迹分析的策略。Monocle不是通过实验将细胞纯化成离散状态，而是使用一种算法来学习每个细胞必须经历的基因表达变化序列，作为动态生物学过程的一部分。一旦它了解了基因表达变化的整体“轨迹”，Monocle就可以将每个细胞置于轨迹中的适当位置。然后，您可以使用Monocle的微分分析工具包来查找在轨迹过程中受到调控的基因，如查找作为伪时间函数变化的基因一节所述。如果这个过程有多个结果，Monocle将重建一个“分支”轨迹。这些分支与细胞的“决策”相对应，Monocle提供了强大的工具来识别受它们影响的基因，并参与这些基因的形成。在分析单细胞轨迹中的分支的小节中，您可以看到如何分析分支。 monocle 能做的不只是拟时分析，或者说为了做拟时分析他也做了sc-rna-seq的基本分析流程：数据读入，均一化，降维（PCA，umap,tsne,），聚类，marker基因筛选以及可视化函数。在新的学习中我们发现monocle能做的远不只这些，例如用shiny开发了web程序，更加用户友好；借助garnett包可以做细胞定义—–monocle已经是一个sc-rna-seq数据分析的工具箱。 Monocel对象的生成Monocle的cds对象其实在一定程度上非常类似于Seurat，只不过表示方法不一样，所以，我们可以很容易的从Seurat中取出需要的数据载入Monocle3 具体方法可以参考简书:scRNA-seq数据分析 || Monocle3 但是，monocle软件有自己的一套流程，囊括了标准化，归一化，降维，聚类等等，所以一般来说，我们都需要提供原始的未经处理的表达矩阵，但是，由于我们是整合了多样本的结果，而我们又不想使用monocle的批次效应去除方法，我们该怎么导入呢？ 方法就是我们导入Seurat多样本整合并标准化的结果矩阵，然后，在后面的预处理过程中，取消掉标准化。 library(Seurat)library(monocle3)endo&lt;-readRDS(&quot;DC.rds&quot;)DefaultAssay(endo)&lt;-&quot;integrated&quot;data &lt;- endo@assays$integrated@datapd &lt;- endo@meta.data#the metadata have many rubbish info,we delete itnew_pd&lt;-select(pd,stim,nCount_RNA,nFeature_RNA,percent.mt)new_pd$Cell.type&lt;-Idents(endo)head(new_pd) stim nCount_RNA nFeature_RNA percent.mtAAACCTGGTATCAGTC-1_1 CTRL 4835 1383 3.536711AACCGCGCATTCCTGC-1_1 CTRL 4029 1230 2.655746AACGTTGAGCCCAATT-1_1 CTRL 2576 918 3.377329AAGACCTGTGGTTTCA-1_1 CTRL 2841 1044 5.455825AAGTCTGCATGAAGTA-1_1 CTRL 3731 1213 3.886358ACAGCCGCATCGGACC-1_1 CTRL 5915 1918 3.043111 Cell.typeAAACCTGGTATCAGTC-1_1 DC2AACCGCGCATTCCTGC-1_1 MDSCAACGTTGAGCCCAATT-1_1 MDSCAAGACCTGTGGTTTCA-1_1 Dendritic.cells.activatedAAGTCTGCATGAAGTA-1_1 MDSCACAGCCGCATCGGACC-1_1 Dendritic.cells.activatedfData &lt;- data.frame(gene_short_name = row.names(data), row.names = row.names(data))#create new cds objcds &lt;- new_cell_data_set(data,cell_metadata = new_pd,gene_metadata = fData) 预处理数据scale并采用PCA降纬 #we use normalized data,so we do not normalize itcds &lt;- preprocess_cds(cds, num_dim = 30,norm_method = &quot;none&quot;) 如果你非要使用Monocle的批次效应功能，可以尝试,batch列必须在你的pData里面 cds &lt;- align_cds(cds, alignment_group = &quot;batch&quot;) 采用umap的方法运行非线性降维 #umapcds &lt;- reduce_dimension(cds,umap.n_neighbors = 20L)#color by seurat clusterplot_cells(cds,label_groups_by_cluster=FALSE,color_cells_by = &quot;cell.type&quot;) 细胞聚类 #clustercds &lt;- cluster_cells(cds,resolution = 0.5)#color by monocle clusterplot_cells(cds, color_cells_by = &quot;partition&quot;,label_groups_by_cluster=FALSE) 伪时间构建其实说白了，就是基于图形以及表达量的变化关系，构建一个个基于起始表达的进化线或者说是分支，这里面肯定既有节点又有分枝。 cds &lt;- learn_graph(cds) 画图展示： plot_cells(cds,color_cells_by = &quot;Cell.type&quot;,label_groups_by_cluster=FALSE,label_leaves=TRUE,label_branch_points=TRUE) 定义起始节点&amp;&amp;伪时间分析起始节点可以基于现有的知识，比如你已经将细胞类型注释好了，其中某种细胞就是起始的早期细胞，我们就可以把它作为root,如果不知道，我们只能使用1在的细胞群作为起始了 一个有用的定义根节点的函数 get_earliest_principal_node &lt;- function(cds, time_bin=&quot;Dendritic.cells.activated&quot;)&#123; cell_ids &lt;- which(colData(cds)[, &quot;Cell.type&quot;] == time_bin) closest_vertex &lt;- cds@principal_graph_aux[[&quot;UMAP&quot;]]$pr_graph_cell_proj_closest_vertex closest_vertex &lt;- as.matrix(closest_vertex[colnames(cds), ]) root_pr_nodes &lt;- igraph::V(principal_graph(cds)[[&quot;UMAP&quot;]])$name[as.numeric(names (which.max(table(closest_vertex[cell_ids,]))))] root_pr_nodes&#125; 定义根节点 #order cellcds &lt;- order_cells(cds, root_pr_nodes=get_earliest_principal_node(cds))#pseudotime analysisplot_cells(cds,color_cells_by = &quot;pseudotime&quot;,label_cell_groups=FALSE,label_leaves=FALSE,label_branch_points=FALSE,graph_label_size=1.5) 在基因层面探索伪时间寻找随时间变异的基因 diff_gene &lt;- graph_test(cds, neighbor_graph=&quot;principal_graph&quot;, cores=4)id &lt;- row.names(subset(diff_gene, q_value &lt; 0.05))head(id)[1] &quot;CD74&quot; &quot;CXCL14&quot; &quot;HLA-DRA&quot; &quot;GAST&quot; &quot;C1QB&quot; &quot;HBA1&quot; 在cds对象中寻找基因并作图，为了看出变化，可以选择case组中的表达量 CASE_genes &lt;- c(&quot;CD74&quot;, &quot;CXCL14&quot;, &quot;HLA-DRA&quot;)CASE_lineage_cds &lt;- cds[rowData(cds)$gene_short_name %in% AFD_genes,colData(cds)$stim %in% c(&quot;CASE&quot;)]#plot genesplot_genes_in_pseudotime(AFD_lineage_cds,color_cells_by=&quot;Cell.type&quot;) 教程到此结束，原创禁止转载文中图片大部分取自官网，仅作为示例。","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"单细胞测序","slug":"单细胞测序","permalink":"https://xiaohanys.top/tags/%E5%8D%95%E7%BB%86%E8%83%9E%E6%B5%8B%E5%BA%8F/"},{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"},{"title":"一文读懂单细胞测序分析流程","slug":"一文读懂单细胞测序分析流程","date":"2019-04-16T04:42:30.000Z","updated":"2024-10-18T05:33:08.808Z","comments":true,"path":"scRNA-seurat/","permalink":"https://xiaohanys.top/scRNA-seurat/","excerpt":"文章已经过时，请去官网查阅相关文档 摘要一文介绍单细胞测序生物信息分析完整流程，这可能是最新也是最全的流程","text":"文章已经过时，请去官网查阅相关文档 摘要一文介绍单细胞测序生物信息分析完整流程，这可能是最新也是最全的流程 基础流程（cellranger） cellranger 数据拆分cellranger mkfastq 可用于将单细胞测序获得的 BCL 文件拆分为可以识别的 fastq 测序数据 cellranger makefastq --run=[ ] --samplesheet=[sample.csv] --jobmode=local --localcores=20 --localmem=80 -–run ：是下机数据 BCL 所在的路径；-–samplesheet ：样品信息列表–共三列（lane id ,sample name ,index name)注意要控制好核心数和内存数 运行产出结果存在于 out 目录中 cellranger 数据统计cellranger count是 cellranger 最主要也是最重要的功能：完成细胞和基因的定量，也就是产生了我们用来做各种分析的基因表达矩阵。 cellranger count \\-–id=sample345 \\-–transcriptome=/opt/refdata-cellranger-GRCh38-1.2.0/GRCh38 \\-–fastqs=/home/jdoe/runs/HAWT7ADXX/outs/fastq_path \\-–indices=SI-3A-A1 \\–-cells=1000 id ：产生的结果都在这个文件中，可以取几号样品（如 sample345）； fastqs ：由 cellranger mkfastq 产生的 fastqs 文件夹所在的路径；fastqs ：由 cellranger mkfastq 产生的 fastqs 文件夹所在的路径； indices：sample index：SI-3A-A1； transcriptome：参考转录组文件路径； cells：预期回复的细胞数； 下游分析cellranger count 计算的结果只能作为初步观测的结果，如果需要进一步分析聚类细胞，还需要进行下游分析，这里使用官方推荐 R 包（Seurat 3.1） 流程参考官方（外周血分析标准流程） 软件安装install.packages(&#x27;Seurat&#x27;)library(Seurat) 生成 Seruat 对象library(dplyr)library(Seurat)# Load the PBMC datasetpbmc.data &lt;- Read10X(data.dir = &quot;../data/pbmc3k/filtered_gene_bc_matrices/hg19/&quot;)# Initialize the Seurat object with the raw (non-normalized data).pbmc &lt;- CreateSeuratObject(counts = pbmc.data, project = &quot;pbmc3k&quot;, min.cells = 3, min.features = 200)pbmc ## An object of class Seurat## 13714 features across 2700 samples within 1 assay## Active assay: RNA (13714 features) 这里读取的是单细胞 count 结果中的矩阵目录；在对象生成的过程中，做了初步的过滤；留下所有在&gt;&#x3D;3 个细胞中表达的基因 min.cells &#x3D; 3；为了除去一些质量差的细胞,留下所有检测到&gt;&#x3D;200 个基因的细胞 min.genes &#x3D; 200。 标准预处理流程# The [[ operator can add columns to object metadata. This is a great place to stash QC statspbmc[[&quot;percent.mt&quot;]] &lt;- PercentageFeatureSet(object = pbmc, pattern = &quot;^MT-&quot;) 这一步 mit-开头的为线粒体基因，这里将其进行标记并统计其分布频率 # Visualize QC metrics as a violin plotVlnPlot(object = pbmc, features = c(&quot;nFeature_RNA&quot;, &quot;nCount_RNA&quot;, &quot;percent.mt&quot;), ncol = 3) 对 pbmc 对象做小提琴图，分别为基因数，细胞数和线粒体占比 pbmc &lt;- subset(x = pbmc, subset = nFeature_RNA &gt; 200 &amp; nFeature_RNA &lt; 2500 &amp; percent.mt &lt; 5) 接下来，根据图片中基因数和线粒体数，分别设置过滤参数，这里基因数 200-2500，线粒体百分比为小于 5% 数据标准化pbmc &lt;- NormalizeData(object = pbmc, normalization.method = &quot;LogNormalize&quot;, scale.factor = 10000)pbmc &lt;- NormalizeData(object = pbmc) 鉴定高度变化基因pbmc &lt;- FindVariableFeatures(object = pbmc, selection.method = &quot;vst&quot;, nfeatures = 2000)# Identify the 10 most highly variable genestop10 &lt;- head(x = VariableFeatures(object = pbmc), 10)# plot variable features with and without labelsplot1 &lt;- VariableFeaturePlot(object = pbmc)plot2 &lt;- LabelPoints(plot = plot1, points = top10, repel = TRUE)CombinePlots(plots = list(plot1, plot2)) 数据归一化all.genes &lt;- rownames(x = pbmc)pbmc &lt;- ScaleData(object = pbmc, features = all.genes) 这里设置对所有的基因都做了scale,但是需要知道的是，其实后续的分析都是基于高变基因的，因此，使用默认参数就可以了，而且提升效率。 pbmc &lt;- ScaleData(object = pbmc) 线形降维pbmc &lt;- RunPCA(object = pbmc, features = VariableFeatures(object = pbmc)) 这里有多种方法展示 pca 结果，本文采用最简单的方法 DimPlot(object = pbmc, reduction = &quot;pca&quot;) 鉴定数据集的可用维度pbmc &lt;- JackStraw(object = pbmc, num.replicate = 100)pbmc &lt;- ScoreJackStraw(object = pbmc, dims = 1:20)JackStrawPlot(object = pbmc, dims = 1:15) 虚线以上的为可用维度，你也可以调整 dims 参数，画出所有 pca 查看 另外一种鉴定手段是绘制所有 PC 的分布点图 ElbowPlot(pbmc) 大多数软件都是通过拾取拐点处的 pc 作为选定数目 细胞聚类pbmc &lt;- FindNeighbors(object = pbmc, dims = 1:10)pbmc &lt;- FindClusters(object = pbmc, resolution = 0.5) 这里的 dims 为上一步计算所用的维度数，而 resolution 参数控制聚类的数目，针对 3K 的细胞数目，最好的范围是0.4-1.2 head(Idents(pbmc), 5) ## AAACATACAACCAC AAACATTGAGCTAC AAACATTGATCAGC AAACCGTGCTTCCG AAACCGTGTATGCG## 1 3 1 2 6## Levels: 0 1 2 3 4 5 6 7 8 执行非线性降维这里注意，这一步聚类有两种聚类方法(umap&#x2F;tSNE)，两种方法都可以使用，但不要混用，这样，后面的结算结果会将先前的聚类覆盖掉，只能保留一个本文采用基于 umap 的聚类方法 pbmc &lt;- RunUMAP(object = pbmc, dims = 1:10)DimPlot(object = pbmc, reduction = &quot;umap&quot;) 完成聚类后，一定要记住保存数据，不然重新计算可要头疼了 saveRDS(pbmc, file = &quot;../output/pbmc_tutorial.rds&quot;) 寻找每个聚类中显著表达的基因cluster1.markers &lt;- FindMarkers(object = pbmc, ident.1 = 1, min.pct = 0.25)head(x = cluster1.markers, n = 5) 这样是寻找单个聚类中的显著基因 cluster5.markers &lt;- FindMarkers(object = pbmc, ident.1 = 5, ident.2 = c(0, 3), min.pct = 0.25)head(x = cluster5.markers, n = 5) 这样寻找所有聚类中显著基因，计算速度很慢，需要等待 另外，我们有多种方法统计基因的显著性 FeaturePlot(object = pbmc, features = c(&quot;MS4A1&quot;, &quot;GNLY&quot;, &quot;CD3E&quot;, &quot;CD14&quot;, &quot;FCER1A&quot;, &quot;FCGR3A&quot;, &quot;LYZ&quot;, &quot;PPBP&quot;, &quot;CD8A&quot;)) top10 &lt;- pbmc.markers %&gt;% group_by(cluster) %&gt;% top_n(n = 10, wt = avg_logFC)DoHeatmap(object = pbmc, features = top10$gene) + NoLegend() 剩下的便是寻找基因 marker 并对细胞类型进行注释 你可能想要知道如何对多样本进行整合分析，请参考教程：整合刺激性和对照性 PBMC 数据集，以学习细胞类型特异性反应 你可能想了解如何灵活操作 Seurat 的 S4 对象，以便轻松的提取表达矩阵；亦或者想要做一些不一样的可视化效果，例如采用平均表达量做热图展示等，请参考Seurat3.1 的灵活操作指南 你可能想对单细胞数据做进一步分析，例如功能分析，请参考10X 单细胞数据针对细胞及其亚型的基因集功能分析和转录调控分析 全自动细胞类型注释众所周知，细胞类型的注释是最困难的一步，除非你有很强的对细胞基因的敏感度，不然很难识别细胞类型。 通常识别细胞类型的方法主要有三种 根据 Marker 基因，采用CellMarker或者panglaoDB数据库，进行细胞注释,可以采用超几何分布算法来进行精确性验证，方法参考ClusterProfiler:真的不只是富集分析 从文献中获取已经验证的 Marker 采用一些自动化注释的软件，例如SingleR,Garnett,celaref等 这里简单介绍下SingleR SingleR:一个全自动细胞注释的 R 包，用法很简单 软件安装BiocManager::install(&quot;SingleR&quot;)browseVignettes(&quot;SingleR&quot;) 创建 SingleR 对象从头预测的方法请参考官方教程 因为我们刚刚从 Seurat 过来的，所以我们应该很想知道 Seurat cluster 的细胞注释结果，因此，对 Seurat 的结果进行注释 我们这里采用两个人类的参考集去做细胞注释 library(Seurat)library(SingleR)library(dplyr)library(tibble)hpca.se &lt;- HumanPrimaryCellAtlasData()bpe.se &lt;- BlueprintEncodeData() 读入Seurat对象转换为SingleCell支持的对象 seurat.obj &lt;- readRDS(&quot;../output/pbmc_tutorial.rds&quot;)seurat.obj@meta.data$cell.type &lt;- Idents(seurat.obj)test &lt;- as.SingleCellExperiment(seurat.obj) 采用两个参考集一起进行注释， Anno &lt;- SingleR(test = test, ref = list(HP = hpca.se , BP = bpe.se), labels = list(hpca.se$label.main , bpe.se$label.main), method = &quot;cluster&quot;, cluster = test$cell.type) 提取需要的细胞分类信息 Anno$cluster &lt;- rownames(Anno)fin &lt;- Anno %&gt;% dplyr::tbl_df() %&gt;% dplyr::select(cluster,labels) 你也可以将细胞注释信息重新添加到Seurat对象中去 new.cluster.ids &lt;- fin$labelsnames(new.cluster.ids) &lt;- levels(seurat.obj)seurat.obj &lt;- RenameIdents(seurat.obj, new.cluster.ids) 伪时间分析伪时间分析建议采用 monocle3.0 软件 软件安装##安装依赖BiocManager::install(c(&#x27;BiocGenerics&#x27;, &#x27;DelayedArray&#x27;, &#x27;DelayedMatrixStats&#x27;, &#x27;limma&#x27;, &#x27;S4Vectors&#x27;, &#x27;SingleCellExperiment&#x27;, &#x27;SummarizedExperiment&#x27;, &#x27;batchelor&#x27;))##安装monocle3devtools::install_github(&#x27;cole-trapnell-lab/leidenbase&#x27;)devtools::install_github(&#x27;cole-trapnell-lab/monocle3&#x27;)library(monocle3) 标准分析流library(Seurat)library(monocle3)endo&lt;-readRDS(&quot;../output/pbmc_tutorial.rds&quot;)data &lt;- endo@assays$RNA@countspd &lt;- endo@meta.data 不要直接把meta.data放入pd中去，太多没用的信息了，先清理下 new_pd&lt;-select(pd,nCount_RNA,nFeature_RNA,percent.mt)new_pd$Cell.type&lt;-Idents(endo)head(new_pd) nCount_RNA nFeature_RNA percent.mtAAACCTGGTATCAGTC-1_1 4835 1383 3.536711AACCGCGCATTCCTGC-1_1 4029 1230 2.655746AACGTTGAGCCCAATT-1_1 2576 918 3.377329AAGACCTGTGGTTTCA-1_1 2841 1044 5.455825AAGTCTGCATGAAGTA-1_1 3731 1213 3.886358ACAGCCGCATCGGACC-1_1 5915 1918 3.043111 Cell.typeAAACCTGGTATCAGTC-1_1 DC2AACCGCGCATTCCTGC-1_1 MDSCAACGTTGAGCCCAATT-1_1 MDSCAAGACCTGTGGTTTCA-1_1 Dendritic.cells.activatedAAGTCTGCATGAAGTA-1_1 MDSCACAGCCGCATCGGACC-1_1 Dendritic.cells.activatedfData &lt;- data.frame(gene_short_name = row.names(data), row.names = row.names(data)) 生成monocle的cds对象 cds &lt;- new_cell_data_set(data,cell_metadata = new_pd,gene_metadata = fData) 运行下游标准流程，无论如何都得跑，是必须的默认流程 cds &lt;- preprocess_cds(cds, num_dim = 30)#umapcds &lt;- reduce_dimension(cds,umap.n_neighbors = 20L)#color by seurat clusterplot_cells(cds,label_groups_by_cluster=FALSE,color_cells_by = &quot;Cell.type&quot;) #clustercds &lt;- cluster_cells(cds,resolution = 0.5)#color by monocle clusterplot_cells(cds, color_cells_by = &quot;partition&quot;,label_groups_by_cluster=FALSE) cds &lt;- learn_graph(cds)plot_cells(cds,color_cells_by = &quot;Cell.type&quot;,label_groups_by_cluster=FALSE,label_leaves=TRUE,label_branch_points=TRUE) 定义时间开始节点&amp;&amp;伪时间分析##一个有用的寻找起源节点的函数get_earliest_principal_node &lt;- function(cds, time_bin=&quot;Dendritic.cells.activated&quot;)&#123; cell_ids &lt;- which(colData(cds)[, &quot;Cell.type&quot;] == time_bin) closest_vertex &lt;- cds@principal_graph_aux[[&quot;UMAP&quot;]]$pr_graph_cell_proj_closest_vertex closest_vertex &lt;- as.matrix(closest_vertex[colnames(cds), ]) root_pr_nodes &lt;- igraph::V(principal_graph(cds)[[&quot;UMAP&quot;]])$name[as.numeric(names (which.max(table(closest_vertex[cell_ids,]))))] root_pr_nodes&#125;#order cellcds &lt;- order_cells(cds, root_pr_nodes=get_earliest_principal_node(cds))#pseudotime analysisplot_cells(cds,color_cells_by = &quot;pseudotime&quot;,label_cell_groups=FALSE,label_leaves=FALSE,label_branch_points=FALSE,graph_label_size=1.5) PS:对多样本进行伪时间分析，请参考使用 Monocle3 对多样本单细胞数据进行伪时间分析 本文纯属原创，部分数据采用官方教程，转载需标明出处 教程链接： 教程：整合刺激性和对照性 PBMC 数据集，以学习细胞类型特异性反应 Seurat3.1 的灵活操作指南 10X 单细胞数据针对细胞及其亚型的基因集功能分析和转录调控分析 ClusterProfiler:真的不只是富集分析 使用 Monocle3 对多样本单细胞数据进行伪时间分析","categories":[{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"}],"tags":[{"name":"单细胞测序","slug":"单细胞测序","permalink":"https://xiaohanys.top/tags/%E5%8D%95%E7%BB%86%E8%83%9E%E6%B5%8B%E5%BA%8F/"},{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"}],"author":"Xiaoguang Pan"}],"categories":[{"name":"网站分享","slug":"网站分享","permalink":"https://xiaohanys.top/categories/%E7%BD%91%E7%AB%99%E5%88%86%E4%BA%AB/"},{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/categories/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"},{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/categories/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"编程语言","slug":"编程语言","permalink":"https://xiaohanys.top/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://xiaohanys.top/tags/AI/"},{"name":"网站分享","slug":"网站分享","permalink":"https://xiaohanys.top/tags/%E7%BD%91%E7%AB%99%E5%88%86%E4%BA%AB/"},{"name":"Python","slug":"Python","permalink":"https://xiaohanys.top/tags/Python/"},{"name":"网站搭建","slug":"网站搭建","permalink":"https://xiaohanys.top/tags/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"},{"name":"前端","slug":"前端","permalink":"https://xiaohanys.top/tags/%E5%89%8D%E7%AB%AF/"},{"name":"生物信息","slug":"生物信息","permalink":"https://xiaohanys.top/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF/"},{"name":"R语言","slug":"R语言","permalink":"https://xiaohanys.top/tags/R%E8%AF%AD%E8%A8%80/"},{"name":"后端","slug":"后端","permalink":"https://xiaohanys.top/tags/%E5%90%8E%E7%AB%AF/"},{"name":"julia","slug":"julia","permalink":"https://xiaohanys.top/tags/julia/"},{"name":"单细胞测序","slug":"单细胞测序","permalink":"https://xiaohanys.top/tags/%E5%8D%95%E7%BB%86%E8%83%9E%E6%B5%8B%E5%BA%8F/"},{"name":"转录组","slug":"转录组","permalink":"https://xiaohanys.top/tags/%E8%BD%AC%E5%BD%95%E7%BB%84/"},{"name":"CRISPR","slug":"CRISPR","permalink":"https://xiaohanys.top/tags/CRISPR/"},{"name":"gRNA","slug":"gRNA","permalink":"https://xiaohanys.top/tags/gRNA/"}]}